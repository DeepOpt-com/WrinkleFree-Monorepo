# Distillation main configuration
#
# Usage:
#   # BitNet distillation (default)
#   python scripts/distill.py student.checkpoint_path=outputs/stage2/checkpoint.pt
#
#   # DLM distillation with TCS
#   python scripts/distill.py \
#     distillation=tcs \
#     student.checkpoint_path=gs://wrinklefree-checkpoints/dlm/bitnet-b1.58-2B-4T-bf16/ \
#     student.type=dlm

defaults:
  - distillation: bitdistill
  - _self_

# Student model (required)
student:
  checkpoint_path: ???  # Required: path to student checkpoint
  type: auto            # "bitnet", "dlm", or "auto" (auto-detect from path)
  model_config: null    # Optional: override model config

# Teacher model
teacher:
  model_name: null      # null = infer from student checkpoint's original model
  use_vllm: false       # Use vLLM server instead of local loading
  vllm_url: "http://localhost:8000"
  vllm_top_k_logprobs: 100
  load_in_4bit: false   # Load teacher in 4-bit quantization
  offload_to_cpu: false # Offload to CPU between forward passes

# Data (uses cheapertraining)
data:
  config_name: mixed_pretrain  # CheaperTraining config to use
  max_seq_length: 2048  # MUST-DO: always use 2048
  num_workers: 4

# Training
training:
  max_steps: 5000
  batch_size: 32
  gradient_accumulation_steps: 8
  gradient_clipping: 1.0

# Optimizer (MUST-DO: always use MuonClip)
optimizer:
  type: muon  # muon (default), adamw, adamw_8bit
  lr: 2.4e-3
  weight_decay: 0.024
  # MuonClip uses separate LRs for Muon and Adam param groups
  lr_muon: 1e-3  # Learning rate for Muon (main weights)
  lr_adam: 5e-5  # Learning rate for Adam (bias/norm)

# Scheduler
scheduler:
  type: cosine
  warmup_steps: 0  # 0 = auto (use warmup_ratio)
  warmup_ratio: 0.03  # 3% of max_steps when warmup_steps=0
  min_lr_ratio: 0.1

# Checkpointing
checkpoint:
  save_interval: 50  # Reduced for testing GCS uploads
  keep_last_n: 3
  resume_from: ""  # Path to checkpoint to resume from (local or gs://)
  gcs:
    enabled: true  # Upload checkpoints to GCS
    bucket: wrinklefree-checkpoints
    upload_interval: 50  # Upload every N steps (reduced for testing)
    keep_n: 5  # Keep N most recent checkpoints on GCS

# Logging
logging:
  log_interval: 10
  eval_interval: 500
  wandb:
    enabled: true
    project: wrinklefree-distillation

# Influence-based rebalancing
influence:
  enabled: true
  update_interval: 1000
  learning_rate: 0.2

# Output
output_dir: outputs/distillation
experiment_name: distill_${now:%Y%m%d_%H%M%S}
