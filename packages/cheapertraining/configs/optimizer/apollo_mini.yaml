# APOLLO-Mini optimizer - extreme memory efficiency (1/1024 of AdamW memory)
# Reference: https://arxiv.org/abs/2412.05270
# MLSys 2025 Outstanding Paper Honorable Mention
#
# Requires optional dependency: pip install cheapertraining[apollo]
#
# Use case: Extreme memory constraints. Can train LLaMA-7B on single GPU
# with <12GB memory when combined with weight quantization.

optimizer:
  type: apollo_mini
  betas: [0.9, 0.95]
  eps: 1.0e-8
  # Note: lr and weight_decay come from training config

# Gradient scaling for mixed precision
grad_scaler:
  enabled: true
  init_scale: 65536
  growth_factor: 2.0
  backoff_factor: 0.5
  growth_interval: 2000
