# Mixed Pre-training Dataset Configuration
# Canonical location for all WrinkleFree projects
#
# OSS-friendly data mixture with influence-based selection (MobileLLM-R1 methodology)
# Reference: arXiv:2509.24945

name: mixed_pretrain

# Multiple data sources with initial weights (adjusted by influence remixing)
# Web + Code + Math + Reasoning + Diversity mix for influence remixing
sources:
  - name: dclm
    path: mlfoundations/dclm-baseline-1.0-parquet  # Parquet version (cleaner)
    subset: null
    weight: 0.225  # 22.5% high-quality web (Apple's curated 4T tokens)
    text_column: text
    streaming: true

  - name: fineweb_edu
    path: HuggingFaceFW/fineweb-edu
    subset: sample-10BT
    weight: 0.27  # 27% educational content (includes math)
    text_column: text
    streaming: true

  - name: github_code
    path: nick007x/github-code-2025  # MIT licensed, 2025 curated code
    subset: null
    weight: 0.135  # 13.5% code
    text_column: content
    streaming: true

  - name: finemath
    path: HuggingFaceTB/finemath  # 34B tokens math text (ODC-By license, commercially friendly)
    subset: finemath-3plus
    weight: 0.135  # 13.5% mathematical reasoning
    text_column: text
    streaming: true

  - name: fineweb
    path: HuggingFaceFW/fineweb
    subset: sample-10BT
    weight: 0.135  # 13.5% diverse web content
    text_column: text
    streaming: true

  - name: synth
    path: PleIAs/SYNTH  # Synthetic reasoning from Wikipedia (CDLA-Permissive 2.0)
    subset: null
    weight: 0.10  # 10% synthetic reasoning (query + reasoning + answer)
    text_column:
      - query
      - synthetic_reasoning
      - synthetic_answer
    text_separator: "\n\n"
    streaming: true

# Multi-domain probe for influence calculation
# Samples FROM our training sources (reduced to 1000 total for memory efficiency)
probe:
  domains:
    web_edu:
      path: HuggingFaceFW/fineweb-edu
      subset: sample-10BT
      samples: 34  # Reduced for memory efficiency
      text_column: text
      split: train

    code:
      path: nick007x/github-code-2025
      subset: null
      samples: 33
      text_column: content
      split: train

    math:
      path: HuggingFaceTB/finemath
      subset: finemath-3plus
      samples: 33
      text_column: text
      split: train

    dclm:
      path: mlfoundations/dclm-baseline-1.0-parquet
      subset: null
      samples: 33
      text_column: text
      split: train

    diverse:
      path: HuggingFaceFW/fineweb
      subset: sample-10BT
      samples: 33
      text_column: text
      split: train

    reasoning:
      path: PleIAs/SYNTH
      subset: null
      samples: 34
      text_column:
        - query
        - synthetic_reasoning
        - synthetic_answer
      text_separator: "\n\n"
      split: train

  total_samples: 200  # Reduced from 1200 to prevent OOM during influence updates
  streaming: true

# Preprocessing settings
preprocessing:
  max_length: 2048
  truncation: true
  padding: false
  packed: true

# Dataloader settings
dataloader:
  batch_size: 32
  num_workers: 4
  prefetch_factor: 2
  pin_memory: true
