# Mixture weight optimization configuration (Phase II)
# Reference: MobileLLM-R1 paper (arXiv:2509.24945) Phase II

defaults:
  - default

# Phase II: Pre-training Mixture Optimization
mixture:
  # Number of samples to evaluate from each dataset for influence calculation
  samples_per_dataset: 256

  # Weight normalization
  normalize_weights: true

  # Weight constraints
  min_weight: 0.05  # Minimum 5% per dataset
  max_weight: 0.60  # Maximum 60% per dataset (prevent single source dominance)

  # How often to recompute weights (in training steps)
  weight_update_interval: 10000

  # EMA smoothing for influence estimates
  # Higher = more responsive, lower = more stable
  influence_smoothing: 0.1

# Multi-domain probe configuration (MobileLLM-R1 style)
# Probe samples from training sources to cover Code, Math, Knowledge domains
probe:
  # Enable multi-domain influence calculation
  multi_domain: true

  # Domain definitions
  domains:
    - code
    - math
    - knowledge

  # Domain weights for joint influence (sum to 1.0)
  domain_weights:
    code: 0.33
    math: 0.33
    knowledge: 0.34

  # Samples per domain for probe
  samples_per_domain: 2000

# Training stage settings for influence-based pretraining
training:
  # Whether to enable dynamic weight updates
  enable_dynamic_weights: true

  # Learning rate for weight updates (interpolation factor)
  weight_learning_rate: 0.2

  # Log weight changes
  log_weight_updates: true
