# Muon optimizer - 2x compute efficiency vs AdamW, faster convergence
# Reference: https://arxiv.org/abs/2502.16982
#
# Muon applies to hidden layer 2D weights only.
# Embeddings, heads, biases, and norms use AdamW at 0.1x learning rate.

optimizer:
  type: muon
  momentum: 0.95
  nesterov: true
  # AdamW settings for embed/head/bias/norm params
  adamw_betas: [0.9, 0.95]
  adamw_eps: 1.0e-8
  # Note: lr and weight_decay come from training config

# Gradient scaling for mixed precision
grad_scaler:
  enabled: true
  init_scale: 65536
  growth_factor: 2.0
  backoff_factor: 0.5
  growth_interval: 2000
