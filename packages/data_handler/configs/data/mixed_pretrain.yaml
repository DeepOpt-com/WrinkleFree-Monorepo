# Mixed Pre-training Dataset Configuration
# Canonical location for all WrinkleFree projects
#
# OSS-friendly data mixture with influence-based selection (MobileLLM-R1 methodology)
# Reference: arXiv:2509.24945

name: mixed_pretrain

# Shuffle buffer size - larger = better mixing but more memory
# 50000 recommended for 5+ sources to ensure proper domain mixing
shuffle_buffer_size: 50000

# Homogeneous batch size for ODM (Online Data Mixing)
# When > 0, yields this many consecutive samples from the same domain
# before sampling a new domain. This implements the ODM paper's
# homogeneous batch sampling strategy (arxiv:2312.02406).
# Set to 0 for standard mixed sampling, or ~128 when using ODM.
homogeneous_batch_size: 0

# Multiple data sources with initial weights (adjusted by influence remixing)
# Web + Code + Math + Reasoning + Diversity mix for influence remixing
sources:
  - name: dclm
    path: mlfoundations/dclm-baseline-1.0-parquet  # Parquet version (cleaner)
    subset: null
    weight: 0  # DISABLED: Slow to load for smoke test
    text_column: text
    streaming: true

  - name: fineweb_edu
    path: HuggingFaceFW/fineweb-edu
    subset: sample-10BT
    weight: 0.50  # Primary educational content
    text_column: text
    streaming: true

  - name: github_code
    path: nick007x/github-code-2025  # MIT licensed, 2025 curated code
    subset: null
    weight: 0.20  # Code (re-enabled with HF auth)
    text_column: content
    streaming: true

  - name: finemath
    path: HuggingFaceTB/finemath  # 34B tokens math text (ODC-By license, commercially friendly)
    subset: finemath-3plus
    weight: 0.30  # Mathematical reasoning
    text_column: text
    streaming: true

  - name: slimpajama
    path: cerebras/SlimPajama-627B  # Cleaned RedPajama (Apache 2.0)
    subset: null
    weight: 0  # DISABLED: HuggingFace CDN 502 errors (2026-01)
    text_column: text
    streaming: true

  - name: synth
    path: PleIAs/SYNTH  # Synthetic reasoning from Wikipedia (CDLA-Permissive 2.0)
    subset: null
    weight: 0  # Disabled for now
    text_column:
      - query
      - synthetic_reasoning
      - synthetic_answer
    text_separator: "\n\n"
    streaming: true

# Multi-domain probe for influence calculation
# IMPORTANT: Keep samples LOW to avoid OOM during gradient caching
# Each sample requires ~500MB for full gradient storage
probe:
  domains:
    web_edu:
      path: HuggingFaceFW/fineweb-edu
      subset: sample-10BT
      samples: 16  # Reduced for memory - main validation domain
      text_column: text
      split: train

    code:
      path: nick007x/github-code-2025
      subset: null
      samples: 0  # DISABLED: Memory intensive
      text_column: content
      split: train

    math:
      path: HuggingFaceTB/finemath
      subset: finemath-3plus
      samples: 16  # Reduced for memory
      text_column: text
      split: train

    dclm:
      path: mlfoundations/dclm-baseline-1.0-parquet
      subset: null
      samples: 0  # DISABLED: Slow to load for smoke test
      text_column: text
      split: train

    diverse:
      path: cerebras/SlimPajama-627B
      subset: null
      samples: 0  # DISABLED: HuggingFace CDN 502 errors (2026-01)
      text_column: text
      split: train

    reasoning:
      path: PleIAs/SYNTH
      subset: null
      samples: 0  # DISABLED: Not used in 2-dataset config
      text_column:
        - query
        - synthetic_reasoning
        - synthetic_answer
      text_separator: "\n\n"
      split: train

  total_samples: 32  # Sum of active probe samples (16+16)
  streaming: true

# Preprocessing settings
preprocessing:
  max_length: 2048
  truncation: true
  padding: false
  packed: true

# Dataloader settings
dataloader:
  batch_size: 32
  # num_workers: Number of data loading workers (default: 4)
  # - 0: Load data in main process (slower but easier to debug)
  # - 4-8: Recommended for streaming datasets (balances throughput and HF API limits)
  num_workers: 4
  prefetch_factor: 2
  pin_memory: true
