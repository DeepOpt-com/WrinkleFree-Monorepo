# Adam optimizer configuration for pretraining
# Reference: Section 3.1 in arXiv:2509.24945

optimizer:
  type: adam
  betas: [0.9, 0.95]
  eps: 1.0e-8
  # Note: lr and weight_decay come from training config

# Gradient scaling for mixed precision
grad_scaler:
  enabled: true
  init_scale: 65536
  growth_factor: 2.0
  backoff_factor: 0.5
  growth_interval: 2000
