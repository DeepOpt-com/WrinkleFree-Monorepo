"""Unit tests for training components.

Tests stages, optimizer, scheduler, and distillation.
"""

import pytest
import torch
import torch.nn as nn

# Skip import errors for legacy modules that don't exist
pytest.importorskip("cheapertraining._legacy.training.stages.base")
from cheapertraining._legacy.training.stages.base import StageConfig, TrainingMetrics
from cheapertraining._legacy.training.scheduler import (
    LinearWarmupLinearDecay,
    LinearWarmupLinearDecayToZero,
    CosineWarmup,
    create_scheduler,
)
from cheapertraining._legacy.distillation.losses import (
    KLDivergenceLoss,
    DistillationLoss,
    ForwardKL,
    ReverseKL,
)


class SimpleModel(nn.Module):
    """Simple model for testing."""

    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(64, 64)
        self.norm = nn.LayerNorm(64)
        self.bias_layer = nn.Linear(64, 64, bias=True)

    def forward(self, x):
        return self.bias_layer(self.norm(self.linear(x)))


class TestOptimizer:
    """Tests for optimizer utilities."""

    def test_parameter_groups(self):
        """Test parameter group separation."""
        model = SimpleModel()
        groups = get_parameter_groups(model, weight_decay=0.1)

        assert len(groups) == 2
        # First group should have weight decay
        assert groups[0]["weight_decay"] == 0.1
        # Second group (bias, norm) should have no decay
        assert groups[1]["weight_decay"] == 0.0

    def test_create_adam_optimizer(self):
        """Test Adam optimizer creation."""
        model = SimpleModel()
        optimizer = create_optimizer(
            model,
            optimizer_type="adam",
            learning_rate=1e-3,
            weight_decay=0.1,
            betas=(0.9, 0.95),
        )

        assert isinstance(optimizer, torch.optim.Adam)
        assert len(optimizer.param_groups) == 2

    def test_num_parameters(self):
        """Test parameter counting."""
        model = SimpleModel()
        num_params = get_num_parameters(model)
        assert num_params > 0

    def test_create_muonclip_optimizer(self):
        """Test MuonClip optimizer creation."""
        pytest.importorskip("muon_clip", reason="muon-clip not installed")
        model = SimpleModel()
        optimizer = create_optimizer(
            model,
            optimizer_type="muonclip",
            learning_rate=4e-3,
            weight_decay=0.1,
            betas=(0.9, 0.95),
            enable_clipping=True,
            clipping_threshold=50.0,
            clipping_alpha=0.5,
        )
        # MuonClip should be returned
        assert optimizer is not None
        assert hasattr(optimizer, "step")

    def test_muonclip_with_clipping_disabled(self):
        """Test MuonClip with QK-clipping disabled."""
        pytest.importorskip("muon_clip", reason="muon-clip not installed")
        model = SimpleModel()
        optimizer = create_optimizer(
            model,
            optimizer_type="muonclip",
            learning_rate=4e-3,
            enable_clipping=False,
        )
        assert optimizer is not None

    def test_muonclip_step(self):
        """Test MuonClip optimizer can perform a step."""
        pytest.importorskip("muon_clip", reason="muon-clip not installed")
        model = SimpleModel()
        optimizer = create_optimizer(
            model,
            optimizer_type="muonclip",
            learning_rate=4e-3,
        )

        # Forward pass
        x = torch.randn(2, 64)
        y = model(x)
        loss = y.sum()

        # Backward pass
        loss.backward()

        # Optimizer step should not raise
        optimizer.step()
        optimizer.zero_grad()


class TestScheduler:
    """Tests for learning rate schedulers."""

    def test_linear_warmup_decay(self):
        """Test linear warmup followed by linear decay."""
        model = SimpleModel()
        optimizer = torch.optim.Adam(model.parameters(), lr=1.0)
        scheduler = LinearWarmupLinearDecay(
            optimizer,
            warmup_steps=100,
            total_steps=1000,
            min_lr_ratio=0.1,
        )

        # At step 0, LR should be near 0
        assert scheduler.get_lr()[0] < 0.1

        # At step 100 (end of warmup), LR should be 1.0
        for _ in range(100):
            scheduler.step()
        assert abs(scheduler.get_lr()[0] - 1.0) < 0.01

        # At step 1000, LR should be 0.1
        for _ in range(900):
            scheduler.step()
        assert abs(scheduler.get_lr()[0] - 0.1) < 0.01

    def test_linear_decay_to_zero(self):
        """Test linear decay to zero."""
        model = SimpleModel()
        optimizer = torch.optim.Adam(model.parameters(), lr=1.0)
        scheduler = LinearWarmupLinearDecayToZero(
            optimizer,
            warmup_steps=0,
            total_steps=100,
        )

        # At step 100, LR should be ~0
        for _ in range(100):
            scheduler.step()
        assert scheduler.get_lr()[0] < 0.01

    def test_cosine_warmup(self):
        """Test cosine warmup scheduler."""
        model = SimpleModel()
        optimizer = torch.optim.Adam(model.parameters(), lr=1.0)
        scheduler = CosineWarmup(
            optimizer,
            warmup_steps=10,
            total_steps=100,
            min_lr_ratio=0.0,
        )

        # LR should smoothly decay
        lrs = []
        for _ in range(100):
            lrs.append(scheduler.get_lr()[0])
            scheduler.step()

        # Final LR should be near 0
        assert lrs[-1] < 0.1

    def test_create_scheduler(self):
        """Test scheduler factory."""
        model = SimpleModel()
        optimizer = torch.optim.Adam(model.parameters(), lr=1.0)

        scheduler = create_scheduler(
            optimizer,
            scheduler_type="linear_decay",
            warmup_steps=100,
            total_steps=1000,
        )
        assert isinstance(scheduler, LinearWarmupLinearDecay)


class TestDistillationLosses:
    """Tests for distillation losses."""

    def test_kl_divergence_loss(self):
        """Test KL divergence loss."""
        loss_fn = KLDivergenceLoss(temperature=1.0)

        student_logits = torch.randn(2, 64, 1000)
        teacher_logits = torch.randn(2, 64, 1000)

        loss = loss_fn(student_logits, teacher_logits)

        assert loss.dim() == 0  # Scalar
        assert loss >= 0  # KL is non-negative

    def test_kl_with_mask(self):
        """Test KL divergence with mask."""
        loss_fn = KLDivergenceLoss(temperature=1.0)

        student_logits = torch.randn(2, 64, 100)
        teacher_logits = torch.randn(2, 64, 100)
        mask = torch.ones(2, 64)
        mask[:, 32:] = 0  # Mask second half

        loss = loss_fn(student_logits, teacher_logits, mask)
        assert not torch.isnan(loss)

    def test_distillation_loss(self):
        """Test combined distillation loss."""
        loss_fn = DistillationLoss(temperature=2.0, alpha=0.5)

        student_logits = torch.randn(2, 64, 100)
        teacher_logits = torch.randn(2, 64, 100)
        labels = torch.randint(0, 100, (2, 64))

        loss, metrics = loss_fn(student_logits, teacher_logits, labels)

        assert loss.dim() == 0
        assert "kd_loss" in metrics
        assert "ce_loss" in metrics

    def test_forward_reverse_kl(self):
        """Test forward and reverse KL."""
        forward_kl = ForwardKL(temperature=1.0)
        reverse_kl = ReverseKL(temperature=1.0)

        student_logits = torch.randn(2, 64, 100)
        teacher_logits = torch.randn(2, 64, 100)

        fwd_loss = forward_kl(student_logits, teacher_logits)
        rev_loss = reverse_kl(student_logits, teacher_logits)

        assert fwd_loss >= 0
        assert rev_loss >= 0


class TestStageConfig:
    """Tests for StageConfig."""

    def test_default_config(self):
        """Test default stage configuration."""
        config = StageConfig(name="test")
        assert config.name == "test"
        assert config.learning_rate == 4e-3
        assert config.weight_decay == 0.1

    def test_custom_config(self):
        """Test custom stage configuration."""
        config = StageConfig(
            name="pretrain",
            num_steps=100000,
            learning_rate=1e-4,
            seq_len=4096,
        )
        assert config.num_steps == 100000
        assert config.learning_rate == 1e-4
        assert config.seq_len == 4096


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
