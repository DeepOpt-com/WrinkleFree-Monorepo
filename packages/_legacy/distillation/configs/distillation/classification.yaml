# Distillation settings for classification tasks
# BitDistill paper (arxiv.org/abs/2510.13998): Equation 11, 13

# Loss weights
lambda_logits: 10.0      # Weight for logits distillation (KL divergence)
gamma_attention: 1.0e-5  # Weight for attention distillation

# Temperature for logits distillation
# Higher = softer distributions = more information transfer
temperature: 5.0

# Attention distillation settings (BitDistill Equation 11)
attention:
  enabled: true
  use_relation_distill: true  # Use AÂ·A^T relation matrices (BitDistill)
  distill_layer: -1           # Single layer (last), paper recommends for optimization flexibility
  alpha: 1.0
