# Distillation settings for summarization/generation tasks
# BitDistill paper (arxiv.org/abs/2510.13998): Equation 11, 13

# Loss weights (different from classification)
lambda_logits: 1.0       # Lower weight for generation tasks
gamma_attention: 1.0e-3  # Higher attention weight for generation

# Temperature
temperature: 5.0

# Attention distillation settings (BitDistill Equation 11)
attention:
  enabled: true
  use_relation_distill: true  # Use AÂ·A^T relation matrices (BitDistill)
  distill_layer: -1           # Single layer (last), paper recommends for optimization flexibility
  alpha: 1.0
