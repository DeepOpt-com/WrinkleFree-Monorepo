# SmolLM2 135M - DLM Conversion Config
#
# Small model for fast iteration and testing.
# Good for validating the conversion pipeline.

name: smollm2_135m

# Source model (from WrinkleFree-1.58Quant)
source_pretrained: HuggingFaceTB/SmolLM2-135M
bitnet_checkpoint: null  # Set via CLI

# Architecture
hidden_size: 576
num_hidden_layers: 30
num_attention_heads: 9
num_kv_heads: 3
intermediate_size: 1536
vocab_size: 49152
max_position_embeddings: 8192

# Block diffusion parameters (can override global)
block_size: 32
diffusion_steps: 8

# Conversion settings
conversion:
  preserve_quantization: true
  add_noise_embedding: true
  modify_attention_mask: true
