# Diffusion Fine-tuning Configuration
#
# Training settings for converting BitNet to DLM.
# Based on Fast-dLLM v2 official training script.

stage: finetune

# Training parameters - MATCHING OFFICIAL FAST-DLLM V2 SCRIPT
# Official: batch=256, seq=2048, LR=2e-5
total_tokens: 1_000_000_000       # ~1B tokens (paper claims ~1B fine-tuning)
max_seq_length: 512               # Official script uses 2048, we use 512
batch_size: 12                    # Per-device batch size (increased from 8, ~50% VRAM headroom)
gradient_accumulation_steps: 32   # Effective batch = 12 * 32 = 384

# Effective batch size: 12 * 32 * 512 = 196,608 tokens/step
# Steps for 1B tokens: ~5,086 steps

# Optimizer: MuonClip (Muon + QK-clipping, used in Kimi K2)
# Reference: https://github.com/GAD-cell/muon-clip
optimizer:
  type: muonclip
  lr: 1.0e-3           # Muon LR for hidden weights (50x higher than AdamW)
  lr_adam: 2.0e-5      # Adam LR for embed/head/bias/norm (standard AdamW LR)
  # Note: Always uses separate LRs (unified_lr=False hardcoded in train_dlm.py)
  weight_decay: 0.01
  betas: [0.9, 0.95]
  momentum: 0.95
  enable_clipping: true
  clipping_threshold: 50.0

scheduler:
  type: constant     # Official: constant_with_warmup
  warmup_ratio: 0.0  # Disabled - Muon is inherently stable
  min_lr_ratio: 0.1

# Gradient clipping (ZClip adaptive clipping is enabled by default)
max_grad_norm: 1.0

# BitNet stability settings
# Lambda warmup gradually increases quantization strength from 0 to 1
# Recommended: 5-10% of total steps, or 100-500 steps
quantization_warmup_steps: 500  # 0 = disabled (full quantization from start)

# Diffusion training settings
diffusion:
  mask_ratio: 0.15
  complementary_training: true
  token_shift: true

# Checkpointing
checkpoint:
  save_interval: 200
  keep_last_n: 3
  hub:
    enabled: false
    repo_id: null
    private: true

# Logging
logging:
  log_interval: 100
  wandb:
    enabled: true
    project: wrinklefree-dlm
    tags: [dlm, conversion, finetune]

# Early stopping: stops training if loss plateaus
early_stopping:
  enabled: false  # Enable to stop on plateau
  patience: 5     # Stop after N consecutive checks without improvement
  min_delta: 0.01 # Minimum loss improvement to count as "better"
  min_evals: 10   # Don't stop before this many checks

# Compute settings
compute:
  dtype: bfloat16
  compile_model: true   # PyTorch 2.0+ speedup (~15-20% faster)
  gradient_checkpointing: true  # Reduces memory, allows larger batch
