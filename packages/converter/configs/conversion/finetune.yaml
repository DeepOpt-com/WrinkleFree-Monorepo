# Diffusion Fine-tuning Configuration
#
# Training settings for converting BitNet to DLM.
# Based on Fast-dLLM v2 official training script.

stage: finetune

# Training parameters - MATCHING OFFICIAL FAST-DLLM V2 SCRIPT
# Official: seq=512, batch=1 per device, 1 epoch
total_tokens: 1_000_000_000       # ~1B tokens (paper claims ~1B fine-tuning)
max_seq_length: 512               # Official script uses 512
batch_size: 8                     # Will be auto-probed for H100
gradient_accumulation_steps: 16   # Effective batch = 8 * 16 = 128

# Effective batch size: 8 * 16 * 512 = 65,536 tokens/step
# Steps for 1B tokens: ~15,258 steps

# Optimizer: AdamW (default)
optimizer:
  type: adamw
  lr: 2.0e-5         # Official: 2e-5
  weight_decay: 0.01
  betas: [0.9, 0.95]

scheduler:
  type: constant     # Official: constant_with_warmup
  warmup_ratio: 0.03 # Official: 3% warmup
  min_lr_ratio: 0.1

# Gradient clipping
max_grad_norm: 1.0

# Diffusion training settings
diffusion:
  mask_ratio: 0.15
  complementary_training: true
  token_shift: true

# Checkpointing
checkpoint:
  save_interval: 5000
  keep_last_n: 3
  hub:
    enabled: false
    repo_id: null
    private: true

# Logging
logging:
  log_interval: 100
  wandb:
    enabled: true
    project: wrinklefree-dlm
    tags: [dlm, conversion, finetune]

# Early stopping: stops training if loss plateaus
early_stopping:
  enabled: false  # Enable to stop on plateau
  patience: 5     # Stop after N consecutive checks without improvement
  min_delta: 0.01 # Minimum loss improvement to count as "better"
  min_evals: 10   # Don't stop before this many checks

# Compute settings
compute:
  dtype: bfloat16
  compile_model: true   # PyTorch 2.0+ speedup (~15-20% faster)
  gradient_checkpointing: true  # Reduces memory, allows larger batch
