# Diffusion Fine-tuning Configuration
#
# Training settings for converting BitNet to DLM.
# Based on Fast-dLLM v2 official training script.

stage: finetune

# Training parameters - MATCHING OFFICIAL FAST-DLLM V2 SCRIPT
# Official: batch=256, seq=2048, LR=2e-5
total_tokens: 1_000_000_000       # ~1B tokens (paper claims ~1B fine-tuning)
max_seq_length: 512               # Official script uses 2048, we use 512
batch_size: 8                     # Per-device batch size
gradient_accumulation_steps: 32   # Effective batch = 8 * 32 = 256 (matches paper)

# Effective batch size: 8 * 32 * 512 = 131,072 tokens/step
# Steps for 1B tokens: ~7,629 steps

# Optimizer: AdamW (default)
optimizer:
  type: adamw
  lr: 2.0e-5         # Official: 2e-5
  weight_decay: 0.01
  betas: [0.9, 0.95]

scheduler:
  type: constant     # Official: constant_with_warmup
  warmup_ratio: 0.03 # Official: 3% warmup
  min_lr_ratio: 0.1

# Gradient clipping (ZClip adaptive clipping is enabled by default)
max_grad_norm: 1.0

# BitNet stability settings
# Lambda warmup gradually increases quantization strength from 0 to 1
# Recommended: 5-10% of total steps, or 100-500 steps
quantization_warmup_steps: 500  # 0 = disabled (full quantization from start)

# Diffusion training settings
diffusion:
  mask_ratio: 0.15
  complementary_training: true
  token_shift: true

# Checkpointing
checkpoint:
  save_interval: 5000
  keep_last_n: 3
  hub:
    enabled: false
    repo_id: null
    private: true

# Logging
logging:
  log_interval: 100
  wandb:
    enabled: true
    project: wrinklefree-dlm
    tags: [dlm, conversion, finetune]

# Early stopping: stops training if loss plateaus
early_stopping:
  enabled: false  # Enable to stop on plateau
  patience: 5     # Stop after N consecutive checks without improvement
  min_delta: 0.01 # Minimum loss improvement to count as "better"
  min_evals: 10   # Don't stop before this many checks

# Compute settings
compute:
  dtype: bfloat16
  compile_model: true   # PyTorch 2.0+ speedup (~15-20% faster)
  gradient_checkpointing: true  # Reduces memory, allows larger batch
