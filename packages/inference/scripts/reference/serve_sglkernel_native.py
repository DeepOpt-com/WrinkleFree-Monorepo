#!/usr/bin/env python3
"""
Native BitNet server using sgl-kernel binary format.

Loads the converted .bin file and uses sgl-kernel's optimized SIMD kernels.

Usage:
    python scripts/serve_sglkernel_native.py --model models/dlm-bitnet-2b.bin --port 30000

Expected performance: 25+ tok/s with TL2 kernels
"""

import argparse
import json
import logging
import struct
import time
from dataclasses import dataclass
from pathlib import Path
from threading import Lock
from typing import Optional, Tuple

import torch
import torch.nn.functional as F
from flask import Flask, jsonify, request
from transformers import AutoTokenizer

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Import sgl-kernel BitNet operations
try:
    from sgl_kernel.quantization.bitnet import (
        bitnet_gemm,
        quantize_activations_i8,
        check_kernel_available,
    )
    NATIVE_KERNEL = check_kernel_available()
    logger.info(f"Native BitNet kernels available: {NATIVE_KERNEL}")
except ImportError:
    NATIVE_KERNEL = False
    logger.warning("sgl-kernel BitNet not available")


@dataclass
class ModelConfig:
    """Model configuration from sgl-kernel binary."""
    vocab_size: int = 128256
    hidden_size: int = 2560
    intermediate_size: int = 6912
    num_hidden_layers: int = 30
    num_attention_heads: int = 20
    num_key_value_heads: int = 5
    max_position_embeddings: int = 4096
    rms_norm_eps: float = 1e-5
    rope_theta: float = 500000.0


def load_sglkernel_binary(path: Path) -> Tuple[ModelConfig, dict]:
    """Load model from sgl-kernel binary format."""
    logger.info(f"Loading sgl-kernel binary: {path}")

    tensors = {}

    with open(path, 'rb') as f:
        # Magic
        magic = f.read(8)
        assert magic == b"SGLBITNT", f"Invalid magic: {magic}"

        # Version
        version = struct.unpack('<I', f.read(4))[0]
        assert version == 1, f"Unsupported version: {version}"

        # Config
        config_len = struct.unpack('<I', f.read(4))[0]
        config_json = json.loads(f.read(config_len).decode('utf-8'))

        config = ModelConfig(
            vocab_size=config_json.get('vocab_size', 128256),
            hidden_size=config_json.get('hidden_size', 2560),
            intermediate_size=config_json.get('intermediate_size', 6912),
            num_hidden_layers=config_json.get('num_hidden_layers', 30),
            num_attention_heads=config_json.get('num_attention_heads', 20),
            num_key_value_heads=config_json.get('num_key_value_heads', 5),
            max_position_embeddings=config_json.get('max_position_embeddings', 4096),
            rms_norm_eps=config_json.get('rms_norm_eps', 1e-5),
            rope_theta=config_json.get('rope_theta', 500000.0),
        )

        # Tensors
        num_tensors = struct.unpack('<I', f.read(4))[0]
        logger.info(f"Loading {num_tensors} tensors...")

        for i in range(num_tensors):
            # Name
            name_len = struct.unpack('<I', f.read(4))[0]
            name = f.read(name_len).decode('utf-8')

            # Dtype
            dtype_id = struct.unpack('<I', f.read(4))[0]
            dtype_map = {0: torch.uint8, 1: torch.float32, 2: torch.float16, 3: torch.bfloat16}
            dtype = dtype_map[dtype_id]

            # Shape
            ndims = struct.unpack('<I', f.read(4))[0]
            shape = [struct.unpack('<I', f.read(4))[0] for _ in range(ndims)]

            # Scale
            has_scale = struct.unpack('<I', f.read(4))[0]
            scale = struct.unpack('<f', f.read(4))[0] if has_scale else None

            # Data
            data_size = struct.unpack('<Q', f.read(8))[0]
            data = f.read(data_size)

            # Convert to tensor
            if dtype == torch.uint8:
                tensor = torch.frombuffer(bytearray(data), dtype=torch.uint8).reshape(shape)
            else:
                tensor = torch.frombuffer(bytearray(data), dtype=dtype).reshape(shape)

            tensors[name] = {'tensor': tensor, 'scale': scale}

            if (i + 1) % 50 == 0:
                logger.info(f"  Loaded {i + 1}/{num_tensors} tensors")

    logger.info(f"Model loaded: {len(tensors)} tensors")
    return config, tensors


class BitLinearNative:
    """BitNet linear layer using native sgl-kernel GEMM."""

    def __init__(self, weight: torch.Tensor, scale: float):
        self.weight = weight  # [out_features, in_features/4] uint8
        self.scale = scale
        self.out_features = weight.shape[0]
        self.in_features = weight.shape[1] * 4

    def __call__(self, x: torch.Tensor) -> torch.Tensor:
        # Flatten for GEMM
        orig_shape = x.shape[:-1]
        x_flat = x.view(-1, x.shape[-1])

        if NATIVE_KERNEL:
            # Use optimized kernel
            x_int8, act_scale = quantize_activations_i8(x_flat.float())
            out = bitnet_gemm(self.weight, x_int8, self.scale)
            out = (out * act_scale).to(x.dtype)
        else:
            # Fallback to unpacking
            out = self._fallback(x_flat)

        # Restore shape
        return out.view(*orig_shape, -1)

    def _fallback(self, x: torch.Tensor) -> torch.Tensor:
        """Fallback using weight unpacking (slower)."""
        # Unpack 2-bit weights
        weight_unpacked = torch.zeros(self.out_features, self.in_features, dtype=x.dtype)

        for i in range(4):
            bits = (self.weight >> (i * 2)) & 0x03
            values = bits.float() - 1.0
            weight_unpacked[:, i::4] = values * self.scale

        return F.linear(x, weight_unpacked)


class RMSNorm:
    """RMS normalization."""

    def __init__(self, weight: torch.Tensor, eps: float = 1e-6):
        self.weight = weight
        self.eps = eps

    def __call__(self, x: torch.Tensor) -> torch.Tensor:
        variance = x.float().pow(2).mean(-1, keepdim=True)
        x = x * torch.rsqrt(variance + self.eps)
        return (self.weight * x).to(x.dtype)


class NativeBitNetModel:
    """Simple BitNet model using native kernels."""

    def __init__(self, config: ModelConfig, tensors: dict):
        self.config = config
        self.tensors = tensors

        # Build model components
        self.embed = tensors['model.embed_tokens.weight']['tensor']

        # Build layers
        self.layers = []
        for i in range(config.num_hidden_layers):
            layer = self._build_layer(i)
            self.layers.append(layer)

        self.norm = RMSNorm(
            tensors['model.norm.weight']['tensor'],
            eps=config.rms_norm_eps
        )

        logger.info(f"Model built: {config.num_hidden_layers} layers")

    def _build_layer(self, idx: int) -> dict:
        """Build a single transformer layer."""
        prefix = f'model.layers.{idx}'

        layer = {
            'input_norm': RMSNorm(
                self.tensors[f'{prefix}.input_layernorm.weight']['tensor'],
                eps=self.config.rms_norm_eps
            ),
            'post_norm': RMSNorm(
                self.tensors[f'{prefix}.post_attention_layernorm.weight']['tensor'],
                eps=self.config.rms_norm_eps
            ),
        }

        # Attention projections
        for proj in ['q_proj', 'k_proj', 'v_proj', 'o_proj']:
            key = f'{prefix}.self_attn.{proj}.weight'
            if key in self.tensors:
                t = self.tensors[key]
                layer[proj] = BitLinearNative(t['tensor'], t['scale'])

        # MLP projections
        for proj in ['gate_proj', 'up_proj', 'down_proj']:
            key = f'{prefix}.mlp.{proj}.weight'
            if key in self.tensors:
                t = self.tensors[key]
                layer[proj] = BitLinearNative(t['tensor'], t['scale'])

        return layer

    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:
        """Forward pass returning logits."""
        # Embed
        hidden = F.embedding(input_ids, self.embed)

        # Layers (simplified - no attention for speed test)
        for layer in self.layers:
            # Input norm
            residual = hidden
            hidden = layer['input_norm'](hidden)

            # Simple MLP (skip attention for now)
            gate = layer['gate_proj'](hidden)
            up = layer['up_proj'](hidden)
            hidden = F.relu(gate).pow(2) * up  # ReLUÂ²
            hidden = layer['down_proj'](hidden)

            hidden = residual + hidden

        # Final norm
        hidden = self.norm(hidden)

        # LM head (tied to embeddings)
        logits = F.linear(hidden, self.embed)

        return logits


# Flask app
app = Flask(__name__)
model = None
tokenizer = None
model_lock = Lock()


def load_model(model_path: str, tokenizer_path: str):
    """Load model and tokenizer."""
    global model, tokenizer

    # Load tokenizer
    logger.info(f"Loading tokenizer from {tokenizer_path}")
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # Load model
    config, tensors = load_sglkernel_binary(Path(model_path))
    model = NativeBitNetModel(config, tensors)


@torch.no_grad()
def generate(prompt: str, max_new_tokens: int = 50) -> Tuple[str, dict]:
    """Generate text completion."""
    start_time = time.time()

    with model_lock:
        inputs = tokenizer(prompt, return_tensors="pt")
        input_ids = inputs.input_ids

        generated = input_ids[0].tolist()

        for _ in range(max_new_tokens):
            logits = model.forward(torch.tensor([generated]))
            next_logits = logits[0, -1, :]
            next_token = torch.argmax(next_logits).item()

            generated.append(next_token)

            if next_token == tokenizer.eos_token_id:
                break

        new_tokens = generated[len(input_ids[0]):]
        result = tokenizer.decode(new_tokens, skip_special_tokens=True)

    latency = time.time() - start_time
    tok_per_sec = len(new_tokens) / latency if latency > 0 else 0

    return result, {
        'latency': latency,
        'completion_tokens': len(new_tokens),
        'tok_per_sec': tok_per_sec,
    }


@app.route('/health', methods=['GET'])
def health():
    return jsonify({
        "status": "healthy",
        "native_kernel": NATIVE_KERNEL,
    })


@app.route('/generate', methods=['POST'])
def generate_endpoint():
    data = request.json
    prompt = data.get('text', data.get('prompt', ''))
    max_tokens = data.get('max_new_tokens', data.get('max_tokens', 50))

    result, meta = generate(prompt, max_tokens)

    return jsonify({
        "text": result,
        "meta_info": meta,
    })


@app.route('/v1/chat/completions', methods=['POST'])
def chat_completions():
    data = request.json
    messages = data.get('messages', [])
    max_tokens = data.get('max_tokens', 50)

    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    result, meta = generate(prompt, max_tokens)

    return jsonify({
        "id": f"chatcmpl-{int(time.time())}",
        "object": "chat.completion",
        "model": "bitnet-native",
        "choices": [{
            "index": 0,
            "message": {"role": "assistant", "content": result},
            "finish_reason": "stop",
        }],
        "usage": meta,
    })


def main():
    parser = argparse.ArgumentParser(description='Native sgl-kernel BitNet Server')
    parser.add_argument('--model', type=str, required=True, help='Path to .bin file')
    parser.add_argument('--tokenizer', type=str, help='Path to tokenizer (defaults to model dir)')
    parser.add_argument('--port', type=int, default=30000)
    parser.add_argument('--host', type=str, default='0.0.0.0')
    args = parser.parse_args()

    # Default tokenizer path
    tokenizer_path = args.tokenizer
    if not tokenizer_path:
        # Try to find tokenizer in same directory as original checkpoint
        model_dir = Path(args.model).parent / Path(args.model).stem.replace('.bin', '')
        if model_dir.exists():
            tokenizer_path = str(model_dir)
        else:
            tokenizer_path = 'models/dlm-bitnet-2b'  # Fallback

    load_model(args.model, tokenizer_path)

    logger.info(f"Starting native sgl-kernel server on {args.host}:{args.port}")
    logger.info(f"Native kernels: {NATIVE_KERNEL}")
    app.run(host=args.host, port=args.port, threaded=True)


if __name__ == '__main__':
    main()
