# SkyPilot configuration for 32B model conversion on GCP
# GCP n2d-standard-48 has 192GB RAM for model conversion
#
# Launch:
#   sky launch skypilot/benchmark/gcp_convert_32b_benchmark.yaml -c convert-32b

name: bench-32b-gcp

resources:
  cloud: gcp
  instance_type: n2d-highmem-32
  region: us-central1
  disk_size: 300
  ports: 8080

envs:
  MODEL_REPO: Qwen/Qwen2.5-32B-Instruct
  MODEL_NAME: Qwen2.5-32B-Instruct
  QUANT_TYPE: i2_s
  NUM_THREADS: "32"
  CTX_SIZE: "2048"
  PORT: "8080"
  HF_HUB_ENABLE_HF_TRANSFER: "1"

setup: |
  set -ex
  sudo apt-get update && sudo apt-get install -y clang cmake git curl

  if [ -d /opt/inference-engine ]; then
    cd /opt/inference-engine
    git pull || true
    git submodule update --init --recursive || true
  else
    sudo git clone --recurse-submodules https://github.com/DeepOpt-com/WrinkleFree-Inference-Engine.git /opt/inference-engine
    sudo chown -R $USER:$USER /opt/inference-engine
  fi

  cd /opt/inference-engine

  pip install uv huggingface_hub hf_transfer safetensors torch
  uv sync --extra benchmark

  cd extern/BitNet
  pip install -r requirements.txt

  cmake -B build -DCMAKE_BUILD_TYPE=Release
  cmake --build build --config Release -j32

  mkdir -p /tmp/results
  mkdir -p models/$MODEL_NAME

run: |
  set -ex
  cd /opt/inference-engine/extern/BitNet

  echo "=== Step 1: Download $MODEL_NAME ==="
  huggingface-cli download $MODEL_REPO --local-dir models/$MODEL_NAME --include "*.safetensors" "*.json" "*.model" "*.txt"

  echo "=== Step 2: Patch config.json ==="
  cd models/$MODEL_NAME
  cp config.json config.json.bak
  python3 -c "import json; c=json.load(open('config.json')); c['architectures']=['BitnetForCausalLM']; c['model_type']='bitnet'; json.dump(c,open('config.json','w'),indent=2); print('Patched config.json')"

  cd /opt/inference-engine/extern/BitNet

  echo "=== Step 3: Convert to GGUF ==="
  python utils/convert-hf-to-gguf-bitnet.py models/$MODEL_NAME --outtype $QUANT_TYPE --outfile models/$MODEL_NAME/ggml-model-$QUANT_TYPE.gguf

  MODEL_PATH="models/$MODEL_NAME/ggml-model-$QUANT_TYPE.gguf"
  ls -lh $MODEL_PATH

  echo "=== Step 4: Start server ==="
  ./build/bin/llama-server -m "$MODEL_PATH" -c $CTX_SIZE -t $NUM_THREADS --host 0.0.0.0 --port $PORT &
  SERVER_PID=$!

  echo "Waiting for server..."
  for i in $(seq 1 300); do
    if curl -sf http://localhost:$PORT/health > /dev/null 2>&1; then
      echo "Server ready!"
      break
    fi
    [ $i -eq 300 ] && { echo "ERROR: Timeout"; exit 1; }
    sleep 2
  done

  echo "=== Step 5: Run benchmark ==="
  cd /opt/inference-engine
  source .venv/bin/activate
  python scripts/benchmark_cost.py --url http://localhost:$PORT --hardware cpu_32 --model qwen2.5-32b-bitnet --model-size 32B --quantization naive --output-dir /tmp/results --duration 60 --full

  cat /tmp/results/*.json 2>/dev/null || echo "No results"
  kill $SERVER_PID 2>/dev/null || true
