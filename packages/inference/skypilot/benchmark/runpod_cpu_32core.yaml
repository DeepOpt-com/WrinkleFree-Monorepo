# SkyPilot configuration for RunPod 32-core CPU benchmarking
#
# Launch:
#   sky launch skypilot/benchmark/runpod_cpu_32core.yaml -c benchmark-cpu32

name: bench-cpu32

resources:
  cloud: runpod
  cpus: 32+
  memory: 128+
  disk_size: 100
  use_spot: false
  ports: 8080

envs:
  MODEL_REPO: microsoft/BitNet-b1.58-2B-4T
  QUANT_TYPE: i2_s
  NUM_THREADS: "32"
  CTX_SIZE: "4096"
  PORT: "8080"
  BENCHMARK_MODE: "true"
  RESULTS_DIR: /results

setup: |
  set -ex

  apt-get update && apt-get install -y clang cmake git curl

  if [ -d /opt/inference-engine ]; then
    cd /opt/inference-engine && git pull && git submodule update --init --recursive
  else
    git clone --recurse-submodules https://github.com/DeepOpt-com/WrinkleFree-Inference-Engine.git /opt/inference-engine
  fi

  cd /opt/inference-engine
  pip install uv
  uv sync --extra benchmark

  cd extern/BitNet
  mkdir -p models/BitNet-b1.58-2B-4T
  python -c "
  from huggingface_hub import hf_hub_download
  import shutil, os
  p = hf_hub_download('microsoft/bitnet-b1.58-2B-4T-gguf', 'ggml-model-i2_s.gguf')
  t = 'models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf'
  os.makedirs(os.path.dirname(t), exist_ok=True)
  shutil.copy(p, t)
  "

  pip install -r requirements.txt
  export LLAMA_AVX512=1
  python setup_env.py --hf-repo $MODEL_REPO -q $QUANT_TYPE
  mkdir -p /results

run: |
  set -ex
  cd /opt/inference-engine/extern/BitNet

  MODEL_PATH="models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf"

  echo "=== BitNet CPU-32 Benchmark ==="

  ./build/bin/llama-server \
    -m "$MODEL_PATH" \
    -c $CTX_SIZE \
    -t $NUM_THREADS \
    --host 0.0.0.0 \
    --port $PORT &

  sleep 60

  cd /opt/inference-engine
  python scripts/benchmark_cost.py \
    --url http://localhost:$PORT \
    --hardware cpu_32 \
    --model bitnet-2b-4t \
    --output-dir $RESULTS_DIR

  echo "Benchmark complete."
