# SkyPilot configuration for ARM wf_server Benchmark
# Tests native Rust BitNet inference with ARM NEON on GCP Tau T2A
#
# Instance: GCP t2a-standard-8 (Ampere Altra ARM64, 8 vCPUs, 32 GB RAM)
# Cost: ~$0.28/hour
#
# Launch:
#   sky launch packages/inference/skypilot/benchmark/arm_wf_server.yaml -c arm-wf -y
#
# Re-run with code changes:
#   sky exec arm-wf --workdir packages/inference
#
# Monitor:
#   sky logs arm-wf --follow

name: arm-wf-server

resources:
  cloud: gcp
  instance_type: t2a-standard-8  # Ampere Altra ARM64
  region: us-central1
  disk_size: 150
  use_spot: false

envs:
  # Default: SmolLM2-135M for quick tests (override with MODEL_REPO/MODEL_NAME)
  MODEL_REPO: HuggingFaceTB/SmolLM2-135M
  MODEL_NAME: SmolLM2-135M
  # Threading - t2a-standard-8 has 8 vCPUs
  RAYON_NUM_THREADS: "8"
  OMP_NUM_THREADS: "8"
  CMAKE_BUILD_PARALLEL_LEVEL: "8"
  # Debug
  RUST_BACKTRACE: "1"
  HF_HUB_ENABLE_HF_TRANSFER: "1"

file_mounts:
  /opt/inference: .

setup: |
  set -ex
  echo "============================================"
  echo "ARM wf_server Benchmark Setup"
  echo "Instance: GCP t2a-standard-8 (Ampere Altra)"
  echo "Architecture: $(uname -m)"
  echo "============================================"

  # Verify we're on ARM
  if [ "$(uname -m)" != "aarch64" ]; then
    echo "ERROR: This is not an ARM64 instance!"
    exit 1
  fi

  # System dependencies
  sudo apt-get update && sudo apt-get install -y \
    build-essential pkg-config libssl-dev curl git cmake \
    clang python3-pip protobuf-compiler

  # Install Rust
  if ! command -v cargo &> /dev/null; then
    curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y
    source "$HOME/.cargo/env"
  fi

  # Verify Rust target is ARM
  source "$HOME/.cargo/env"
  echo "Rust target: $(rustc -vV | grep host)"
  if ! rustc -vV | grep -q "host: aarch64"; then
    echo "ERROR: Rust is not targeting ARM64!"
    exit 1
  fi

  # Python packages for model download/conversion
  pip install --upgrade pip
  pip install "huggingface_hub[cli]" hf_transfer safetensors torch numpy "transformers>=4.40.0" sentencepiece gguf

  # Directories
  sudo mkdir -p /results /models
  sudo chown $(whoami) /results /models

  # Build llama.cpp for ARM (for GGUF conversion tools)
  echo "=== Building llama.cpp for ARM ==="
  cd /opt/inference/extern/llama.cpp
  cmake -B build -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=ON \
    -DGGML_NATIVE=ON \
    -DCMAKE_C_FLAGS="-march=native" -DCMAKE_CXX_FLAGS="-march=native"
  cmake --build build -j${CMAKE_BUILD_PARALLEL_LEVEL}
  ls -lh build/bin/llama-cli build/bin/llama-quantize

  # Build wf_server for ARM
  echo "=== Building wf_server (pure Rust, ARM NEON) ==="
  cd /opt/inference/rust
  cargo build --release --bin wf_server --features native-inference
  ls -lh target/release/wf_server

  # Show CPU capabilities detected
  echo "=== CPU Info ==="
  lscpu | grep -E "(Model name|Architecture|CPU\(s\)|Flags)"

  echo "=== Setup Complete ==="

run: |
  set -ex
  source "$HOME/.cargo/env"

  # Use sky_workdir for live code sync
  LLAMA_CPP="/opt/inference/extern/llama.cpp"
  WF_SERVER_WORKDIR="$HOME/sky_workdir/rust"
  WF_SERVER_FALLBACK="/opt/inference/rust"

  # Rebuild wf_server if code changed (sky_workdir is synced on sky exec)
  if [ -d "$WF_SERVER_WORKDIR" ]; then
    echo "=== Rebuilding wf_server from synced workdir ==="
    cd "$WF_SERVER_WORKDIR"
    cargo build --release --bin wf_server --features native-inference
    WF_SERVER="$WF_SERVER_WORKDIR/target/release/wf_server"
  else
    WF_SERVER="$WF_SERVER_FALLBACK/target/release/wf_server"
  fi
  ls -lh "$WF_SERVER"

  echo "============================================"
  echo "ARM wf_server Benchmark"
  echo "Architecture: $(uname -m)"
  echo "Model: $MODEL_REPO"
  echo "Threads: RAYON=$RAYON_NUM_THREADS"
  echo "============================================"

  # Step 1: Download model
  echo "=== Step 1: Downloading $MODEL_NAME ==="
  if [ ! -d /models/$MODEL_NAME ]; then
    python3 -c '
from huggingface_hub import snapshot_download
import os
snapshot_download(
    os.environ["MODEL_REPO"],
    local_dir="/models/" + os.environ["MODEL_NAME"],
    allow_patterns=["*.safetensors", "*.json", "*.model", "*.txt", "*.tiktoken"]
)
'
    ls -lh /models/$MODEL_NAME/
  else
    echo "Model already downloaded"
  fi

  # Step 2: Convert to GGUF F16
  echo "=== Step 2: Converting to GGUF F16 ==="
  if [ ! -f /models/$MODEL_NAME/model-f16.gguf ]; then
    cd /opt/inference
    python3 $LLAMA_CPP/convert_hf_to_gguf.py /models/$MODEL_NAME \
      --outfile /models/$MODEL_NAME/model-f16.gguf \
      --outtype f16
    ls -lh /models/$MODEL_NAME/model-f16.gguf
  else
    echo "F16 GGUF already exists"
  fi

  # Step 3: Quantize to I2_S (recommended for BitNet)
  echo "=== Step 3: Quantizing to I2_S ==="
  if [ ! -f /models/$MODEL_NAME/model-i2s.gguf ]; then
    $LLAMA_CPP/build/bin/llama-quantize \
      /models/$MODEL_NAME/model-f16.gguf \
      /models/$MODEL_NAME/model-i2s.gguf \
      I2_S
    ls -lh /models/$MODEL_NAME/model-i2s.gguf
  else
    echo "I2_S GGUF already exists"
  fi

  MODEL_PATH="/models/$MODEL_NAME/model-i2s.gguf"
  echo "Model file: $(ls -lh $MODEL_PATH)"

  # Step 4: Baseline with llama.cpp (ARM NEON)
  echo "=== Step 4: llama.cpp ARM Baseline ==="
  echo "The quick brown fox jumps over the lazy dog. " | head -c 2000 > /tmp/test_prompt.txt

  echo "--- llama.cpp I2_S baseline (ARM NEON) ---"
  $LLAMA_CPP/build/bin/llama-cli \
    -m $MODEL_PATH \
    -f /tmp/test_prompt.txt \
    -n 1 \
    -t $OMP_NUM_THREADS \
    -c 2048 \
    --temp 0 \
    --no-display-prompt 2>&1 | grep -E "(prompt eval|model size)"

  # Step 5: wf_server benchmark (ARM NEON)
  echo ""
  echo "=== Step 5: wf_server Benchmark (ARM NEON) ==="
  echo "Running native Rust inference engine with ARM NEON kernels..."

  "$WF_SERVER" \
    --model-path $MODEL_PATH \
    --context-len 2048 \
    --benchmark \
    --benchmark-iterations 10 \
    --benchmark-max-tokens 64

  # Step 6: Thread scaling test
  echo ""
  echo "=== Step 6: Thread Scaling ==="
  for threads in 1 2 4 8; do
    echo "--- Threads: $threads ---"
    RAYON_NUM_THREADS=$threads "$WF_SERVER" \
      --model-path $MODEL_PATH \
      --context-len 2048 \
      --benchmark \
      --benchmark-iterations 3 \
      --benchmark-max-tokens 32 2>&1 | grep -E "(throughput|prefill|decode)"
  done

  # Step 7: Summary
  echo ""
  echo "============================================"
  echo "ARM BENCHMARK COMPLETE"
  echo "============================================"
  echo "Architecture: $(uname -m)"
  echo "CPU: $(lscpu | grep 'Model name' | cut -d: -f2 | xargs)"
  echo "Model: $MODEL_NAME (I2_S format)"
  echo "Model size: $(ls -lh $MODEL_PATH | awk '{print $5}')"
  echo ""
  echo "Compare wf_server ARM NEON vs x86 AVX2/AVX512 results."
