# SkyPilot configuration for ARM wf_server Benchmark
# Tests native Rust BitNet inference with ARM NEON + dotprod on GCP Tau T2A
#
# Instance: GCP t2a-standard-8 (Ampere Altra ARM64, 8 vCPUs, 32 GB RAM)
# Cost: ~$0.28/hour
#
# Features:
#   - Uses Rust nightly for ARMv8.2+ dotprod extension (vdotq_s32)
#   - ~4x faster GEMV kernels vs standard NEON
#   - Sanity check with BitNet-b1.58-2B to verify output quality
#
# Launch:
#   sky launch packages/inference/skypilot/benchmark/arm_wf_server.yaml -c arm-wf -y
#
# Re-run with code changes:
#   sky exec arm-wf --workdir packages/inference
#
# Monitor:
#   sky logs arm-wf --follow

name: arm-wf-server

resources:
  cloud: gcp
  instance_type: t2a-standard-8  # Ampere Altra ARM64
  region: us-central1
  disk_size: 150
  use_spot: false
  # ARM64 Ubuntu image - required for ARM instances
  image_id: projects/ubuntu-os-cloud/global/images/ubuntu-2204-jammy-arm64-v20251218

envs:
  # Default: SmolLM2-135M for quick tests (override with MODEL_REPO/MODEL_NAME)
  MODEL_REPO: HuggingFaceTB/SmolLM2-135M
  MODEL_NAME: SmolLM2-135M
  # Threading - t2a-standard-8 has 8 vCPUs
  RAYON_NUM_THREADS: "8"
  OMP_NUM_THREADS: "8"
  CMAKE_BUILD_PARALLEL_LEVEL: "8"
  # Debug
  RUST_BACKTRACE: "1"
  HF_HUB_ENABLE_HF_TRANSFER: "1"

file_mounts:
  /opt/inference: .

setup: |
  set -ex
  echo "============================================"
  echo "ARM wf_server Benchmark Setup"
  echo "Instance: GCP t2a-standard-8 (Ampere Altra)"
  echo "Architecture: $(uname -m)"
  echo "============================================"

  # Verify we're on ARM
  if [ "$(uname -m)" != "aarch64" ]; then
    echo "ERROR: This is not an ARM64 instance!"
    exit 1
  fi

  # System dependencies
  sudo apt-get update && sudo apt-get install -y \
    build-essential pkg-config libssl-dev curl git cmake \
    clang python3-pip protobuf-compiler

  # Install Rust nightly (required for ARMv8.2+ dotprod intrinsics)
  if ! command -v cargo &> /dev/null; then
    curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y --default-toolchain nightly
    source "$HOME/.cargo/env"
  else
    source "$HOME/.cargo/env"
    rustup default nightly
  fi

  # Verify Rust nightly is active and targeting ARM
  echo "Rust toolchain: $(rustup show active-toolchain)"
  echo "Rust target: $(rustc -vV | grep host)"
  if ! rustc -vV | grep -q "host: aarch64"; then
    echo "ERROR: Rust is not targeting ARM64!"
    exit 1
  fi

  # Python packages for model download/conversion
  pip install --upgrade pip
  pip install "huggingface_hub[cli]" hf_transfer safetensors torch numpy "transformers>=4.40.0" sentencepiece gguf

  # Directories
  sudo mkdir -p /results /models
  sudo chown $(whoami) /results /models

  # Clone llama.cpp (excluded from file_mounts by .gitignore)
  echo "=== Cloning llama.cpp ==="
  mkdir -p /opt/inference/extern
  if [ ! -d /opt/inference/extern/llama.cpp ]; then
    git clone --depth 1 https://github.com/ggml-org/llama.cpp.git /opt/inference/extern/llama.cpp
  fi

  # Build llama.cpp for ARM (for GGUF conversion tools)
  echo "=== Building llama.cpp for ARM ==="
  cd /opt/inference/extern/llama.cpp
  cmake -B build -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=ON \
    -DGGML_NATIVE=ON -DLLAMA_CURL=OFF \
    -DCMAKE_C_FLAGS="-march=native" -DCMAKE_CXX_FLAGS="-march=native"
  cmake --build build -j${CMAKE_BUILD_PARALLEL_LEVEL}
  ls -lh build/bin/llama-cli build/bin/llama-quantize

  # Build wf_server for ARM with dotprod support
  echo "=== Building wf_server (pure Rust, ARM NEON + dotprod) ==="
  cd /opt/inference/rust
  cargo +nightly build --release --bin wf_server --features native-inference
  ls -lh target/release/wf_server

  # Show CPU capabilities detected
  echo "=== CPU Info ==="
  lscpu | grep -E "(Model name|Architecture|CPU\(s\)|Flags)"

  echo "=== Setup Complete ==="

run: |
  set -ex
  source "$HOME/.cargo/env"

  # Use sky_workdir for live code sync
  LLAMA_CPP="/opt/inference/extern/llama.cpp"
  WF_SERVER_WORKDIR="$HOME/sky_workdir/rust"
  WF_SERVER_FALLBACK="/opt/inference/rust"

  # Rebuild wf_server if code changed (sky_workdir is synced on sky exec)
  if [ -d "$WF_SERVER_WORKDIR" ]; then
    echo "=== Rebuilding wf_server from synced workdir (nightly + dotprod) ==="
    cd "$WF_SERVER_WORKDIR"
    cargo +nightly build --release --bin wf_server --features native-inference
    WF_SERVER="$WF_SERVER_WORKDIR/target/release/wf_server"
  else
    WF_SERVER="$WF_SERVER_FALLBACK/target/release/wf_server"
  fi
  ls -lh "$WF_SERVER"

  echo "============================================"
  echo "ARM wf_server Benchmark"
  echo "Architecture: $(uname -m)"
  echo "Threads: RAYON=$RAYON_NUM_THREADS"
  echo "============================================"

  # Step 1: Download BitNet-2B model (pre-quantized ternary model)
  # Note: wf_server is designed for BitNet ternary models, not regular FP16/BF16 models
  echo "=== Step 1: Downloading BitNet-b1.58-2B-4T ==="
  BITNET_MODEL_DIR="/models/BitNet-2B"
  BITNET_MODEL_PATH="$BITNET_MODEL_DIR/ggml-model-i2_s.gguf"

  if [ ! -f "$BITNET_MODEL_PATH" ]; then
    mkdir -p "$BITNET_MODEL_DIR"
    huggingface-cli download microsoft/BitNet-b1.58-2B-4T-gguf \
      ggml-model-i2_s.gguf \
      --local-dir "$BITNET_MODEL_DIR"
  fi
  ls -lh "$BITNET_MODEL_PATH"
  echo "Model file: $(ls -lh $BITNET_MODEL_PATH)"

  # Step 2: wf_server benchmark (ARM NEON + dotprod)
  echo ""
  echo "=== Step 2: wf_server Benchmark (ARM NEON) ==="
  echo "Running native Rust BitNet inference engine with ARM NEON kernels..."

  "$WF_SERVER" \
    --model-path "$BITNET_MODEL_PATH" \
    --context-len 2048 \
    --benchmark \
    --benchmark-iterations 10 \
    --benchmark-max-tokens 64

  # Step 3: Thread scaling test
  echo ""
  echo "=== Step 3: Thread Scaling ==="
  for threads in 1 2 4 8; do
    echo "--- Threads: $threads ---"
    RAYON_NUM_THREADS=$threads "$WF_SERVER" \
      --model-path "$BITNET_MODEL_PATH" \
      --context-len 2048 \
      --benchmark \
      --benchmark-iterations 3 \
      --benchmark-max-tokens 32 2>&1 | grep -E "(throughput|prefill|decode)"
  done

  # Step 4: Sanity Check - Text Generation (via benchmark with custom prompt)
  echo ""
  echo "=== Step 4: Sanity Check - Text Generation ==="
  echo "Running benchmark with custom prompt to verify output quality..."
  echo ""

  # The benchmark mode includes a sanity check that shows generated text
  "$WF_SERVER" \
    --model-path "$BITNET_MODEL_PATH" \
    --context-len 2048 \
    --benchmark \
    --benchmark-iterations 1 \
    --benchmark-max-tokens 64 \
    --benchmark-prompt "The capital of France is"

  # Step 5: Summary
  echo ""
  echo "============================================"
  echo "ARM BENCHMARK COMPLETE"
  echo "============================================"
  echo "Architecture: $(uname -m)"
  echo "CPU: $(lscpu | grep 'Model name' | cut -d: -f2 | xargs)"
  echo "Rust toolchain: $(rustup show active-toolchain)"
  echo "Model: BitNet-b1.58-2B-4T (I2_S format)"
  echo "Model size: $(ls -lh $BITNET_MODEL_PATH | awk '{print $5}')"
  echo ""
  echo "Using ARMv8.2+ dotprod extension (vdotq_s32) for ~4x faster GEMV."
  echo "Compare wf_server ARM NEON+dotprod vs x86 AVX2/AVX512 results."
