# Fairy2 W2 (2-stage) training configuration
# ~2 bits per weight, recommended for best quality

stage: fairy2_qat
mode: w2

# Quantization settings
quantization:
  num_stages: 2  # W2 = 2 stages = ~2 bits per weight
  use_axis_scaling: true

# Optimizer (AdamW per Fairy2i paper)
optimizer:
  type: adamw
  lr: 1e-4
  weight_decay: 0.01
  betas: [0.9, 0.95]

# Scheduler (Warmup-Stable-Decay)
scheduler:
  type: wsd
  warmup_steps: 500
  decay_ratio: 0.1

# Training settings
max_steps: 10000
max_seq_length: 512
batch_size: 8
gradient_accumulation_steps: 8
gradient_clipping: 1.0

# Logging
log_interval: 100
logging:
  wandb:
    enabled: true
    project: wrinklefree-fairy2
    tags: [fairy2, w2, qat]

# Checkpointing
save_interval: 2000
output_dir: ./outputs

# Loss settings
ignore_index: -100
label_smoothing: 0.0

# Influence-based data selection (MobileLLM-R1 style)
# Enabled by default with mixed_pretrain data config
influence:
  enabled: true
  update_interval: 1000  # Recalculate mixture weights every N steps
  learning_rate: 0.2     # Learning rate for weight updates
  config:
    lambda_val: 0.1      # Regularization term
    gamma_val: 0.1       # Gamma term
    temperature: 1.0
