# Fairy2i Qwen3-4B Training - 1B tokens on Nebius H100
#
# Full training run with Qwen3-4B model.
#
# Launch:
#   cd WrinkleFree-Deployer && source .venv/bin/activate
#   sky launch skypilot/fairy2_qwen3_4b_train.yaml -y --infra nebius
#
# Monitor:
#   sky logs <cluster-name>
#
# Tokens calculation:
#   batch=8, seq=512, grad_accum=8 → 32,768 tokens/step
#   1B tokens / 32,768 = 30,518 steps

name: fairy2-qwen3-4b

resources:
  cloud: runpod
  accelerators: H100:1
  use_spot: false
  disk_size: 200  # Larger disk for model + checkpoints

# Sync both Fairy2 and CheaperTraining
workdir: ../WrinkleFree-Fairy2

file_mounts:
  # Mount CheaperTraining for data loading
  ~/cheapertraining:
    name: cheapertraining-src
    source: ../WrinkleFree-CheaperTraining
    mode: COPY
  # GCS credentials for checkpointing
  ~/.config/gcloud/application_default_credentials.json: ~/.config/gcloud/application_default_credentials.json

envs:
  # Model and mode
  MODEL: qwen3_4b
  MODE: w2

  # H100 optimized batch settings for full GPU utilization
  # batch=16, seq=2048, grad_accum=4 → 131K tokens/step → ~7,630 steps for 1B tokens
  BATCH_SIZE: "16"
  SEQ_LEN: "2048"
  GRAD_ACCUM: "4"
  MAX_STEPS: "7630"

  # W&B tracking
  WANDB_API_KEY: b95b2998e7ecb4a69680f83796f01a38672baf4a
  WANDB_PROJECT: wrinklefree-fairy2

  # GCS checkpointing (every 500 steps ≈ 65M tokens)
  GCS_BUCKET: wrinklefree-checkpoints
  SAVE_INTERVAL: "500"

  # GCS credentials path
  GOOGLE_APPLICATION_CREDENTIALS: ~/.config/gcloud/application_default_credentials.json

  # Disable fast HF transfer (not installed)
  HF_HUB_ENABLE_HF_TRANSFER: "0"

setup: |
  set -e

  # Install uv
  curl -LsSf https://astral.sh/uv/install.sh | sh
  export PATH="$HOME/.cargo/bin:$PATH"

  # Install Fairy2 first (creates venv)
  echo "Installing Fairy2..."
  cd ~/sky_workdir
  uv sync

  # Install CheaperTraining into Fairy2's venv
  echo "Installing CheaperTraining..."
  uv pip install -e ~/cheapertraining

  # Verify GPU setup
  uv run python -c 'import torch; print(f"PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}, Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else None}")'

  # Verify CheaperTraining import
  uv run python -c 'from fairy2.data import CHEAPERTRAINING_AVAILABLE; print(f"CheaperTraining: {CHEAPERTRAINING_AVAILABLE}")'

  # Pre-download model to avoid timeout during training
  echo "Pre-downloading Qwen3-4B model..."
  uv run python -c "from transformers import AutoModelForCausalLM, AutoTokenizer; AutoTokenizer.from_pretrained('Qwen/Qwen3-4B', trust_remote_code=True); print('Tokenizer downloaded')"

run: |
  set -e
  cd ~/sky_workdir
  export PATH="$HOME/.cargo/bin:$PATH"

  TOKENS_PER_STEP=$((BATCH_SIZE * SEQ_LEN * GRAD_ACCUM))
  TOTAL_TOKENS=$((MAX_STEPS * TOKENS_PER_STEP))

  echo "=============================================="
  echo "Fairy2i Training - Qwen3-4B on H100"
  echo "=============================================="
  echo "Model: ${MODEL}"
  echo "Mode: ${MODE}"
  echo "Batch size: ${BATCH_SIZE}"
  echo "Sequence length: ${SEQ_LEN}"
  echo "Gradient accumulation: ${GRAD_ACCUM}"
  echo "Tokens per step: ${TOKENS_PER_STEP}"
  echo "Total steps: ${MAX_STEPS}"
  echo "Total tokens: ~${TOTAL_TOKENS} (1B)"
  echo "Save interval: ${SAVE_INTERVAL} steps"
  echo "=============================================="

  # Run training with H100-optimized settings
  uv run python scripts/train.py \
    model=${MODEL} \
    training=fairy2_${MODE} \
    training.batch_size=${BATCH_SIZE} \
    training.max_seq_length=${SEQ_LEN} \
    training.gradient_accumulation_steps=${GRAD_ACCUM} \
    training.max_steps=${MAX_STEPS} \
    training.save_interval=${SAVE_INTERVAL} \
    training.influence.enabled=true \
    training.logging.wandb.enabled=true \
    training.logging.wandb.project=${WANDB_PROJECT} \
    gcs.enabled=true \
    gcs.bucket=${GCS_BUCKET}

  echo ""
  echo "=============================================="
  echo "TRAINING COMPLETE!"
  echo "=============================================="
