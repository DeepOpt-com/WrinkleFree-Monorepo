# WrinkleFree Evaluation Job
#
# Launch evaluation on remote GPU:
#   sky launch skypilot/eval.yaml -e MODEL_PATH=HuggingFaceTB/SmolLM2-135M -e BENCHMARK=smoke_test
#
# Launch as managed job (with spot recovery):
#   sky jobs launch skypilot/eval.yaml -e MODEL_PATH=gs://bucket/checkpoint -e BENCHMARK=bitdistill
#
# For DLM (diffusion) models:
#   sky jobs launch skypilot/eval.yaml -e MODEL_PATH=gs://wrinklefree-checkpoints/dlm/... -e USE_DLM=true
#
# Monitor:
#   sky jobs queue
#   sky jobs logs <job_id>

name: wrinklefree-eval

resources:
  accelerators: A40:1
  use_spot: false  # On-demand for reliable evals
  disk_tier: best
  # cloud: runpod  # Commented out - use --infra to specify cloud

# Sync eval code to cluster (monorepo structure)
workdir: /home/lev/code/WrinkleFreeDev/packages/eval

# Note: For GCS model paths (gs://...), use Nebius with --infra nebius --gpus L40S:1
# RunPod doesn't support GCS file_mounts. For HuggingFace models, RunPod works fine.

# GCP credentials for GCS access (required for gs:// model paths)
file_mounts:
  /tmp/gcp-creds.json: /home/lev/code/WrinkleFreeDev/packages/deployer/credentials/gcp-service-account.json

envs:
  # Model configuration (override with -e)
  MODEL_PATH: HuggingFaceTB/SmolLM2-135M
  BENCHMARK: smoke_test
  DTYPE: bfloat16
  BATCH_SIZE: auto

  # DLM (diffusion) model evaluation
  USE_DLM: "false"
  MC_ITERATIONS: "128"

  # GCP credentials for GCS access
  GOOGLE_APPLICATION_CREDENTIALS: /tmp/gcp-creds.json

  # Results storage (disabled - bucket doesn't exist)
  OUTPUT_BUCKET: ""
  OUTPUT_PREFIX: ""

  # Weights & Biases (optional - leave empty to disable)
  WANDB_PROJECT: ""
  WANDB_RUN_ID: ""
  WANDB_API_KEY: ""

  # Enable fast HF downloads
  HF_HUB_ENABLE_HF_TRANSFER: "1"

setup: |
  set -e
  cd ~/sky_workdir

  # Install uv if not present
  if ! command -v uv &> /dev/null; then
    curl -LsSf https://astral.sh/uv/install.sh | sh
    source $HOME/.local/bin/env
  fi

  # Install dependencies (including optional wandb and gcs for GCS access)
  # hf_transfer is now in base dependencies for fast HF downloads
  uv sync --extra wandb --extra gcs

  # Authenticate gsutil with GCP service account (for GCS model access)
  if [ -f /tmp/gcp-creds.json ]; then
    gcloud auth activate-service-account --key-file=/tmp/gcp-creds.json 2>/dev/null || true
  fi

run: |
  set -e
  cd ~/sky_workdir
  source $HOME/.local/bin/env 2>/dev/null || true

  OUTPUT_DIR="/tmp/eval_results"
  mkdir -p $OUTPUT_DIR

  echo "========================================="
  echo "WrinkleFree Evaluation"
  echo "Model: $MODEL_PATH"
  echo "Benchmark: $BENCHMARK"
  echo "Output: $OUTPUT_DIR"
  echo "========================================="

  # Handle GCS paths - download model first if needed
  EVAL_MODEL_PATH="$MODEL_PATH"
  if [[ "$MODEL_PATH" == gs://* ]]; then
    echo "Downloading model from GCS..."

    # Authenticate gsutil if credentials available
    if [ -f /tmp/gcp-creds.json ]; then
      echo "Authenticating with GCP service account..."
      gcloud auth activate-service-account --key-file=/tmp/gcp-creds.json
    fi

    LOCAL_MODEL_DIR="/tmp/model"
    mkdir -p $LOCAL_MODEL_DIR
    # Remove trailing slash from path if present
    GCS_PATH="${MODEL_PATH%/}"
    gsutil -m cp -r "$GCS_PATH/*" $LOCAL_MODEL_DIR/
    EVAL_MODEL_PATH=$LOCAL_MODEL_DIR
    echo "Model downloaded to $EVAL_MODEL_PATH"
  fi

  # Build evaluation command
  EVAL_CMD="uv run python scripts/run_eval.py \
    --model-path $EVAL_MODEL_PATH \
    --benchmark $BENCHMARK \
    --dtype $DTYPE \
    --batch-size $BATCH_SIZE \
    --output-dir $OUTPUT_DIR"

  # Add smoke test flag for quick validation
  if [ "$BENCHMARK" = "smoke_test" ]; then
    EVAL_CMD="$EVAL_CMD --smoke-test"
  fi

  # Add DLM evaluation mode for diffusion models
  if [ "$USE_DLM" = "true" ]; then
    echo "[Eval] Using DLM (diffusion) evaluation mode"
    EVAL_CMD="$EVAL_CMD --use-dlm --mc-iterations $MC_ITERATIONS"
  fi

  # Add W&B logging if configured
  if [ -n "$WANDB_PROJECT" ]; then
    EVAL_CMD="$EVAL_CMD --wandb-project $WANDB_PROJECT"
    if [ -n "$WANDB_RUN_ID" ]; then
      EVAL_CMD="$EVAL_CMD --wandb-run-id $WANDB_RUN_ID"
    fi
  fi

  echo ""
  echo "[Eval] Running evaluation..."
  $EVAL_CMD

  echo ""
  echo "[Eval] Evaluation complete!"

  # Upload results to GCS (if credentials available)
  if [ -n "$OUTPUT_BUCKET" ] && [ -f "$GOOGLE_APPLICATION_CREDENTIALS" ]; then
    echo ""
    echo "[Upload] Uploading results to gs://$OUTPUT_BUCKET/$OUTPUT_PREFIX/"

    # Use upload script if available, otherwise use gsutil directly
    if [ -f scripts/upload_results.py ]; then
      OUTPUT_DIR=$OUTPUT_DIR GCS_BUCKET=$OUTPUT_BUCKET GCS_PREFIX=$OUTPUT_PREFIX \
        uv run python scripts/upload_results.py
    else
      gsutil -m cp -r $OUTPUT_DIR/* gs://$OUTPUT_BUCKET/$OUTPUT_PREFIX/
    fi

    echo "[Upload] Results uploaded!"
  elif [ -n "$OUTPUT_BUCKET" ]; then
    echo ""
    echo "[Upload] Skipping GCS upload - credentials not available"
    echo "[Upload] Results are saved locally at: $OUTPUT_DIR"
  fi

  echo ""
  echo "========================================="
  echo "Evaluation Complete!"
  echo "Results: $OUTPUT_DIR/results.json"
  if [ -n "$OUTPUT_BUCKET" ]; then
    echo "GCS: gs://$OUTPUT_BUCKET/$OUTPUT_PREFIX/"
  fi
  echo "========================================="
