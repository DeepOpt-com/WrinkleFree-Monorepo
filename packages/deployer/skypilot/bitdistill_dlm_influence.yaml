# BitNet + DLM + CE Training with Influence-Based Remixing (1x L40)
#
# SmolLM2-135M training run with:
# - BitNet 1.58-bit quantization (ternary weights {-1, 0, 1})
# - Diffusion Language Model (DLM) objective
# - Standard cross-entropy loss
# - Influence-based dataset weight optimization
#
# Curriculum:
# - First 20%: fineweb-edu warmup (CE only)
# - Remaining 80%: mixed_pretrain with influence (CE + DLM)
#
# Usage:
#   cd packages/deployer
#   source credentials/.env
#   sky launch skypilot/bitdistill_dlm_influence.yaml -y --cluster bitdistill-dlm
#
# Monitor:
#   sky logs bitdistill-dlm
#
# Teardown:
#   sky down bitdistill-dlm -y

name: wrinklefree-bitdistill-dlm-influence

resources:
  accelerators: L40S:1
  memory: 64+
  disk_size: 100
  use_spot: false
  cloud: nebius

workdir: /home/lev/code/WrinkleFreeDevWrapper/Refact

file_mounts:
  /tmp/gcp-creds.json: /home/lev/code/WrinkleFreeDevWrapper/Refact/packages/deployer/credentials/gcp-service-account.json
  /tmp/credentials.env: /home/lev/code/WrinkleFreeDevWrapper/Refact/packages/deployer/credentials/.env
  /tmp/global.env: ~/.config/.env.global

envs:
  GOOGLE_APPLICATION_CREDENTIALS: /tmp/gcp-creds.json
  GCS_BUCKET: wrinklefree-checkpoints
  MODEL: smollm2_135m
  TRAINING_CONFIG: bitdistill_dlm_influence
  TOTAL_TOKENS: 5000000000  # 5B tokens
  MAX_STEPS: 38147  # 5B tokens / (8*8*2048) tokens per step (auto batch, grad_accum=8)
  MUON_LR: 0.01  # Muon LR for 2D weights
  ADAM_LR: 1.5e-4  # AdamW LR for embeddings/biases (halved from 3e-4)
  EXPERIMENT_NAME: bitnet_muon_5b_v2
  WANDB_PROJECT: wrinklefree
  WANDB_RUN_ID: bitdistill-dlm-${SKYPILOT_TASK_ID}
  HF_HUB_ENABLE_HF_TRANSFER: "1"

setup: |
  set -e
  cd ~/sky_workdir

  # Install uv if not present
  if ! command -v uv &> /dev/null; then
    curl -LsSf https://astral.sh/uv/install.sh | sh
  fi
  export PATH="$HOME/.cargo/bin:$PATH"

  # Install gcloud CLI - handle existing broken installations
  export PATH="$HOME/google-cloud-sdk/bin:$PATH"
  if ! gcloud version &> /dev/null; then
    echo "Installing Google Cloud SDK..."
    # Remove any broken existing installation
    rm -rf $HOME/google-cloud-sdk
    curl -sSL https://sdk.cloud.google.com | bash -s -- --disable-prompts --install-dir=$HOME
  fi
  echo "gcloud version: $(gcloud version 2>/dev/null | head -1)"

  # Install dependencies (including muon-clip for single GPU training)
  uv sync --all-packages
  # Install muon-clip from GitHub (required for MuonClip optimizer on single GPU)
  uv pip install git+https://github.com/GAD-cell/muon-clip.git

  # Activate GCS credentials and verify access
  echo "Activating GCS credentials..."
  gcloud auth activate-service-account --key-file=/tmp/gcp-creds.json
  if gsutil ls gs://${GCS_BUCKET}/ > /dev/null 2>&1; then
    echo "GCS access: OK"
  else
    echo "ERROR: GCS access failed!"
    exit 1
  fi

  # Verify WANDB_API_KEY is available
  source /tmp/credentials.env
  source /tmp/global.env
  if [ -z "$WANDB_API_KEY" ]; then
    echo "ERROR: WANDB_API_KEY not set!"
    exit 1
  fi
  echo "WANDB_API_KEY: configured (length=${#WANDB_API_KEY})"

  echo "Setup complete!"

run: |
  set -e
  cd ~/sky_workdir
  export PATH="$HOME/.cargo/bin:$HOME/google-cloud-sdk/bin:$PATH"

  # Load credentials
  source /tmp/credentials.env
  source /tmp/global.env
  export HF_TOKEN="${HUGGINGFACE_WRITE_TOKEN}"
  # Explicitly export WANDB_API_KEY for subprocesses
  export WANDB_API_KEY="${WANDB_API_KEY}"

  # Activate GCS credentials
  gcloud auth activate-service-account --key-file=/tmp/gcp-creds.json
  export GOOGLE_APPLICATION_CREDENTIALS=/tmp/gcp-creds.json

  # Create checkpoint directory
  CHECKPOINT_DIR="/tmp/checkpoints"
  rm -rf $CHECKPOINT_DIR  # Clean previous runs
  mkdir -p $CHECKPOINT_DIR

  # Debug: verify credentials
  echo "WANDB_API_KEY length: ${#WANDB_API_KEY}"

  echo "================================================"
  echo "WrinkleFree BitNet + DLM + Influence Training"
  echo "================================================"
  echo "Model: $MODEL"
  echo "Training Config: $TRAINING_CONFIG"
  echo "Total Tokens: $TOTAL_TOKENS (5B)"
  echo "Max Steps: $MAX_STEPS"
  echo "Muon LR: $MUON_LR (2D weights)"
  echo "AdamW LR: $ADAM_LR (embeddings/biases)"
  echo "Auto Batch Size: enabled"
  echo "Experiment: $EXPERIMENT_NAME"
  echo "WandB Project: $WANDB_PROJECT"
  echo "GCS Bucket: $GCS_BUCKET"
  echo "================================================"
  echo ""
  echo "Training Schedule:"
  echo "- First 20%: warmup (CE only)"
  echo "- Remaining 80%: mixed_pretrain with influence (CE + DLM)"
  echo ""
  echo "Objectives:"
  echo "- continue_pretrain (CE): weight=1.0"
  echo "- dlm: weight=0.5 (after warmup)"
  echo ""
  echo "Influence Settings:"
  echo "- warmup_steps: 200"
  echo "- update_interval: 500"
  echo "- learning_rate: 0.1"
  echo "================================================"
  echo ""

  # IMPORTANT: num_workers=0 is REQUIRED for influence tracking
  # The JVPEmbeddingExtractor's nn.ModuleList references cause deadlock with multiprocessing
  export DATA_NUM_WORKERS=0

  echo "[Training] Starting Lightning training..."
  # Using unified config with Muon+AdamW optimizer and auto batch sizing
  uv run --package wrinklefree python packages/training/scripts/train_lightning.py \
    model=${MODEL} \
    training=unified \
    data.config_name=mixed_pretrain \
    training.max_steps=${MAX_STEPS} \
    training.auto_batch_size=true \
    training.gradient_accumulation_steps=8 \
    training.optimizer.type=muon \
    training.optimizer.lr_muon=${MUON_LR} \
    training.optimizer.lr_adam=${ADAM_LR} \
    training.lambda_warmup.enabled=false \
    training.influence.enabled=true \
    training.influence.method=datainf \
    training.influence.update_interval=500 \
    training.influence.warmup_steps=200 \
    training.influence.learning_rate=0.1 \
    training.influence.samples_per_dataset=32 \
    output_dir=$CHECKPOINT_DIR \
    experiment_name=${EXPERIMENT_NAME} \
    training.checkpoint.save_interval=500 \
    training.logging.wandb.enabled=true \
    training.logging.wandb.project=${WANDB_PROJECT} \
    training.logging.log_interval=10 \
    training.validation.enabled=false \
    resume.skip_completed=false \
    gcs.enabled=true \
    gcs.bucket=${GCS_BUCKET} \
    2>&1 | tee training.log

  echo ""
  echo "================================================"
  echo "Training Complete!"
  echo "================================================"

  # Verify training completed successfully
  echo ""
  echo "[Verify] Training summary:"

  # Check for key metrics in logs
  if grep -q "loss" training.log; then
    echo "Loss metrics found in logs"
    grep -E "train/loss|train/continue_pretrain|train/dlm|train/lrc" training.log | tail -20
  fi

  # Check influence weight updates
  echo ""
  echo "[Verify] Influence weight updates:"
  if grep -q "influence" training.log; then
    echo "Influence logging found"
    grep -E "influence|weight_update" training.log | tail -10
  else
    echo "WARNING: No influence messages found"
  fi

  # List checkpoints
  echo ""
  echo "[Verify] Checkpoints:"
  find $CHECKPOINT_DIR -type f \( -name "*.ckpt" -o -name "*.pt" \) | head -10

  # Final summary
  echo ""
  echo "================================================"
  echo "WandB: https://wandb.ai/${WANDB_PROJECT}/runs/${WANDB_RUN_ID}"
  echo "GCS: gs://${GCS_BUCKET}/checkpoints/${EXPERIMENT_NAME}/"
  echo "================================================"
  echo ""
  echo "Check WandB for:"
  echo "- train/loss, train/continue_pretrain_loss, train/dlm_loss"
  echo "- influence/weight_* (dataset mixture weights)"
  echo "- schedule/* (curriculum phase weights)"
  echo "================================================"
