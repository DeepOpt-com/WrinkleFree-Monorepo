# BitNet + DLM + CE Training with Meta-Optimization (1x L40)
#
# SmolLM2-135M training run with:
# - BitNet 1.58-bit quantization (ternary weights {-1, 0, 1})
# - Diffusion Language Model (DLM) objective
# - Standard cross-entropy loss
# - Meta-optimization: LDC-MTL (objective weights) + ODM (dataset weights)
#
# Meta-Optimization (replaces DataInf influence):
# - LDC-MTL: Learns optimal CE vs DLM weights via router network
# - ODM/EXP3: Learns optimal data mixing via multi-armed bandit
# - Both are O(1) complexity with ~0% overhead
#
# References:
# - LDC-MTL: https://arxiv.org/abs/2502.08585
# - ODM: https://arxiv.org/abs/2312.02406
#
# Usage:
#   cd packages/deployer
#   source credentials/.env
#   sky launch skypilot/bitdistill_dlm_influence.yaml -y --cluster bitdistill-dlm
#
# Monitor:
#   sky logs bitdistill-dlm
#
# Teardown:
#   sky down bitdistill-dlm -y

name: wrinklefree-bitdistill-dlm-meta

resources:
  accelerators: L40S:1
  memory: 64+
  disk_size: 100
  use_spot: false
  cloud: nebius

workdir: /home/lev/code/WrinkleFreeDevWrapper/InfluenceClean

file_mounts:
  /tmp/gcp-creds.json: /home/lev/code/WrinkleFree/WrinkleFree-Deployer/credentials/gcp-service-account.json
  /tmp/credentials.env: /home/lev/code/WrinkleFree/WrinkleFree-Deployer/credentials/.env
  /tmp/global.env: ~/.config/.env.global

envs:
  GOOGLE_APPLICATION_CREDENTIALS: /tmp/gcp-creds.json
  GCS_BUCKET: wrinklefree-checkpoints
  MODEL: smollm2_135m
  TRAINING_CONFIG: bitdistill_dlm_influence
  TOTAL_TOKENS: 5000000000  # 5B tokens
  MAX_STEPS: 19073  # 5B tokens / (4*32*2048) tokens per step (batch=4, grad_accum=32)
  MUON_LR: 0.02  # Muon LR for 2D weights (recommended default)
  ADAM_LR: 3e-4  # AdamW LR for embeddings/biases (recommended default)
  EXPERIMENT_NAME: smollm2_135m_5b_meta_opt
  WANDB_PROJECT: wrinklefree
  WANDB_RUN_ID: bitdistill-dlm-${SKYPILOT_TASK_ID}
  HF_HUB_ENABLE_HF_TRANSFER: "1"

setup: |
  set -e
  cd ~/sky_workdir

  # Install uv if not present
  if ! command -v uv &> /dev/null; then
    curl -LsSf https://astral.sh/uv/install.sh | sh
  fi
  export PATH="$HOME/.cargo/bin:$PATH"

  # Install gcloud CLI - handle existing broken installations
  export PATH="$HOME/google-cloud-sdk/bin:$PATH"
  if ! gcloud version &> /dev/null; then
    echo "Installing Google Cloud SDK..."
    # Remove any broken existing installation
    rm -rf $HOME/google-cloud-sdk
    curl -sSL https://sdk.cloud.google.com | bash -s -- --disable-prompts --install-dir=$HOME
  fi
  echo "gcloud version: $(gcloud version 2>/dev/null | head -1)"

  # Install dependencies (including muon-clip for single GPU training)
  uv sync --all-packages
  # Install muon-clip from GitHub (required for MuonClip optimizer on single GPU)
  uv pip install git+https://github.com/GAD-cell/muon-clip.git
  # Install gcsfs for Lightning checkpoint loading from GCS
  uv pip install gcsfs

  # Activate GCS credentials and verify access
  echo "Activating GCS credentials..."
  gcloud auth activate-service-account --key-file=/tmp/gcp-creds.json
  if gsutil ls gs://${GCS_BUCKET}/ > /dev/null 2>&1; then
    echo "GCS access: OK"
  else
    echo "ERROR: GCS access failed!"
    exit 1
  fi

  # Verify WANDB_API_KEY is available
  source /tmp/credentials.env
  source /tmp/global.env
  if [ -z "$WANDB_API_KEY" ]; then
    echo "ERROR: WANDB_API_KEY not set!"
    exit 1
  fi
  echo "WANDB_API_KEY: configured (length=${#WANDB_API_KEY})"

  echo "Setup complete!"

run: |
  set -e
  cd ~/sky_workdir
  export PATH="$HOME/.cargo/bin:$HOME/google-cloud-sdk/bin:$PATH"

  # Load credentials
  source /tmp/credentials.env
  source /tmp/global.env
  export HF_TOKEN="${HUGGINGFACE_WRITE_TOKEN}"
  # Explicitly export WANDB_API_KEY for subprocesses
  export WANDB_API_KEY="${WANDB_API_KEY}"

  # Activate GCS credentials
  gcloud auth activate-service-account --key-file=/tmp/gcp-creds.json
  export GOOGLE_APPLICATION_CREDENTIALS=/tmp/gcp-creds.json

  # Create checkpoint directory
  CHECKPOINT_DIR="/tmp/checkpoints"
  rm -rf $CHECKPOINT_DIR  # Clean previous runs
  mkdir -p $CHECKPOINT_DIR

  # Debug: verify credentials
  echo "WANDB_API_KEY length: ${#WANDB_API_KEY}"

  echo "================================================"
  echo "WrinkleFree BitNet + DLM + Influence Training"
  echo "================================================"
  echo "Model: $MODEL"
  echo "Training Config: $TRAINING_CONFIG"
  echo "Total Tokens: $TOTAL_TOKENS (5B)"
  echo "Max Steps: $MAX_STEPS"
  echo "Optimizer: PyTorch Muon (official)"
  echo "Muon LR: $MUON_LR (2D weights, recommended=0.02)"
  echo "AdamW LR: $ADAM_LR (embeddings/biases, recommended=3e-4)"
  echo "Weight Decay: 0.01 (CRITICAL for Muon stability)"
  echo "Batch Size: 4 (fixed for influence compatibility)"
  echo "Grad Accum: 32"
  echo "Experiment: $EXPERIMENT_NAME"
  echo "WandB Project: $WANDB_PROJECT"
  echo "GCS Bucket: $GCS_BUCKET"
  echo "================================================"
  echo ""
  echo "Training Schedule:"
  echo "- First 5%: warmup (CE only)"
  echo "- Remaining 95%: mixed_pretrain with meta-opt (CE + DLM)"
  echo ""
  echo "Objectives:"
  echo "- continue_pretrain (CE): adaptive weight via LDC-MTL"
  echo "- dlm: adaptive weight via LDC-MTL"
  echo ""
  echo "Meta-Optimization Settings:"
  echo "- LDC-MTL: router_lr=0.001, step_interval=32"
  echo "- ODM: reward_smoothing=0.9, warmup_ratio=0.01"
  echo "================================================"
  echo ""

  echo "[Training] Starting Lightning training with meta-optimization..."
  # Using unified config with Muon+AdamW optimizer and meta-optimization
  uv run --package wrinklefree python packages/training/scripts/train_lightning.py \
    model=${MODEL} \
    training=unified \
    data.config_name=mixed_pretrain \
    training.max_steps=${MAX_STEPS} \
    training.auto_batch_size=false \
    training.batch_size=4 \
    training.gradient_accumulation_steps=32 \
    training.optimizer.type=muon \
    training.optimizer.lr_muon=${MUON_LR} \
    training.optimizer.lr_adam=${ADAM_LR} \
    training.optimizer.weight_decay=0.01 \
    training.lambda_warmup.enabled=false \
    training.influence.enabled=false \
    training.meta_optimization.enabled=true \
    training.meta_optimization.ldc_mtl.enabled=true \
    training.meta_optimization.ldc_mtl.lambda_penalty=0.1 \
    training.meta_optimization.ldc_mtl.router_lr=0.001 \
    training.meta_optimization.ldc_mtl.step_interval=32 \
    training.meta_optimization.odm.enabled=true \
    training.meta_optimization.odm.reward_smoothing=0.9 \
    training.meta_optimization.odm.warmup_ratio=0.01 \
    training.meta_optimization.log_interval=50 \
    output_dir=$CHECKPOINT_DIR \
    experiment_name=${EXPERIMENT_NAME} \
    training.checkpoint.save_interval=500 \
    training.logging.wandb.enabled=true \
    training.logging.wandb.project=${WANDB_PROJECT} \
    training.logging.log_interval=10 \
    training.validation.enabled=false \
    gcs.enabled=true \
    gcs.bucket=${GCS_BUCKET} \
    2>&1 | tee training.log

  echo ""
  echo "================================================"
  echo "Training Complete!"
  echo "================================================"

  # Verify training completed successfully
  echo ""
  echo "[Verify] Training summary:"

  # Check for key metrics in logs
  if grep -q "loss" training.log; then
    echo "Loss metrics found in logs"
    grep -E "train/loss|train/continue_pretrain|train/dlm|train/lrc" training.log | tail -20
  fi

  # Check meta-optimization weight updates
  echo ""
  echo "[Verify] Meta-optimization updates:"
  if grep -q "ldc_mtl\|odm" training.log; then
    echo "Meta-optimization logging found"
    grep -E "ldc_mtl|odm|objective_weight|dataset_weight" training.log | tail -10
  else
    echo "WARNING: No meta-optimization messages found"
  fi

  # List checkpoints
  echo ""
  echo "[Verify] Checkpoints:"
  find $CHECKPOINT_DIR -type f \( -name "*.ckpt" -o -name "*.pt" \) | head -10

  # Final summary
  echo ""
  echo "================================================"
  echo "WandB: https://wandb.ai/${WANDB_PROJECT}/runs/${WANDB_RUN_ID}"
  echo "GCS: gs://${GCS_BUCKET}/checkpoints/${EXPERIMENT_NAME}/"
  echo "================================================"
  echo ""
  echo "Check WandB for:"
  echo "- train/loss, train/continue_pretrain_loss, train/dlm_loss"
  echo "- meta/ldc_mtl/objective_weight_* (learned objective weights)"
  echo "- meta/odm/dataset_weight_* (learned dataset weights)"
  echo "- meta/odm/exploration_rate (EXP3 exploration decay)"
  echo "================================================"
