# WrinkleFree Influence Tracking E2E Test (1x L40)
#
# Tests the influence-based dataset remixing integration:
# - InfluenceTrackerCallback initialization
# - Probe gradient caching at train start
# - Multi-source data loading (mixed_pretrain config)
# - Weight update logging to WandB
#
# Usage:
#   cd packages/deployer
#   source credentials/.env
#   sky launch skypilot/smoke_test_influence.yaml -y --cluster influence-smoke
#
# Monitor:
#   sky logs influence-smoke
#
# Teardown:
#   sky down influence-smoke -y

name: wrinklefree-influence-smoke

resources:
  accelerators: L40S:1
  memory: 64+
  use_spot: false
  cloud: nebius

workdir: /home/lev/code/WrinkleFreeDevWrapper/Refact

file_mounts:
  /tmp/gcp-creds.json: /home/lev/code/WrinkleFreeDevWrapper/Refact/packages/deployer/credentials/gcp-service-account.json
  /tmp/credentials.env: /home/lev/code/WrinkleFreeDevWrapper/Refact/packages/deployer/credentials/.env
  /tmp/global.env: ~/.config/.env.global

envs:
  GOOGLE_APPLICATION_CREDENTIALS: /tmp/gcp-creds.json
  GCS_BUCKET: wrinklefree-checkpoints
  MODEL: smollm2_135m
  MAX_STEPS: 50
  WANDB_PROJECT: wrinklefree
  WANDB_RUN_ID: influence-smoke-${SKYPILOT_TASK_ID}
  HF_HUB_ENABLE_HF_TRANSFER: "1"

setup: |
  set -e
  cd ~/sky_workdir

  if ! command -v uv &> /dev/null; then
    curl -LsSf https://astral.sh/uv/install.sh | sh
    export PATH="$HOME/.cargo/bin:$PATH"
  fi

  # Install gcloud CLI if not available (required for GCS checkpoint uploads)
  echo "Checking gcloud CLI..."
  if ! gcloud version &> /dev/null; then
    echo "Installing gcloud CLI..."
    curl -sSL https://sdk.cloud.google.com > /tmp/install_gcloud.sh
    bash /tmp/install_gcloud.sh --disable-prompts --install-dir=$HOME
    export PATH="$HOME/google-cloud-sdk/bin:$PATH"
  fi
  echo "gcloud version: $(gcloud version 2>/dev/null | head -1)"

  # Authenticate gcloud with service account
  gcloud auth activate-service-account --key-file=/tmp/gcp-creds.json
  echo "GCS authentication configured"

  uv sync --all-packages
  echo "Setup complete!"

run: |
  set -e
  cd ~/sky_workdir
  export PATH="$HOME/.cargo/bin:$HOME/google-cloud-sdk/bin:$PATH"

  source /tmp/credentials.env
  source /tmp/global.env
  export HF_TOKEN="${HUGGINGFACE_WRITE_TOKEN}"
  export WANDB_API_KEY="${WANDB_API_KEY}"

  # Re-authenticate gcloud (in case setup ran in different shell)
  gcloud auth activate-service-account --key-file=/tmp/gcp-creds.json 2>/dev/null || true
  export HF_HUB_DOWNLOAD_TIMEOUT=300  # Increase timeout to 5 minutes
  export HF_HUB_ENABLE_HF_TRANSFER=1  # Use faster/more reliable transfer library
  export HF_DATASETS_OFFLINE=0
  export DATASETS_TRUST_REMOTE_CODE=1
  export REQUESTS_TIMEOUT=300

  # Authenticate with HuggingFace (required for some datasets)
  # Use huggingface_hub Python API to avoid git credential issues
  echo "Authenticating with HuggingFace..."
  uv run --package wf-train python -c "from huggingface_hub import login; login(token='${HF_TOKEN}')"
  echo "HF authentication complete"

  echo "WandB API key set: ${WANDB_API_KEY:0:8}..."
  echo "HF timeout: $HF_HUB_DOWNLOAD_TIMEOUT"

  CHECKPOINT_DIR="/tmp/checkpoints"
  mkdir -p $CHECKPOINT_DIR

  echo "================================================"
  echo "WrinkleFree Influence Tracking E2E Test"
  echo "================================================"
  echo "Model: $MODEL"
  echo "Max Steps: $MAX_STEPS"
  echo "Data Config: mixed_pretrain (multi-source with probes)"
  echo "Influence: enabled"
  echo "================================================"
  echo ""
  echo "Expected behavior:"
  echo "1. InfluenceAwareBatchSizeFinder initializes influence cache"
  echo "2. BatchSizeFinder finds max batch with remaining memory"
  echo "3. Training starts with auto-discovered batch size"
  echo "4. Weight updates triggered every 20 steps"
  echo "5. Weights logged to WandB (influence/weight_*)"
  echo "================================================"
  echo ""

  echo "[Training] Running Lightning training with influence..."
  # Using mixed_pretrain data config for multi-source with probes
  # auto_batch_size=true with influence uses InfluenceAwareBatchSizeFinder
  # which initializes influence cache BEFORE batch size search
  # IMPORTANT: num_workers=0 is REQUIRED for influence tracking
  # The JVPEmbeddingExtractor's nn.ModuleList references cause deadlock with multiprocessing
  export DATA_NUM_WORKERS=0
  uv run --package wf-train python packages/training/scripts/train_lightning.py \
    model=${MODEL} \
    training=base \
    data.config_name=mixed_pretrain \
    training.max_steps=${MAX_STEPS} \
    training.auto_batch_size=true \
    training.batch_size=2 \
    training.gradient_accumulation_steps=16 \
    training.optimizer.type=muon \
    training.influence.enabled=true \
    training.influence.method=datainf \
    training.influence.update_interval=20 \
    training.influence.warmup_steps=5 \
    training.influence.learning_rate=0.1 \
    training.influence.samples_per_dataset=4 \
    output_dir=$CHECKPOINT_DIR \
    experiment_name=influence_smoke_$(date +%s) \
    training.checkpoint.save_interval=25 \
    training.logging.log_interval=1 \
    training.logging.wandb.enabled=true \
    training.logging.wandb.project=${WANDB_PROJECT} \
    training.validation.enabled=false \
    resume.skip_completed=false \
    gcs.enabled=true \
    gcs.bucket=${GCS_BUCKET} \
    2>&1 | tee training.log

  echo ""
  echo "================================================"
  echo "Influence Smoke Test Complete!"
  echo "================================================"

  # Verify influence logs appeared
  echo ""
  echo "[Verify] Checking for influence-related log messages:"
  if grep -q "InfluenceAwareBatchSizeFinder" training.log; then
    echo "✓ InfluenceAwareBatchSizeFinder found in logs"
  else
    echo "✗ WARNING: InfluenceAwareBatchSizeFinder not found in logs"
  fi

  if grep -q "InfluenceTrackerCallback" training.log; then
    echo "✓ InfluenceTrackerCallback found in logs"
  else
    echo "✗ WARNING: InfluenceTrackerCallback not found in logs"
  fi

  if grep -q "probe gradients" training.log || grep -q "Probe gradients" training.log; then
    echo "✓ Probe gradient caching message found"
  else
    echo "✗ WARNING: Probe gradient caching message not found"
  fi

  if grep -q "influence" training.log; then
    echo "✓ Influence-related messages found"
  else
    echo "✗ WARNING: No influence messages in logs"
  fi

  # Verify weight updates
  echo ""
  echo "[Verify] Weight updates:"
  if grep -q "updated weights" training.log || grep -q "optimal weights" training.log; then
    echo "✓ Weight update messages found"
    grep -E "(updated weights|optimal weights)" training.log | tail -5
  else
    echo "✗ WARNING: No weight update messages found"
  fi

  # Verify checkpoints
  echo ""
  echo "[Verify] Local Checkpoints:"
  find $CHECKPOINT_DIR -type f -name "*.ckpt" | head -10

  # Verify GCS uploads
  echo ""
  echo "[Verify] GCS Checkpoints:"
  gcloud storage ls "gs://${GCS_BUCKET}/checkpoints/" 2>/dev/null | head -10 || echo "GCS listing failed"

  # WandB link
  echo ""
  echo "WandB: https://wandb.ai/${WANDB_PROJECT}/runs/${WANDB_RUN_ID}"
  echo ""
  echo "Check WandB for influence/weight_* metrics!"
