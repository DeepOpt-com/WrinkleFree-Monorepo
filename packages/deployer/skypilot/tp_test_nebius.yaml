# Tensor Parallelism Test - 8x H100 on Nebius
#
# Launch:
#   cd packages/deployer
#   source credentials/.env
#   uv run sky launch skypilot/tp_test_nebius.yaml -y --cluster tp-test
#
# Monitor:
#   uv run sky logs tp-test
#
# Down:
#   uv run sky down tp-test -y

name: tp-test-8h100

resources:
  image_id: docker:nvcr.io/nvidia/pytorch:24.01-py3
  cloud: nebius
  accelerators: H100:8  # 8x H100 for TP testing
  use_spot: false
  disk_size: 100
  disk_tier: best

workdir: ../..

envs:
  # Fix: SkyPilot doesn't set CUDA_VISIBLE_DEVICES for jobs without --gpus
  # See: https://github.com/skypilot-org/skypilot/issues/2510
  CUDA_VISIBLE_DEVICES: "0,1,2,3,4,5,6,7"

setup: |
  set -e
  cd ~/sky_workdir

  # Install uv
  curl -LsSf https://astral.sh/uv/install.sh | sh
  source ~/.local/bin/env

  # Install all packages in monorepo
  uv sync --all-packages

  # Verify GPU setup
  echo "=== GPU Configuration ==="
  uv run python -c "import torch; print(f'GPUs: {torch.cuda.device_count()}'); [print(f'  {i}: {torch.cuda.get_device_name(i)}') for i in range(torch.cuda.device_count())]"

run: |
  set -e
  cd ~/sky_workdir/packages/training
  source ~/.local/bin/env

  echo "=============================================="
  echo "Tensor Parallelism Test - 8x H100 on Nebius"
  echo "=============================================="

  # Test 1: TP=8, DP=1 (pure tensor parallelism)
  echo ""
  echo "=== Test 1: TP=8, DP=1 (30 steps, rebalance at 15) ==="
  uv run torchrun --standalone --nproc_per_node=8 \
    scripts/test_tp_smoke.py \
    --tp-size 8 \
    --steps 30 \
    --rebalance-step 15 \
    --batch-size 4 \
    --seq-length 256 \
    --check-sync

  echo ""
  echo "=== Test 1 Complete ==="

  # Test 2: TP=4, DP=2 (2D parallelism)
  echo ""
  echo "=== Test 2: TP=4, DP=2 (30 steps, rebalance at 15) ==="
  uv run torchrun --standalone --nproc_per_node=8 \
    scripts/test_tp_smoke.py \
    --tp-size 4 \
    --steps 30 \
    --rebalance-step 15 \
    --batch-size 4 \
    --seq-length 256 \
    --check-sync

  echo ""
  echo "=== Test 2 Complete ==="

  # Test 3: TP=2, DP=4 (more data parallel)
  echo ""
  echo "=== Test 3: TP=2, DP=4 (30 steps, rebalance at 15) ==="
  uv run torchrun --standalone --nproc_per_node=8 \
    scripts/test_tp_smoke.py \
    --tp-size 2 \
    --steps 30 \
    --rebalance-step 15 \
    --batch-size 4 \
    --seq-length 256 \
    --check-sync

  echo ""
  echo "=============================================="
  echo "ALL TENSOR PARALLELISM TESTS COMPLETE!"
  echo "=============================================="
