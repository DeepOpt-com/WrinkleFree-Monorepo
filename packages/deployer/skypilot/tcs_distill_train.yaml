# TCS Distillation Training Job
#
# Distills knowledge from AR teacher into DLM student using Target Concrete Score.
# Uses block-wise attention distillation for ARâ†’DLM alignment.
#
# Launch via wf CLI (recommended):
#   wf tcs-distill --checkpoint gs://wrinklefree-checkpoints/dlm/bitnet-b1.58-2B-4T-bf16/
#   wf tcs-distill --checkpoint gs://... --teacher 1bitLLM/bitnet_b1_58-2B
#
# Or manually:
#   sky launch skypilot/tcs_distill_train.yaml -y \
#     --env DLM_CHECKPOINT=gs://wrinklefree-checkpoints/dlm/bitnet-b1.58-2B-4T-bf16/
#
# Monitor:
#   wf logs wf-tcs-distill

name: wf-tcs-distill

resources:
  # Cloud and accelerators set by core.py based on --cloud flag
  accelerators: H100:1  # Single GPU for 2B model
  use_spot: false
  disk_size: 200
  job_recovery:
    max_restarts_on_errors: 3

# Sync distillation code to cluster (monorepo path)
workdir: ../distillation

# Mount dependencies and GCS credentials
file_mounts:
  # Data handler library
  ~/data_handler: ../data_handler
  # GCS credentials for checkpoint download/upload
  /tmp/gcp-adc.json: ~/.config/gcloud/application_default_credentials.json

# Environment variables (WANDB_API_KEY set by core.py via update_envs)
envs:
  # DLM checkpoint path (required)
  DLM_CHECKPOINT: ""

  # Teacher model name (optional - inferred from dlm_config.json if not set)
  TEACHER_MODEL: ""

  # Hydra overrides
  HYDRA_OVERRIDES: ""

  # Data loading - force single-threaded to avoid HuggingFace streaming deadlocks
  DATA_NUM_WORKERS: "0"

  # W&B tracking (CRITICAL - pass --env WANDB_API_KEY=<key> when launching!)
  WANDB_PROJECT: wrinklefree-distillation
  WANDB_API_KEY: ""  # REQUIRED for monitoring! Will print loud warning if not set

  # HuggingFace token for gated models (e.g., microsoft/bitnet-b1.58-2B-4T-bf16)
  HF_TOKEN: ""

  # GCS credentials
  GOOGLE_APPLICATION_CREDENTIALS: /tmp/gcp-adc.json
  GCS_BUCKET: wrinklefree-checkpoints
  GCLOUD_PROJECT: wrinklefree-481904
  CLOUDSDK_CORE_PROJECT: wrinklefree-481904

setup: |
  set -e
  cd ~/sky_workdir

  # Install gcloud CLI for GCS operations
  if ! command -v gcloud &> /dev/null; then
    echo "Installing gcloud CLI..."
    curl -sSL https://sdk.cloud.google.com > /tmp/install_gcloud.sh
    bash /tmp/install_gcloud.sh --disable-prompts --install-dir=$HOME
    export PATH="$HOME/google-cloud-sdk/bin:$PATH"
  fi
  export PATH="$HOME/google-cloud-sdk/bin:$PATH"

  # Setup GCP credentials
  mkdir -p ~/.config/gcloud
  cp /tmp/gcp-adc.json ~/.config/gcloud/application_default_credentials.json

  # Install uv
  if ! command -v uv &> /dev/null; then
    curl -LsSf https://astral.sh/uv/install.sh | sh
    export PATH="$HOME/.local/bin:$PATH"
  fi
  export PATH="$HOME/.local/bin:$PATH"

  # Create venv and install dependencies
  uv venv .venv
  source .venv/bin/activate

  # Remove only workspace sources (keep git sources like muon-clip)
  sed -i '/workspace = true/d' pyproject.toml

  # Install data_handler first (dependency)
  uv pip install -e ~/data_handler

  # Install distillation package
  uv pip install -e .

  # Install zstd compression support (required for DCLM parquet)
  uv pip install zstandard

  # Verify GPU setup
  python -c "import torch; print(f'GPUs: {torch.cuda.device_count()}'); [print(f'  {i}: {torch.cuda.get_device_name(i)}') for i in range(torch.cuda.device_count())]"

  # Verify GCS access
  echo "Testing GCS access..."
  python -c "from google.cloud import storage; client = storage.Client(); bucket = client.bucket('${GCS_BUCKET}'); print(f'GCS bucket access: {bucket.exists()}')" || echo "GCS check failed..."

run: |
  set -e
  cd ~/sky_workdir
  export PATH="$HOME/google-cloud-sdk/bin:$HOME/.local/bin:$PATH"
  source .venv/bin/activate

  echo "=============================================="
  echo "TCS Distillation Training"
  echo "DLM Checkpoint: ${DLM_CHECKPOINT}"
  echo "Teacher: ${TEACHER_MODEL:-'(from dlm_config.json)'}"
  echo "Overrides: ${HYDRA_OVERRIDES:-'(none)'}"
  echo "=============================================="

  # Validate checkpoint path
  if [ -z "${DLM_CHECKPOINT}" ]; then
    echo "ERROR: DLM_CHECKPOINT is required!"
    echo "Usage: wf tcs-distill --checkpoint gs://..."
    exit 1
  fi

  # Build Hydra command
  HYDRA_CMD="python src/distillation/distill.py"
  HYDRA_CMD="${HYDRA_CMD} distillation=tcs"
  HYDRA_CMD="${HYDRA_CMD} student.checkpoint_path=${DLM_CHECKPOINT}"
  HYDRA_CMD="${HYDRA_CMD} student.type=dlm"

  # MuonClip + AdamW optimizer (MUST-DO: always use this)
  HYDRA_CMD="${HYDRA_CMD} optimizer.type=muon"

  # seq_len=2048 (MUST-DO: never reduce), batch_size=1 to fit in 80GB VRAM
  HYDRA_CMD="${HYDRA_CMD} data.max_seq_length=2048"
  HYDRA_CMD="${HYDRA_CMD} training.batch_size=1"
  HYDRA_CMD="${HYDRA_CMD} training.gradient_accumulation_steps=128"
  HYDRA_CMD="${HYDRA_CMD} training.max_steps=500"
  HYDRA_CMD="${HYDRA_CMD} logging.log_interval=2"

  # Add teacher model if provided
  if [ -n "${TEACHER_MODEL}" ]; then
    HYDRA_CMD="${HYDRA_CMD} teacher.model_name=${TEACHER_MODEL}"
  fi

  # Add any additional overrides
  if [ -n "${HYDRA_OVERRIDES}" ]; then
    HYDRA_CMD="${HYDRA_CMD} ${HYDRA_OVERRIDES}"
  fi

  echo "Running: ${HYDRA_CMD}"
  eval ${HYDRA_CMD}

  echo ""
  echo "=============================================="
  echo "TCS DISTILLATION COMPLETE!"
  echo "Checkpoints: gs://${GCS_BUCKET}/distillation/"
  echo "=============================================="
