# Fast-dLLM v2 Training Job
#
# Trains a model with Fast-dLLM v2 SFT recipe for 2.5x faster inference.
# Auto-resumes from GCS checkpoint if available (checkpoint-step-* dirs).
#
# Launch via wf CLI (recommended):
#   wf dlm -m bitnet_2b                # Resume from checkpoint (Nebius default)
#   wf dlm -m qwen3_4b -s hf://org/checkpoint
#
# Or manually:
#   sky launch skypilot/dlm_train.yaml -y \
#     --env MODEL=bitnet_2b
#
# Monitor:
#   wf logs wf-dlm-train

name: wf-dlm-train

resources:
  # Cloud and accelerators set by core.py based on --cloud flag
  accelerators: H100:1  # Single GPU, scale via wf dlm --scale
  use_spot: false
  disk_size: 200
  # disk_tier: best  # Removed - not supported on RunPod
  job_recovery:
    max_restarts_on_errors: 3  # Handle transient errors

# Sync DLM-Converter code to cluster (monorepo path)
workdir: ../converter

# Mount GCS credentials for checkpoint upload
# Note: On GCP, VMs use IAM automatically. This is for non-GCP clouds.
file_mounts:
  # CheaperTraining library for PlateauEarlyStopping
  ~/cheapertraining: ../cheapertraining
  /tmp/gcp-adc.json: ~/.config/gcloud/application_default_credentials.json

# W&B tracking (pass via --secret WANDB_API_KEY=...)
secrets:
  WANDB_API_KEY: null

envs:
  # Model config name (matches configs/model/*.yaml)
  MODEL: bitnet_2b

  # Source checkpoint (optional - can be set via Hydra override)
  SOURCE_PATH: ""

  # Hydra overrides (passed from wf dlm command)
  HYDRA_OVERRIDES: ""

  # W&B tracking (set via WANDB_API_KEY env var)
  WANDB_PROJECT: wrinklefree-dlm

  # GCS credentials (ADC)
  GOOGLE_APPLICATION_CREDENTIALS: /tmp/gcp-adc.json
  GCS_BUCKET: wrinklefree-checkpoints
  GCLOUD_PROJECT: wrinklefree-481904
  CLOUDSDK_CORE_PROJECT: wrinklefree-481904

setup: |
  set -e
  cd ~/sky_workdir

  # Install gcloud CLI for GCS operations
  if ! command -v gcloud &> /dev/null; then
    echo "Installing gcloud CLI..."
    curl -sSL https://sdk.cloud.google.com > /tmp/install_gcloud.sh
    bash /tmp/install_gcloud.sh --disable-prompts --install-dir=$HOME
    export PATH="$HOME/google-cloud-sdk/bin:$PATH"
  fi
  export PATH="$HOME/google-cloud-sdk/bin:$PATH"

  # Setup GCP credentials directory
  mkdir -p ~/.config/gcloud
  cp /tmp/gcp-adc.json ~/.config/gcloud/application_default_credentials.json

  # Install uv for fast package management
  if ! command -v uv &> /dev/null; then
    curl -LsSf https://astral.sh/uv/install.sh | sh
    export PATH="$HOME/.local/bin:$PATH"
  fi
  export PATH="$HOME/.local/bin:$PATH"

  # Create venv and install dependencies
  uv venv .venv
  source .venv/bin/activate

  # Remove workspace source (only works in monorepo context)
  sed -i '/\[tool.uv.sources\]/,/^$/d' pyproject.toml

  # Install CheaperTraining first (dependency for PlateauEarlyStopping)
  uv pip install -e ~/cheapertraining

  # Install converter package
  uv pip install -e .

  # Install MuonClip optimizer (required for training)
  uv pip install git+https://github.com/GAD-cell/muon-clip.git@main

  # Verify GPU setup
  python -c "import torch; print(f'GPUs: {torch.cuda.device_count()}'); [print(f'  {i}: {torch.cuda.get_device_name(i)}') for i in range(torch.cuda.device_count())]"

  # Verify GCS credentials (on GCP, IAM is used; on other clouds, ADC file is used)
  echo "Testing GCS access..."
  python -c "from google.cloud import storage; client = storage.Client(); bucket = client.bucket('${GCS_BUCKET}'); print(f'GCS bucket access: {bucket.exists()}')" || echo "GCS check failed (may work via IAM on GCP)..."

run: |
  set -e
  cd ~/sky_workdir
  export PATH="$HOME/google-cloud-sdk/bin:$HOME/.local/bin:$PATH"
  source .venv/bin/activate

  echo "=============================================="
  echo "Fast-dLLM v2 SFT Training (Hydra)"
  echo "Model: ${MODEL}"
  echo "Source: ${SOURCE_PATH:-'(from config)'}"
  echo "Overrides: ${HYDRA_OVERRIDES:-'(none)'}"
  echo "=============================================="

  # Build Hydra command
  HYDRA_CMD="python scripts/train_dlm.py model=${MODEL}"

  # Add source path if provided
  if [ -n "${SOURCE_PATH}" ]; then
    HYDRA_CMD="${HYDRA_CMD} source.path=${SOURCE_PATH}"
  fi

  # Add any additional overrides
  if [ -n "${HYDRA_OVERRIDES}" ]; then
    HYDRA_CMD="${HYDRA_CMD} ${HYDRA_OVERRIDES}"
  fi

  echo "Running: ${HYDRA_CMD}"
  eval ${HYDRA_CMD}

  # Upload to GCS using gsutil
  echo ""
  echo "Uploading to GCS..."
  OUTPUT_DIR="./outputs/dlm/${MODEL}"
  if [ -d "${OUTPUT_DIR}" ]; then
    gsutil -m cp -r ${OUTPUT_DIR}/* gs://${GCS_BUCKET}/dlm/${MODEL}/ || echo "gsutil failed, trying gcloud storage..."
    gcloud storage cp -r ${OUTPUT_DIR}/* gs://${GCS_BUCKET}/dlm/${MODEL}/ || echo "Upload failed"
  else
    echo "Warning: Output directory ${OUTPUT_DIR} not found"
  fi

  echo ""
  echo "=============================================="
  echo "TRAINING COMPLETE!"
  echo "GCS: gs://${GCS_BUCKET}/dlm/${MODEL}/"
  echo "=============================================="
