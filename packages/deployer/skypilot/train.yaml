# WrinkleFree Training Job
#
# Launch training as a managed job (auto-restart on preemption):
#   source credentials/.env
#   sky jobs launch train.yaml --secret WANDB_API_KEY --secret SKYPILOT_DOCKER_PASSWORD
#
# Or use the CLI (auto-prepares secrets):
#   wf train -m qwen3_4b -s 2 --cloud nebius
#
# Monitor:
#   sky jobs queue
#   sky jobs logs <job_id>
#
# Cancel:
#   sky jobs cancel <job_id>

name: wrinklefree-train

resources:
  # TODO: Enable Docker image once built and pushed to GAR
  # image_id: docker:us-docker.pkg.dev/wrinklefree-481904/wf-train/wf-train:latest
  accelerators: H100:1  # Override from core.py based on scale
  use_spot: false  # On-demand for stability (spot keeps getting preempted)
  disk_size: 200  # GB - enough for checkpoints and model weights
  disk_tier: best
  cloud: nebius  # Nebius for H100 training
  job_recovery:
    max_restarts_on_errors: 3  # Handle NCCL timeouts, driver issues

# Sync local training code to cluster (monorepo paths)
workdir: ../training

# Mount dependencies and credentials
file_mounts:
  # CheaperTraining library for influence-based training
  ~/cheapertraining: ../cheapertraining
  # GCP credentials for GCS checkpointing
  ~/.config/gcloud/application_default_credentials.json: ~/.config/gcloud/application_default_credentials.json

envs:
  # Training configuration (override with -e)
  MODEL: qwen3_4b
  STAGE: "2"

  # Weights & Biases tracking
  WANDB_PROJECT: wrinklefree

  # WANDB_API_KEY is set via core.py at launch time
  # HF_TOKEN is optional, set via core.py if available
  #
  # Docker registry auth (uncomment when image_id is enabled):
  # SKYPILOT_DOCKER_USERNAME: _json_key
  # SKYPILOT_DOCKER_SERVER: us-docker.pkg.dev
  # SKYPILOT_DOCKER_PASSWORD is set via core.py for non-GCP clouds

setup: |
  set -e
  cd ~/sky_workdir

  # Install uv if not present (required for dependency management)
  if ! command -v uv &> /dev/null; then
    curl -LsSf https://astral.sh/uv/install.sh | sh
    export PATH="$HOME/.local/bin:$PATH"
  fi
  export PATH="$HOME/.local/bin:$PATH"

  # Create venv and install dependencies
  uv venv .venv
  source .venv/bin/activate

  # Remove workspace source (only works in monorepo context)
  # This allows pip install to work with the mounted cheapertraining package
  sed -i '/\[tool.uv.sources\]/,/^$/d' pyproject.toml

  # Install CheaperTraining first (dependency of wrinklefree)
  uv pip install -e ~/cheapertraining

  # Install muon-clip from git (removed from pyproject.toml sources by sed above)
  uv pip install "muon-clip @ git+https://github.com/GAD-cell/muon-clip"

  # Install the training package
  uv pip install -e .

  # Note: Flash Attention is handled via PyTorch's native SDPA (scaled_dot_product_attention)
  # No separate flash-attn package needed - PyTorch 2.2+ includes FlashAttention-2 backend

  # Verify GPU setup
  python -c "import torch; print(f'GPUs: {torch.cuda.device_count()}'); [print(f'  {i}: {torch.cuda.get_device_name(i)}') for i in range(torch.cuda.device_count())]"

run: |
  set -e
  cd ~/sky_workdir
  export PATH="$HOME/.local/bin:$PATH"
  source .venv/bin/activate

  # Determine training config based on stage
  case $STAGE in
    1)   TRAINING_CONFIG="stage1_subln" ;;
    1.9) TRAINING_CONFIG="stage1_9_layerwise" ;;
    2)   TRAINING_CONFIG="stage2_pretrain" ;;
    3)   TRAINING_CONFIG="stage3_distill" ;;
    *)   echo "Unknown STAGE: $STAGE"; exit 1 ;;
  esac

  # Check for checkpoint to resume from
  CHECKPOINT_DIR="$HOME/checkpoints/${MODEL}/stage${STAGE}"
  mkdir -p $CHECKPOINT_DIR
  RESUME_FLAG=""

  # Priority: 1. RESUME_CHECKPOINT env var (passed from CLI), 2. Local checkpoint dir
  # Note: Use +key= to add new keys to Hydra config (not in struct)
  if [ -n "${RESUME_CHECKPOINT:-}" ]; then
    echo "Resuming from checkpoint: $RESUME_CHECKPOINT"
    RESUME_FLAG="+training.resume_from_checkpoint=$RESUME_CHECKPOINT"
  elif [ "$(ls -A $CHECKPOINT_DIR 2>/dev/null)" ]; then
    echo "Found existing checkpoint at $CHECKPOINT_DIR, resuming..."
    RESUME_FLAG="+training.resume_from_checkpoint=$CHECKPOINT_DIR"
  else
    echo "No checkpoint found, starting fresh training..."
  fi

  # Determine data config based on stage
  # Note: data/default.yaml delegates to CheaperTraining's mixed_pretrain config
  case $STAGE in
    1)   DATA_CONFIG="" ;;  # Stage 1 doesn't need data
    1.9) DATA_CONFIG="data=default" ;;
    2)   DATA_CONFIG="data=default" ;;
    3)   DATA_CONFIG="data=default" ;;  # Fine-tuning uses default config
    *)   DATA_CONFIG="data=default" ;;
  esac

  # Debug: show configuration
  echo "HYDRA_OVERRIDES: ${HYDRA_OVERRIDES:-}"
  echo "RESUME_CHECKPOINT: ${RESUME_CHECKPOINT:-not set}"

  # Run training
  # Note: distributed config (single_gpu vs fsdp_multi) is set via HYDRA_OVERRIDES from core.py
  python scripts/train.py \
    model=${MODEL} \
    training=${TRAINING_CONFIG} \
    $DATA_CONFIG \
    output_dir=${CHECKPOINT_DIR} \
    $RESUME_FLAG \
    ${HYDRA_OVERRIDES:-}
