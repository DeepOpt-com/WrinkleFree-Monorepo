# Salient + LoRA + Hadamard Combined Smoke Test - Nebius 1x H100
#
# Tests the full BitNet v2 pipeline with online Hadamard quantization:
# 1. Auto-converts model to BitNet with Hadamard (HBitLinear for o_proj/down_proj)
# 2. Calibrates salient columns using activation statistics
# 3. Converts BitLinear -> BitLinearSalient (~1% columns in FP16)
# 4. Wraps with LoRAAdapter (low-rank correction on top)
# 5. Trains 40 steps
#
# The Hadamard transform decorrelates activations, reducing outliers
# and enabling better low-bit quantization (per BitNet v2 paper).
#
# Based on:
# - BitNet v2: https://arxiv.org/abs/2504.18415 (online Hadamard)
# - AWQ: https://arxiv.org/abs/2306.00978 (salient columns)
# - LoRA: https://arxiv.org/abs/2106.09685 (low-rank adaptation)
# - LRC: https://arxiv.org/abs/2412.07902 (quantization error correction)
#
# Launch:
#   cd packages/deployer
#   source credentials/.env
#   sky launch skypilot/smoke_test_salient_lora_hadamard.yaml -y --cluster wf-smoke-hadamard
#
# Monitor:
#   sky logs wf-smoke-hadamard
#
# Down:
#   sky down wf-smoke-hadamard -y

name: wf-smoke-salient-lora-hadamard

resources:
  accelerators: H100:1
  memory: 64+
  use_spot: false
  cloud: nebius

# Run from monorepo root
workdir: ../..

file_mounts:
  /tmp/gcp-creds.json: credentials/gcp-service-account.json
  /tmp/credentials.env: credentials/.env

envs:
  GOOGLE_APPLICATION_CREDENTIALS: /tmp/gcp-creds.json
  GOOGLE_CLOUD_PROJECT: wrinklefree-481904
  GCS_BUCKET: wrinklefree-checkpoints
  MODEL: smollm2_135m
  WANDB_PROJECT: wrinklefree_v2
  HF_HUB_ENABLE_HF_TRANSFER: "1"

setup: |
  set -e
  cd ~/sky_workdir

  if ! command -v uv &> /dev/null; then
    curl -LsSf https://astral.sh/uv/install.sh | sh
    export PATH="$HOME/.cargo/bin:$PATH"
  fi

  uv sync --all-packages

  # Verify imports for Hadamard, salient, and LoRA
  printf "Verifying imports...\n"
  uv run python -c "from wf_arch import HBitLinear, hadamard_transform, hadamard_transform_weights; print('Hadamard imports OK')"
  uv run python -c "from wf_arch import BitLinearSalient, convert_bitlinear_to_salient, calibrate_salient_columns; print('Salient imports OK')"
  uv run python -c "from wf_arch import LoRAAdapter, LoRAConfig, add_lora_to_model; print('LoRA imports OK')"

  printf "Setup complete\n"

run: |
  set -e
  cd ~/sky_workdir
  export PATH="$HOME/.cargo/bin:$PATH"

  set -a
  source /tmp/credentials.env
  set +a
  export HF_TOKEN="${HUGGINGFACE_WRITE_TOKEN}"

  echo "WANDB_API_KEY is set: $([ -n \"$WANDB_API_KEY\" ] && echo 'yes' || echo 'no')"

  CHECKPOINT_DIR="/tmp/checkpoints"
  mkdir -p $CHECKPOINT_DIR

  echo "=============================================="
  echo "Salient + LoRA + HADAMARD Smoke Test"
  echo "=============================================="
  echo "Model: ${MODEL}"
  echo ""
  echo "What will happen:"
  echo "  1. Load SmolLM2-135M from HuggingFace"
  echo "  2. Auto-convert to BitNet WITH HADAMARD:"
  echo "     - o_proj, down_proj -> HBitLinear (online Hadamard)"
  echo "     - Other projections -> BitLinear"
  echo "     - Weight matrices transformed: W' = W @ H"
  echo "  3. Calibrate salient columns (16 samples, fast)"
  echo "  4. Convert BitLinear -> BitLinearSalient (~1% FP16 columns)"
  echo "  5. Wrap all layers with LoRAAdapter"
  echo "  6. Train 40 steps"
  echo ""
  echo "Key features (BitNet v2):"
  echo "  - Online Hadamard decorrelates activations"
  echo "  - Reduces outliers for better quantization"
  echo "  - ~1% of columns kept in FP16 (AWQ-style)"
  echo "  - LoRA correction on top (2% rank)"
  echo "=============================================="

  # Enable CUDA_LAUNCH_BLOCKING for accurate error locations (debug mode)
  export CUDA_LAUNCH_BLOCKING=1

  uv run --package wf-train python packages/training/scripts/train_lightning.py \
    model=${MODEL} \
    training=salient_lora_hadamard_run \
    training.max_steps=40 \
    training.batch_size=8 \
    training.gradient_accumulation_steps=1 \
    training.max_seq_length=256 \
    training.logging.log_interval=5 \
    training.auto_batch_size=false \
    training.torch_compile.enabled=false \
    training.salient.calibration_samples=16 \
    training.meta_optimization.enabled=false \
    output_dir=$CHECKPOINT_DIR \
    experiment_name=salient_lora_hadamard_smoke_test \
    distributed=single_gpu \
    gcs.enabled=true \
    gcs.bucket=${GCS_BUCKET} \
    training.logging.wandb.enabled=true \
    training.logging.wandb.project=${WANDB_PROJECT}

  echo ""
  echo "=============================================="
  echo "SALIENT + LORA + HADAMARD SMOKE TEST COMPLETE!"
  echo ""
  echo "What was tested:"
  echo "  - Auto BitNet conversion with Hadamard"
  echo "  - HBitLinear for o_proj/down_proj"
  echo "  - Weight Hadamard transformation (W' = W @ H)"
  echo "  - Online Hadamard activation transform"
  echo "  - AWQ-style salient column calibration"
  echo "  - LoRAAdapter wrapping"
  echo "  - Forward/backward through combined layers"
  echo "  - WandB logging"
  echo "  - GCS checkpoint upload"
  echo ""
  echo "Check W&B: https://wandb.ai/${WANDB_PROJECT}"
  echo "Check GCS: gs://${GCS_BUCKET}/"
  echo "=============================================="
