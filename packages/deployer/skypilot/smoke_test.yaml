# WrinkleFree Smoke Test - Quick 5-minute training validation
#
# Launch:
#   sky launch skypilot/smoke_test.yaml -y --cluster smoke-test
#
# Monitor:
#   sky status
#   sky logs smoke-test
#
# Tear down:
#   sky down smoke-test -y

name: wrinklefree-smoke-test

resources:
  # Pre-built image with ML deps - reduces startup from ~10min to ~30s
  image_id: docker:gcr.io/wrinklefree-481904/wf-train:latest
  accelerators: RTX4090:1  # 24GB VRAM, good availability
  use_spot: false  # On-demand for better availability
  cloud: runpod

# Sync training code
workdir: /home/lev/code/WrinkleFree/WrinkleFree-1.58Quant

# Upload credentials for checkpoint upload and wandb
file_mounts:
  /tmp/gcp-creds.json: /home/lev/code/WrinkleFree/WrinkleFree-Deployer/credentials/gcp-service-account.json
  /tmp/credentials.env: /home/lev/code/WrinkleFree/WrinkleFree-Deployer/credentials/.env

envs:
  GOOGLE_APPLICATION_CREDENTIALS: /tmp/gcp-creds.json
  GCS_BUCKET: wrinklefree-checkpoints
  MODEL: smollm2_135m
  # Limit training to ~5 minutes worth of steps
  MAX_STEPS: 100
  WANDB_PROJECT: wrinklefree
  WANDB_RUN_ID: smoke-test-${SKYPILOT_TASK_ID}
  # Enable fast HF downloads
  HF_HUB_ENABLE_HF_TRANSFER: "1"

setup: |
  set -e
  cd ~/sky_workdir

  # Base dependencies are pre-installed in the Docker image
  source /app/.venv/bin/activate

  # Install wrinklefree package (editable)
  pip install -e . --no-deps

run: |
  set -e
  cd ~/sky_workdir
  source /app/.venv/bin/activate

  # Load credentials (WANDB_API_KEY, etc.)
  source /tmp/credentials.env

  CHECKPOINT_DIR="/tmp/checkpoints"
  mkdir -p $CHECKPOINT_DIR

  echo "========================================="
  echo "WrinkleFree Smoke Test"
  echo "Model: $MODEL"
  echo "Max Steps: $MAX_STEPS"
  echo "Checkpoint dir: $CHECKPOINT_DIR"
  echo "========================================="

  # Stage 1: SubLN insertion (model conversion, no training)
  echo ""
  echo "[Stage 1] Converting model with SubLN insertion..."
  python scripts/train.py \
    model=${MODEL} \
    training=stage1_subln \
    distributed=single_gpu \
    training.output_checkpoint=$CHECKPOINT_DIR/stage1_model

  echo ""
  echo "[Stage 1] Complete! Saved to $CHECKPOINT_DIR/stage1_model"

  # Stage 1.9: Layer-wise distillation (limited steps for smoke test)
  echo ""
  echo "[Stage 1.9] Running layer-wise distillation (limited to $MAX_STEPS steps)..."
  python scripts/train.py \
    model=${MODEL} \
    training=stage1_9_layerwise \
    data=fineweb \
    distributed=single_gpu \
    training.max_steps=${MAX_STEPS} \
    output_dir=$CHECKPOINT_DIR \
    experiment_name=smoke_test \
    training.checkpoint.save_interval=50 \
    training.logging.log_interval=10 \
    training.early_stopping.enabled=false

  echo ""
  echo "========================================="
  echo "Smoke Test Complete!"
  echo "========================================="

  # List what was saved (checkpoints are in output_dir/experiment_name/)
  echo "Checkpoints:"
  find $CHECKPOINT_DIR -type f -name "*.pt" -o -name "*.safetensors" | head -20

  # Upload checkpoints to GCS
  echo ""
  echo "[Upload] Uploading checkpoints to GCS bucket: $GCS_BUCKET"
  CHECKPOINT_DIR=$CHECKPOINT_DIR/smoke_test GCS_BUCKET=$GCS_BUCKET GCS_PREFIX="checkpoints/smoke-test" \
    python scripts/upload_to_gcs.py
  echo ""
  echo "[Upload] Complete!"
