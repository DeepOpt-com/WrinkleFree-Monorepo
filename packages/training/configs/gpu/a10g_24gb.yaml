# GPU Profile: NVIDIA A10G 24GB
# Use with: python scripts/train.py gpu=a10g_24gb ...
name: a10g_24gb
vram_gb: 24
disk_gb: 100

# Qwen3-4B batch sizes by stage
# Target: ~18GB VRAM usage (leave 6GB headroom)
qwen3_4b:
  stage1_9:
    batch_size: 2
    gradient_accumulation_steps: 32
    # Teacher + Student loaded, tight fit
  stage2:
    batch_size: 4
    gradient_accumulation_steps: 16
    # Student only

# SmolLM2-135M batch sizes
# Effective batch = batch_size * grad_accum = 64
smollm2_135m:
  stage1_9:
    batch_size: 16
    gradient_accumulation_steps: 4
  stage2:
    batch_size: 32
    gradient_accumulation_steps: 2
  stage3:
    batch_size: 16
    gradient_accumulation_steps: 4
