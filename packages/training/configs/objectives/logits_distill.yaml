# Logits Distillation Objective (BitDistill)
# KL divergence on teacher/student logits with temperature scaling
# L_LD = KL(P_teacher || P_student) * T^2
enabled: false  # Off by default
weight: 10.0

# Temperature for softmax (higher = softer distributions)
temperature: 5.0

# Index to ignore in labels (padding)
ignore_index: -100

# Whether to shift labels for next-token prediction (AR models)
# Set to false for DLM models that predict masked tokens
shift_labels: true
