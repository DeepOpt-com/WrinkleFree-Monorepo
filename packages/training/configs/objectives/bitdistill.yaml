# BitDistill Combined Objective
# L = lambda_logits * L_LD + gamma_attention * L_AD
# Convenience wrapper combining logits + attention distillation
enabled: false  # Off by default
weight: 1.0

# Weight for logits distillation component
lambda_logits: 10.0

# Weight for attention distillation component (0 = disabled)
gamma_attention: 1.0e-5

# Temperature for KL divergence
temperature: 5.0

# Layer for attention distillation (-1 = last layer)
distill_layer: -1

# Index to ignore in labels
ignore_index: -100
