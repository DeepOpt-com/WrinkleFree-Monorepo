# Block Attention Distillation Objective (for AR->DLM transfer)
# Matches attention patterns WITHIN blocks where both models use bidirectional attention
# Based on Fast-dLLM v2 block-causal attention pattern
enabled: false  # Off by default
weight: 1.0e-4

# Block size for attention matching (Fast-dLLM v2 uses 32)
block_size: 32

# Layer for distillation (-1 = last layer)
distill_layer: -1

# Index to ignore in labels
ignore_index: -100
