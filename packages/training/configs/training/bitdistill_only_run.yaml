# Salient + LoRA: CE-only
#
# Uses only continue_pretrain (cross-entropy) loss.

defaults:
  - base

stage: bitdistill_only_run

# === SALIENT COLUMNS === Disabled
salient:
  enabled: false
  ratio: 0.01
  calibration_samples: 128
  calibration_data: fineweb

# === LORA ADAPTER === Disabled
lora:
  enabled: false
  rank_percentage: 0.02
  init_method: svd_residual
  alpha: 1.0
  dropout: 0.0
  target_modules: null
  quantized: false
  freeze_base: false  # Train WHOLE model (base + LoRA)

# AdamW 8-bit optimizer
optimizer:
  type: adamw
  use_8bit: true
  learning_rate: 1e-4
  weight_decay: 0.01

# Very short warmup
scheduler:
  warmup_steps: 10

# Training parameters
total_tokens: 5_000_000_000 # 5 billion tokens
max_seq_length: 1024
batch_size: 16
gradient_accumulation_steps: 32
auto_batch_size: false

# Lambda warmup enabled (gradual quantization over 100 steps)
lambda_warmup:
  enabled: true
  warmup_steps: 100

# === OBJECTIVES (CE only) ===
objectives:
  continue_pretrain:
    weight: 1.0

# === META-OPTIMIZATION ===
# Data rebalancing via ODM for second half of curriculum
meta_optimization:
  enabled: true
  ldc_mtl:
    enabled: false  # Single objective, no need
  odm:
    enabled: true
    reward_smoothing: 0.9
    warmup_ratio: 0.5  # Start rebalancing at main phase (50%)
    min_weight: 0.05
    max_weight: 0.60
  layer_lr:
    enabled: false
  log_interval: 100

# Curriculum: 50% fineweb(edu) -> 100% main (CE only)
curriculum:
  phases:
    - name: warmup
      end_ratio: 0.50
      data_config: fineweb
      objectives:
        continue_pretrain: 1.0
    - name: main
      end_ratio: 1.0
      data_config: mixed_pretrain
      objectives:
        continue_pretrain: 1.0

# Logging
logging:
  log_interval: 2
  wandb:
    project: wrinklefree_v2
    tags: [bitdistill, bitnet, ce_only]

# Memory optimization
memory:
  gradient_checkpointing: true
  clear_cache_interval: 100
