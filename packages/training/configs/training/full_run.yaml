# Full Run Configuration (Legacy)
# Pretrain with Distillation -> SFT Configuration
#
# Three-phase training:
# - Phase 1 (0-10%): Warmup on FineWeb
# - Phase 2 (10-90%): Pretrain with CE + distillation on mixed data
# - Phase 3 (90-100%): SFT on Nemotron dataset

defaults:
  - base

stage: unified_full_run

# MuonClip optimizer for stability
# NOTE: lr_muon=0.02 causes NaN for 0.6B+ models. Use 0.005-0.01 for stability.
optimizer:
  type: muonclip
  lr_muon: 0.005  # Reduced from 0.02 - prevents NaN on larger models
  lr_adam: 1e-4   # Reduced from 3e-4 for stability
  weight_decay: 0.01
  enable_clipping: true
  clipping_threshold: 10.0  # Reduced from 50 - catches gradient explosions earlier
  clipping_alpha: 0.5

scheduler:
  warmup_steps: 1000  # Increased from 550 - longer warmup for stability

# Training parameters
max_seq_length: 512
auto_batch_size_margin: 0.10  # Safety buffer (10%)

# Lambda warmup for gradual quantization
lambda_warmup:
  warmup_steps: 1000  # Match scheduler warmup

# === OBJECTIVES ===
# CE + distillation for pretrain, SFT for final phase
objectives:
  # SFT for final phase
  sft:
    enabled: true
    weight: 1.0
    ignore_index: -100
    label_smoothing: 0.0

  # Unified distillation with TCS (sparse logits)
  distill:
    enabled: true
    weight: 1.0
    hidden:
      enabled: false
      weight: 0.5
      loss_type: mse_normalized
      layer_weights: progressive
      normalize: true
    logits:
      enabled: true
      weight: 5.0
      temperature: 5.0
      mode: sparse  # TCS-style top-K sparse matching
      top_k: 100
      shift_labels: true  # Use shifted labels for CE phases
      ignore_index: -100
    attention:
      enabled: false
    lrc:
      enabled: false

# Phase-based training
curriculum:
  interpolation: linear
  phases:
    # Phase 1: Warmup on fineweb only (10%)
    - name: only_fineweb
      end_ratio: 0.1
      data_config: fineweb
      objectives:
        continue_pretrain: 1.0
        sft: 0.0
        distill: 0.0

    # Phase 2: Mixed pretrain with distillation (10%-90%)
    - name: pretrain_distill
      end_ratio: 0.9
      data_config: mixed_pretrain
      objectives:
        continue_pretrain: 1.0
        sft: 0.0
        distill: 1.0

    # Phase 3: SFT on instruction data (90%-100%)
    - name: sft
      end_ratio: 1.0
      data_config: sft_nemotron
      objectives:
        continue_pretrain: 0.0
        sft: 1.0
        distill: 0.0

# Logging
logging:
  wandb:
    project: wrinklefree_v2
    tags: [full_run, bitnet, sft, distill]
