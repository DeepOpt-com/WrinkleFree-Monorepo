# Full Run Configuration (Legacy)
# DLM + Quantization -> Distillation Configuration
#
# Key insight: AR teachers produce causal (left-to-right) predictions, while DLM
# students need bidirectional masked token predictions. This config uses:
# - TCS (Target Concrete Score) distillation: no token shifting, designed for DLM
# - Teacher receives unmasked input (via _original_input_ids) for meaningful predictions
# - Student receives masked input for DLM training
#
# Reference: "Distilled Diffusion Language Models" (OpenReview)

defaults:
  - base

stage: unified_full_run

# MuonClip optimizer for stability
# NOTE: lr_muon=0.02 causes NaN for 0.6B+ models. Use 0.005-0.01 for stability.
optimizer:
  type: muonclip
  lr_muon: 0.005  # Reduced from 0.02 - prevents NaN on larger models
  lr_adam: 1e-4   # Reduced from 3e-4 for stability
  weight_decay: 0.01
  enable_clipping: true
  clipping_threshold: 10.0  # Reduced from 50 - catches gradient explosions earlier
  clipping_alpha: 0.5

scheduler:
  warmup_steps: 1000  # Increased from 550 - longer warmup for stability

# Training parameters
max_seq_length: 512
auto_batch_size_margin: 0.10  # Safety buffer (10%)

# Lambda warmup for gradual quantization
lambda_warmup:
  warmup_steps: 1000  # Match scheduler warmup

# === OBJECTIVES ===
# DLM + TCS distillation for fast inference with quality transfer
objectives:
  # Unified distillation with TCS (sparse logits) for DLM
  distill:
    enabled: true
    weight: 1.0
    hidden:
      enabled: false
      weight: 0.5
      loss_type: mse_normalized
      layer_weights: progressive
      normalize: true
    logits:
      enabled: true
      weight: 5.0
      temperature: 5.0
      mode: sparse  # TCS-style top-K sparse matching
      top_k: 100
      shift_labels: false  # NO shifting for DLM
      ignore_index: -100
    attention:
      enabled: false
    lrc:
      enabled: false

# Phase-based training
curriculum:
  interpolation: linear
  phases:
    # Phase 1: Warmup on fineweb only (10%)
    - name: only_fineweb
      end_ratio: 0.1
      data_config: fineweb
      objectives:
        continue_pretrain: 0.5
        dlm: 0.5
        distill: 0.0

    # Phase 2: Mixed pretrain without distillation (10%-90%)
    - name: dlm_ramp
      end_ratio: 0.9
      data_config: mixed_pretrain
      objectives:
        continue_pretrain: 1.0
        dlm: 0.5
        distill: 0.0

    # Phase 3: Full DLM + distillation (90%-100%)
    - name: main
      end_ratio: 1.0
      data_config: mixed_pretrain
      objectives:
        continue_pretrain: 0.5
        dlm: 0.5
        distill: 1.0

# Logging
logging:
  wandb:
    project: wrinklefree_v2
    tags: [dlm_distill, bitnet, tcs]
