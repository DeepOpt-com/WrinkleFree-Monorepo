# Full Run Configuration (Legacy)
# Pretrain with Distillation -> SFT+DLM Configuration
#
# Two-phase training:
# - Phase 1 (0-90%): Pretrain with CE + distillation on FineWeb
# - Phase 2 (90-100%): SFT+DLM on Nemotron dataset
#
# NOTE: DLM requires SFT data - it only masks OUTPUT tokens (labels != -100).
# Fast-dLLM v2 was trained on Qwen2.5-7B-Instruct, not a base model.
#
# Reference: "Fast-dLLM v2" (arXiv:2509.26328)

defaults:
  - base

stage: unified_full_run

# MuonClip optimizer for stability
# NOTE: lr_muon=0.02 causes NaN for 0.6B+ models. Use 0.005-0.01 for stability.
optimizer:
  type: muonclip
  lr_muon: 0.005  # Reduced from 0.02 - prevents NaN on larger models
  lr_adam: 1e-4   # Reduced from 3e-4 for stability
  weight_decay: 0.01
  enable_clipping: true
  clipping_threshold: 10.0  # Reduced from 50 - catches gradient explosions earlier
  clipping_alpha: 0.5

scheduler:
  warmup_steps: 1000  # Increased from 550 - longer warmup for stability

# Training parameters
max_seq_length: 512
auto_batch_size_margin: 0.10  # Safety buffer (10%)

# Lambda warmup for gradual quantization
lambda_warmup:
  warmup_steps: 1000  # Match scheduler warmup

# === OBJECTIVES ===
# CE + distillation for pretrain, SFT+DLM for final phase
objectives:
  # SFT for final phase
  sft:
    enabled: true
    weight: 1.0
    ignore_index: -100
    label_smoothing: 0.0

  # DLM for Fast-dLLM v2 inference (only used with SFT phase)
  dlm:
    enabled: true
    weight: 0.5
    mask_prob: 0.15
    use_complementary_masks: true

  # Unified distillation with TCS (sparse logits)
  distill:
    enabled: true
    weight: 1.0
    hidden:
      enabled: false
      weight: 0.5
      loss_type: mse_normalized
      layer_weights: progressive
      normalize: true
    logits:
      enabled: true
      weight: 5.0
      temperature: 5.0
      mode: sparse  # TCS-style top-K sparse matching
      top_k: 100
      shift_labels: true  # Use shifted labels for CE phases
      ignore_index: -100
    attention:
      enabled: false
    lrc:
      enabled: false

# Phase-based training
curriculum:
  interpolation: linear
  phases:
    # Phase 1: Warmup on fineweb only (10%)
    - name: only_fineweb
      end_ratio: 0.1
      data_config: fineweb
      objectives:
        continue_pretrain: 1.0
        dlm: 0.0  # DLM requires SFT
        sft: 0.0
        distill: 0.0

    # Phase 2: Mixed pretrain with distillation (10%-90%)
    - name: pretrain_distill
      end_ratio: 0.9
      data_config: mixed_pretrain
      objectives:
        continue_pretrain: 1.0
        dlm: 0.0  # DLM requires SFT
        sft: 0.0
        distill: 1.0

    # Phase 3: SFT+DLM on instruction data (90%-100%)
    # DLM only masks response tokens (labels != -100)
    - name: sft_dlm
      end_ratio: 1.0
      data_config: sft_nemotron
      objectives:
        continue_pretrain: 0.0
        dlm: 0.5  # DLM with SFT - masks response tokens only
        sft: 1.0
        distill: 0.0

# Logging
logging:
  wandb:
    project: wrinklefree_v2
    tags: [full_run, bitnet, sft, dlm, distill]
