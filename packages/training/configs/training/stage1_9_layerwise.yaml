# Stage 1.9: Layer-wise Distillation
# Align BitNet hidden states with the original full-precision teacher
# Runs after Stage 1 (SubLN insertion) but before Stage 2 (pre-training)
#
# Research basis:
# - OneBit (arxiv.org/abs/2402.11295): L2-normalized MSE for scale-invariance
# - BitDistill (arxiv.org/abs/2510.13998): Later layers often more important
stage: unified  # Uses unified trainer with objectives system

# torch.compile for 2.9x training speedup (A40 benchmark)
# First step takes ~97s to compile, then 3x faster steady-state
# Set TORCHINDUCTOR_CACHE_DIR for persistent caching across runs
torch_compile:
  enabled: true
  mode: default

# Teacher model (original pre-quantized model, BEFORE Stage 1 conversion)
teacher:
  # This should be the same model used as input to Stage 1
  model_name: ${model.pretrained_name}
  fp16: true  # Load teacher in FP16 for memory efficiency
  offload_to_cpu: false  # Set true to offload between forward passes (slower but saves VRAM)

# === OBJECTIVES SYSTEM ===
# Layer-wise distillation via unified distill objective
objectives:
  # LM loss for maintaining generation quality
  continue_pretrain:
    enabled: true
    weight: 0.5
    ignore_index: -100

  # Unified distillation with hidden states component enabled
  distill:
    enabled: true
    weight: 1.0
    hidden:
      enabled: true
      weight: 1.0
      # mse_normalized: MSE with L2 normalization (OneBit style, recommended)
      loss_type: mse_normalized
      # progressive: Later layers weighted more (linearly increasing)
      layer_weights: progressive
      # L2 normalize hidden states before loss computation
      normalize: true
    logits:
      enabled: false
    attention:
      enabled: false
    lrc:
      enabled: false

# Curriculum for distillation schedule (ramps down distillation over training)
curriculum:
  enabled: true
  interpolation: cosine
  phases:
    # Phase 1: High distillation weight
    - name: distill_heavy
      end_ratio: 0.5
      objectives:
        continue_pretrain: 0.5
        distill: 1.0
    # Phase 2: Ramp down distillation, focus on LM
    - name: lm_focus
      end_ratio: 1.0
      objectives:
        continue_pretrain: 1.0
        distill: 0.0

# Optimization (Muon optimizer for better convergence)
optimizer:
  type: muonclip
  lr: 1.5e-3  # Muon-appropriate LR (similar to stage 2)
  momentum: 0.95
  weight_decay: 0.024
  adamw_betas: [0.9, 0.95]  # For embed/head params
  # QK-clipping settings (Kimi K2 defaults)
  enable_clipping: true
  clipping_threshold: 50.0
  clipping_alpha: 0.5

scheduler:
  type: cosine
  warmup_steps: 50  # ~10% of 500 steps; was 500 which caused T_max=0 error
  min_lr_ratio: 0.1

# Training parameters
# Full 1000 steps needed for distill_schedule to complete (ramps to 0)
max_steps: 1000
total_tokens: 100000000  # ~100M tokens (can be overridden by max_steps)
max_seq_length: 512
batch_size: 16
gradient_accumulation_steps: 8  # Effective batch = 128
gradient_clipping: 1.0

# Early stopping disabled - need full 1000 steps for distill_schedule to complete
early_stopping:
  enabled: false

# Output directory for checkpoints
output_dir: outputs/stage1_9

# Checkpointing (lightweight since short stage)
checkpoint:
  save_interval: 1000
  keep_last_n: 2
  save_optimizer: false  # Don't need optimizer state for this short stage

# Logging (no eval - all data is unseen in streaming pre-training)
logging:
  log_interval: 50
  wandb:
    enabled: true
    project: wrinklefree
    entity: null
    name: null  # Override with run name
    tags: [stage1_9, layerwise_distillation]

# Memory optimization
memory:
  gradient_checkpointing: true
  clear_cache_interval: 100
