# SFT Training Configuration
# Supervised Fine-Tuning using nvidia/Llama-Nemotron-Post-Training-Dataset
#
# Usage:
#   uv run --package wf-train python scripts/train_lightning.py \
#     model=qwen3_4b training=sft_run
#
# The dataset has ~3.9M examples across 5 splits: code, math, science, chat, safety
# Uses Qwen chat template format - labels are masked for instruction tokens.

defaults:
  - base

stage: sft

# Lower LRs for fine-tuning
optimizer:
  type: muon
  lr_muon: 0.002
  lr_adam: 3e-5

scheduler:
  warmup_steps: 100
  decay_ratio: 0.1

# Training parameters
total_tokens: 1_000_000_000  # 1B tokens for SFT
batch_size: 8
gradient_accumulation_steps: 8

# Lambda warmup disabled for SFT (model already quantized)
lambda_warmup:
  enabled: false

# === OBJECTIVES ===
# SFT objective only - instruction tokens masked, only response contributes to loss
objectives:
  continue_pretrain:
    enabled: false
  dlm:
    enabled: false
  sft:
    enabled: true
    weight: 1.0
    ignore_index: -100
    label_smoothing: 0.0

# Single SFT phase
curriculum:
  phases:
    - name: sft
      end_ratio: 1.0
      data_config: sft_nemotron
      objectives:
        sft: 1.0

# Checkpointing
checkpoint:
  save_interval: 500
  keep_last_n: 3

# Logging
logging:
  wandb:
    tags: [sft, bitnet, nemotron]

# Validation disabled for SFT
validation:
  enabled: false
