# SFT Training Configuration
# Supervised Fine-Tuning using nvidia/Llama-Nemotron-Post-Training-Dataset
#
# Usage:
#   uv run --package wf-train python scripts/train_lightning.py \
#     model=qwen3_4b training=sft_run
#
# The dataset has ~3.9M examples across 5 splits: code, math, science, chat, safety
# Uses Qwen chat template format - labels are masked for instruction tokens.

stage: sft

# Auto-convert: Converts model to BitNet on-the-fly if not already converted
auto_convert:
  enabled: true
  exclude_layers:
    - embed_tokens
    - lm_head
    - embed_positions

# torch.compile for training speedup
torch_compile:
  enabled: true
  mode: default

# Sequence packing for efficient training
packing:
  enabled: true

# Optimizer: Muon for 2D weights, AdamW for embeddings
optimizer:
  type: muon
  lr_muon: 0.002  # Lower LR for fine-tuning
  lr_adam: 3e-5
  weight_decay: 0.01
  momentum: 0.95

scheduler:
  type: wsd
  warmup_steps: 100
  decay_ratio: 0.1
  decay_type: linear
  min_lr_ratio: 0.0

# Training parameters
total_tokens: 1_000_000_000  # 1B tokens for SFT
max_steps: null
max_seq_length: 2048
batch_size: 8
gradient_accumulation_steps: 8
gradient_clipping: 1.0
auto_batch_size: true

# Lambda warmup disabled for SFT
lambda_warmup:
  enabled: false

# Resume configuration
resume:
  checkpoint_path: null
  load_optimizer_state: true
  load_scheduler_state: true
  load_training_state: true
  strict_model_load: true

# === OBJECTIVES SYSTEM ===
# SFT objective only - instruction tokens masked, only response contributes to loss
objectives:
  continue_pretrain:
    enabled: false
    weight: 0.0

  dlm:
    enabled: false
    weight: 0.0

  sft:
    enabled: true
    weight: 1.0
    ignore_index: -100
    label_smoothing: 0.0

# === CURRICULUM SCHEDULE ===
# Single SFT phase
curriculum:
  enabled: true
  interpolation: step
  phases:
    - name: sft
      end_ratio: 1.0
      data_config: sft_nemotron
      objectives:
        sft: 1.0

# Teacher model (not used for SFT)
teacher:
  fp16: true
  offload_to_cpu: false
  load_in_4bit: false
  use_flash_attention: false

# Checkpointing
checkpoint:
  save_interval: 500
  keep_last_n: 3
  save_optimizer: true

# Logging
logging:
  log_interval: 10
  wandb:
    enabled: true
    project: wrinklefree
    entity: null
    name: null
    tags: [sft, bitnet, nemotron]

# Early stopping
early_stopping:
  enabled: false
  patience: 3
  min_delta: 0.01
  min_evals: 5

# Validation (using SFT data for val too)
validation:
  enabled: false  # SFT typically doesn't need validation during training
  config_name: null
  val_check_interval: 1000
  limit_val_batches: 20
  batch_size: 4

# Meta-optimization disabled for SFT
meta_optimization:
  enabled: false
