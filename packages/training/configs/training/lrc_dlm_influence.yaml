# BitNet + DLM + CE Training with Influence-Based Data Remixing
#
# Combines:
# - BitNet 1.58-bit quantization (ternary weights {-1, 0, 1})
# - Diffusion Language Model (DLM) masking objective
# - Standard cross-entropy loss (continue_pretrain)
# - Influence-based dataset weight optimization
#
# Curriculum:
# - First 20%: warmup on fineweb-edu (CE only, no DLM)
# - Remaining 80%: mixed_pretrain with influence remixing (CE + DLM)
#
# Usage:
#   uv run python scripts/train_lightning.py \
#     model=smollm2_135m training=lrc_dlm_influence
#
stage: bitnet_dlm_influence

# LRC configuration - DISABLED (using pure BitNet)
lrc:
  enabled: false
  rank_percentage: 0.1
  init_method: zeros

# Auto-convert to BitNet if not already converted
auto_convert:
  enabled: true
  # insert_subln=False: Direct BitLinear replacement preserves pretrained weights
  # insert_subln=True: Full BitDistill Stage 1 (requires Stage 1.9 to realign)
  insert_subln: false
  exclude_layers:
    - embed_tokens
    - lm_head
    - embed_positions

# torch.compile for training speedup
torch_compile:
  enabled: true
  mode: default

# FP8 disabled for L40 (only H100/H200 support FP8)
fp8:
  enabled: false

# Sequence packing
packing:
  enabled: true

# === OPTIMIZER ===
# AdamW - stable default for BitNet training
# MuonClip disabled: lr_muon=0.01 causes divergence with BitNet (see GitHub issue)
optimizer:
  type: adamw
  lr_adam: 1e-4      # Learning rate for all params
  weight_decay: 0.01  # Regularization
  # MuonClip settings (for future use once LR tuning is done)
  # type: muonclip
  # lr_muon: 0.001   # Needs tuning - 0.01 diverges with BitNet
  # momentum: 0.95
  # enable_clipping: false

# Scheduler: WSD with short warmup
scheduler:
  type: wsd
  warmup_steps: 200
  decay_ratio: 0.2
  decay_type: linear
  min_lr_ratio: 0.0

# === TRAINING PARAMETERS ===
total_tokens: 1_000_000_000  # 1B tokens for testing (scale up for prod)
# max_steps computed: 1B tokens / (batch * accum * seq_len) = 1B / (4 * 16 * 2048) = ~7,630 steps
max_steps: 7630  # 1B tokens at batch=4, accum=16, seq=2048
max_seq_length: 2048  # Longer context for better training signal
batch_size: 4  # Reduced for longer sequences (BatchSizeFinder may adjust)
gradient_accumulation_steps: 16  # Effective batch = 64, but 4x more tokens/step
gradient_clipping: 1.0
# auto_batch_size works with MuonClip after fix for upstream hook bug
# MuonClipInitCallback re-registers hooks after BatchSizeFinder completes
auto_batch_size: false  # Disabled for debugging

# Lambda warmup for gradual quantization
# Increased from 200 to 1000 to match 15% curriculum warmup phase (~1145 steps)
lambda_warmup:
  enabled: true
  warmup_steps: 1000
  schedule: linear

# === RESUME CONFIGURATION ===
resume:
  checkpoint_path: null
  load_optimizer_state: true
  load_scheduler_state: true
  load_training_state: true
  strict_model_load: true

# === OBJECTIVES ===
# Multi-task: CE + DLM + LRC reconstruction
objectives:
  # Standard next-token prediction (CE loss)
  continue_pretrain:
    enabled: true
    weight: 1.0
    ignore_index: -100
    label_smoothing: 0.0

  # Diffusion Language Model masking
  dlm:
    enabled: true
    weight: 0.5  # Will be curriculum-controlled
    mask_prob: 0.5  # 50% for symmetric complementary masking (view1: 50%, view2: 50%)
    mask_token_id: 0
    use_complementary_masks: true  # Enabled: doubles batch with mask m and 1-m

  # LRC reconstruction (disabled - requires teacher model setup)
  # Enable later with teacher model for post-quantization recovery
  lrc_reconstruction:
    enabled: false
    weight: 0.3
    loss_type: mse
    layer_weights: progressive
    temperature: 1.0
    normalize: false

# Teacher model for LRC reconstruction targets
teacher:
  fp16: true
  offload_to_cpu: false
  load_in_4bit: false
  use_flash_attention: false

# === CURRICULUM ===
# Phase 1 (0-15%): Pure CE warmup on fineweb-edu
# Phase 2 (15-40%): CE + DLM ramp-up on fineweb-edu
# Phase 3 (40-100%): CE + DLM on mixed_pretrain with influence
curriculum:
  enabled: true
  interpolation: linear
  phases:
    # Phase 1: Pure CE warmup - no DLM
    - name: warmup
      end_ratio: 0.15
      data_config: fineweb
      objectives:
        continue_pretrain: 1.0
        dlm: 0.0

    # Phase 2: DLM ramp-up on fineweb-edu (still simple data)
    - name: dlm_ramp
      end_ratio: 0.40
      data_config: fineweb
      objectives:
        continue_pretrain: 1.0
        dlm: 0.5

    # Phase 3: Main training - CE + DLM with influence remixing
    - name: main
      end_ratio: 1.0
      data_config: mixed_pretrain
      objectives:
        continue_pretrain: 1.0
        dlm: 0.5

# === INFLUENCE-BASED DATA REMIXING ===
# Enabled during Phase 2 (after 20% warmup)
influence:
  enabled: true
  method: datainf
  # Warmup matches curriculum phase 1 (20% of training)
  # For 1B tokens at ~16k tokens/step, 20% = ~12500 steps
  warmup_steps: 2000  # Start after 20% warmup
  update_interval: 500  # Update weights every 500 steps
  learning_rate: 0.15
  samples_per_dataset: 32
  lambda_reg: 1e-4
  smoothing: 0.1
  # Constraints to prevent any domain from dominating
  min_weight: 0.05
  max_weight: 0.60

# === CHECKPOINTING ===
checkpoint:
  save_interval: 500
  keep_last_n: 5
  save_optimizer: true

# === LOGGING ===
# WandB with influence weight tracking
logging:
  log_interval: 10
  wandb:
    enabled: true
    project: wrinklefree
    entity: null
    name: null
    tags: [bitnet, dlm, influence, smollm2]

# Early stopping disabled for exploration
early_stopping:
  enabled: false

# === VALIDATION (C4 perplexity) ===
# Periodic evaluation on held-out C4 validation set
# License: ODC-BY (commercially friendly)
validation:
  enabled: true
  config_name: c4_validation    # References data_handler config
  val_check_interval: 500       # Evaluate every N steps
  limit_val_batches: 50         # Number of validation batches (streaming)
  batch_size: 8                 # Smaller batch for eval (no gradients)

# Memory optimization
memory:
  gradient_checkpointing: true
  clear_cache_interval: 100

# Output directory
output_dir: outputs/lrc_dlm_influence
