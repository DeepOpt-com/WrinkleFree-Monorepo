# BitDistill Full Training Configuration
# Combines CE + logits distillation + attention distillation
# Based on arxiv.org/abs/2510.13998

defaults:
  - base

stage: unified  # Uses unified trainer with distillation objectives

# MuonClip optimizer for BitNet stability
optimizer:
  type: muonclip
  lr_muon: 0.005
  lr_adam: 5e-5
  weight_decay: 0.0
  enable_clipping: true
  clipping_threshold: 50.0
  clipping_alpha: 0.5

# Training parameters (distillation typically needs less data)
total_tokens: 1_000_000_000  # 1B tokens
max_seq_length: 512
batch_size: 8  # Reduced for distillation (teacher + student + attention weights)
gradient_accumulation_steps: 64

# Gradient checkpointing to reduce memory
gradient_checkpointing: true

# Longer lambda warmup for distillation stability
lambda_warmup:
  warmup_steps: 500

# === OBJECTIVES ===
# BitDistill: CE + DLM + logits distillation + attention distillation
objectives:
  dlm:
    weight: 1.0  # Full weight (base has 0.5)

  # Unified distillation: logits + attention
  distill:
    enabled: true
    weight: 1.0
    hidden:
      enabled: false
    logits:
      enabled: true
      weight: 10.0
      temperature: 5.0
      mode: full
      shift_labels: true
      ignore_index: -100
    attention:
      enabled: true
      weight: 1.0e-5
      distill_layer: -1
      mode: relation
      temperature: 1.0
    lrc:
      enabled: false

# Single phase: All objectives from step 0
curriculum:
  phases:
    - name: main
      end_ratio: 1.0
      data_config: fineweb
      objectives:
        continue_pretrain: 1.0
        dlm: 1.0
        distill: 1.0

# Teacher model configuration
teacher:
  model_name: null  # Auto-detect from student's original model
  use_vllm: false
  use_flash_attention: false  # Must be false for attention distillation
  use_eager_attention: true   # Required for attention weights

# Logging
logging:
  wandb:
    tags: [bitdistill, bitnet]

# Influence-based data selection
influence:
  enabled: true
  update_interval: 1000
  learning_rate: 0.2
  config:
    lambda_val: 0.1
    gamma_val: 0.1
    temperature: 1.0
