# DLM + Distillation Configuration
# Pretrain with Distillation -> SFT+DLM Configuration
#
# Three-phase training:
# - Phase 1 (0-10%): CE-only warmup on FineWeb
# - Phase 2 (10-90%): CE + TCS distillation on mixed_pretrain
# - Phase 3 (90-100%): SFT+DLM on Nemotron dataset
#
# NOTE: DLM requires SFT data - it only masks OUTPUT tokens (labels != -100).
# Fast-dLLM v2 was trained on Qwen2.5-7B-Instruct, not a base model.
#
# Reference: "Fast-dLLM v2" (arXiv:2509.26328)

defaults:
  - base

stage: dlm_distill

# MuonClip optimizer for BitNet stability
optimizer:
  type: muonclip
  lr_muon: 0.005
  lr_adam: 5e-5
  weight_decay: 0.0
  enable_clipping: true
  clipping_threshold: 50.0
  clipping_alpha: 0.5

# Training parameters
max_seq_length: 512
auto_batch_size: false

# Longer lambda warmup
lambda_warmup:
  warmup_steps: 500

# === OBJECTIVES ===
# CE + distillation for pretrain, SFT+DLM for final phase
objectives:
  # SFT for final phase
  sft:
    enabled: true
    weight: 1.0
    ignore_index: -100
    label_smoothing: 0.0

  # DLM for Fast-dLLM v2 inference (only used with SFT phase)
  dlm:
    enabled: true
    weight: 0.5
    mask_prob: 0.15
    use_complementary_masks: true

  # Unified distillation with TCS (sparse logits)
  distill:
    enabled: true
    weight: 1.0
    hidden:
      enabled: false
      weight: 0.5
      loss_type: mse_normalized
      layer_weights: progressive
      normalize: true
    logits:
      enabled: true
      weight: 5.0
      temperature: 5.0  # Higher temp for softer targets
      mode: sparse  # TCS-style top-K sparse matching
      top_k: 100  # Sparse TCS estimation for efficiency
      shift_labels: true  # Use shifted labels for CE phases
      ignore_index: -100
    attention:
      enabled: false
    lrc:
      enabled: false

# Phase-based training: CE+distill first, then SFT+DLM
curriculum:
  interpolation: linear
  phases:
    # Phase 1: Warmup - CE only, no distillation yet (first 10%)
    - name: warmup
      end_ratio: 0.1
      data_config: fineweb
      objectives:
        continue_pretrain: 1.0
        dlm: 0.0  # DLM requires SFT
        sft: 0.0
        distill: 0.0

    # Phase 2: CE + distillation (10% - 90%)
    - name: pretrain_distill
      end_ratio: 0.9
      data_config: mixed_pretrain
      objectives:
        continue_pretrain: 1.0
        dlm: 0.0  # DLM requires SFT
        sft: 0.0
        distill: 1.0

    # Phase 3: SFT+DLM on instruction data (90% - 100%)
    # DLM only masks response tokens (labels != -100)
    - name: sft_dlm
      end_ratio: 1.0
      data_config: sft_nemotron
      objectives:
        continue_pretrain: 0.0
        dlm: 0.5  # DLM with SFT - masks response tokens only
        sft: 1.0
        distill: 0.0

# Logging
logging:
  wandb:
    tags: [dlm_distill, bitnet, sft, dlm, tcs]

# Influence-based data selection
influence:
  enabled: true
  update_interval: 1000
  learning_rate: 0.2
  config:
    lambda_val: 0.1
    gamma_val: 0.1
    temperature: 1.0
