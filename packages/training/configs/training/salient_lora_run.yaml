# Salient + LoRA: Combined AWQ-style saliency with low-rank correction
#
# This config enables BOTH features orthogonally:
# 1. Salient columns: Keep ~1% of columns in FP16 (AWQ-style)
# 2. LoRA adapter: Add low-rank correction matrices (U, V)
#
# The features are applied sequentially:
#   model -> convert_to_bitnet -> convert_to_salient -> add_lora
#
# Usage:
#   uv run python scripts/train_lightning.py \
#     model=smollm2_135m training=salient_lora_run
#
# Based on:
# - AWQ: https://arxiv.org/abs/2306.00978 (salient columns)
# - LoRA: https://arxiv.org/abs/2106.09685 (low-rank adaptation)
# - LRC: https://arxiv.org/abs/2412.07902 (quantization error correction)

defaults:
  - base

stage: salient_lora_run

# === SALIENT COLUMNS ===
salient:
  enabled: true
  ratio: 0.01
  calibration_samples: 128
  calibration_data: fineweb

# === LORA ADAPTER ===
lora:
  enabled: true
  rank_percentage: 0.02
  init_method: svd_residual
  alpha: 1.0
  dropout: 0.0
  target_modules: null
  quantized: false
  freeze_base: false  # Train WHOLE model (base + LoRA)

# AdamW 8-bit optimizer
optimizer:
  type: adamw
  use_8bit: true
  learning_rate: 5e-5
  weight_decay: 0.01

# Very short warmup
scheduler:
  warmup_steps: 10

# Training parameters
total_tokens: 1_000_000_000
max_seq_length: 1024
batch_size: 16
gradient_accumulation_steps: 32
auto_batch_size: false

# Lambda warmup disabled (start with full quantization)
lambda_warmup:
  enabled: false

# === OBJECTIVES ===
# CE only (DLM requires SFT data, not available in this config)
objectives:
  continue_pretrain:
    weight: 1.0

  # DLM disabled - requires SFT data. Use salient_run.yaml for DLM.
  dlm:
    enabled: false

# Curriculum: 10% warmup -> 90% main (CE only)
curriculum:
  phases:
    - name: warmup
      end_ratio: 0.10
      data_config: fineweb
      objectives:
        continue_pretrain: 1.0
    - name: main
      end_ratio: 1.0
      data_config: mixed_pretrain
      objectives:
        continue_pretrain: 1.0

# Logging
logging:
  log_interval: 2
  wandb:
    project: wrinklefree_v2
    tags: [salient, lora, bitnet, combined]

# Memory optimization
memory:
  gradient_checkpointing: true
  clear_cache_interval: 100
