# LRC + DLM Run: Low-Rank Correction with Diffusion Language Model
#
# Combines:
# - LRC (Low-Rank Correction) for quantization error recovery
# - DLM (Diffusion Language Model) for fast parallel inference
# - CE (Continue Pretrain) for standard autoregressive generation
#
# Key features:
# - 25% LRC rank (higher than calibration's 10% for better error recovery)
# - Base model frozen (only U, V matrices trainable)
# - No distillation (pure LRC + DLM + CE)
#
# Based on: "Low-Rank Correction for Quantized LLMs" (arxiv 2412.07902)
#
# Usage:
#   uv run python scripts/train_lightning.py \
#     model=qwen3_0.6b training=lrc_run

stage: lrc_dlm_run

# LRC Configuration - 15% rank, frozen base model
lrc:
  enabled: true
  # Rank as percentage of min(in_features, out_features)
  # 15% provides good error recovery while fitting in H100 memory
  # (25% caused OOM with batch_size=32, 161M trainable params)
  rank_percentage: 0.15
  # Initialization options:
  # - "kaiming": LoRA-style (V=Kaiming, U=zeros). Fast, good gradients. (recommended)
  # - "svd_residual": SVD of quantization error. Slow but best quality.
  # - "zeros": Both U,V=zeros. DON'T USE - causes zero gradients!
  init_method: kaiming
  # Freeze base model - only U, V matrices trainable
  trainable_weight: false
  # Delete original weights to save memory (kaiming init doesn't need them)
  keep_original_weight: false
  # QLRC: QA-LoRA style quantized adapters with STE (trainable!)
  # Uses Straight-Through Estimator for gradient flow through quantized adapters.
  # Based on QA-LoRA: https://arxiv.org/abs/2309.14717 (ICLR 2024)
  # Weights stored in full precision, quantized on-the-fly during forward pass.
  # Tracking issue: https://github.com/DeepOpt-com/WrinkleFree-Monorepo/issues/37
  qlrc:
    # TODO: turn back on! (see issue #37)
    enabled: false  # Enable for memory-efficient training
    bits: 4        # 4-bit or 8-bit quantization
    group_size: 32 # Elements per quantization group (QA-LoRA default)

# Auto-convert to BitNet if needed
auto_convert:
  enabled: true
  exclude_layers:
    - embed_tokens
    - lm_head
    - embed_positions

# torch.compile for training speedup (38% on H100)
# Now ENABLED after enable_input_require_grads() fix in train_lightning.py
# Uses mode="default" which works reliably with LRC layers
torch_compile:
  enabled: true
  mode: default
  fullgraph: false  # fullgraph=False allows dynamic control flow

# Sequence packing
packing:
  enabled: true

# Optimizer: MuonClip (without QK-clipping for LRC)
#
# QK-clipping is DISABLED for LRC because:
# - QK-clipping expects trainable Q/K projection weights
# - LRC has U/V correction matrices as trainable params (frozen Q/K)
# - The matmul shapes don't match (U/V have different dimensions than Q/K)
#
# Muon's Newton-Schulz orthogonalization still benefits the U/V matrices.
# optimizer:
#   type: muonclip
#   lr_muon: 0.01          # Muon LR for 2D trainable params (U,V matrices)
#   lr_adam: 1e-4          # AdamW LR for 1D params (biases, weights etc)
#   weight_decay: 0.01
#   momentum: 0.95
#   enable_clipping: false  # Disabled for LRC - only works with full attention training
#   clipping_threshold: 50.0
#   clipping_alpha: 0.5

optimizer:
  type: adamw
  use_8bit: true  # defaults to true anyway
  learning_rate: 1e-5
  weight_decay: 0.01

# Scheduler: WSD
scheduler:
  type: wsd
  warmup_steps: 500
  decay_ratio: 0.2
  decay_type: linear
  min_lr_ratio: 0.0

# Training: 1B tokens
total_tokens: 1_000_000_000
max_steps: null  # Token-based termination
max_seq_length: 1024
# Batch size tuned for H100 + qwen3_0.6b + LRC (15% rank)
# With gradient checkpointing (use_reentrant=False) we can use larger batch sizes
# Previous OOM at batch_size=16 was without gradient checkpointing
batch_size: 16  # Increased from 8 with gradient checkpointing enabled
gradient_accumulation_steps: 32  # Effective batch = 16*32 = 512
gradient_clipping: 1.0
# Auto batch size DISABLED - Lightning BatchSizeFinder is too slow (stuck in loop)
# Using fixed batch sizes from lookup table instead
auto_batch_size: false

# Lambda warmup disabled (LRC uses pre-computed quantized weights)
lambda_warmup:
  enabled: false

# Resume configuration
resume:
  checkpoint_path: null
  load_optimizer_state: true
  load_scheduler_state: true
  load_training_state: true
  strict_model_load: true

# === OBJECTIVES ===
# CE + DLM (no distillation)
objectives:
  # Standard next-token prediction (CE loss)
  continue_pretrain:
    enabled: true
    weight: 1.0
    ignore_index: -100
    label_smoothing: 0.0

  # DLM: Diffusion Language Model for fast parallel inference
  dlm:
    enabled: true
    weight: 0.5
    mask_prob: 0.15
    mask_token_id: 0  # Use unk_token as mask token
    use_complementary_masks: false  # Disabled - doubles memory usage

  # Distillation disabled - LRC distill has hidden_states bug, using CE+DLM only
  distill:
    enabled: false

# No curriculum (fixed CE+DLM weights)
curriculum:
  enabled: false

# Teacher model configuration (for LRC layer matching if needed)
teacher:
  fp16: true
  offload_to_cpu: false
  load_in_4bit: false
  use_flash_attention: false

# Checkpointing
checkpoint:
  save_interval: 200
  keep_last_n: 5
  save_optimizer: true

# Logging
logging:
  log_interval: 10
  wandb:
    enabled: true
    project: wrinklefree_v2
    entity: null
    name: null
    tags: [lrc, dlm, bitnet, 25pct_rank]

# Early stopping
early_stopping:
  enabled: false
  patience: 5
  min_delta: 0.01
  min_evals: 10

# Validation (C4 perplexity)
validation:
  enabled: true
  config_name: c4_validation
  val_check_interval: 500
  limit_val_batches: 50
  batch_size: 8

# Memory optimization
# Gradient checkpointing now works with LRC after fixing use_reentrant=False
# in train_lightning.py. This saves significant VRAM.
memory:
  gradient_checkpointing: true  # FIXED: Now uses use_reentrant=False
  clear_cache_interval: 100

# === META-OPTIMIZATION ===
# LDC-MTL for pareto optimal objective weighting (CE vs DLM)
# TODO: Disabled for now - causes instability with LRC. See GitHub issue.
meta_optimization:
  enabled: false

  ldc_mtl:
    enabled: false  # Disabled - investigate instability with LRC
    lambda_penalty: 0.1        # Discrepancy penalty (encourages balanced losses)
    hidden_dim: 32             # Router MLP hidden size
    router_lr: 0.001           # Router learning rate
    step_interval: 1           # Update router every step

  odm:
    enabled: false             # Single dataset, not needed

  layer_lr:
    enabled: false             # Not using per-layer LR for LRC

  log_interval: 100
