# Stage 2: Continue Pre-Training
# Adapt model to 1.58-bit quantization with large-scale pre-training
stage: continue_pretrain

# torch.compile for 2.9x training speedup (A40 benchmark)
# First step takes ~97s to compile, then 3x faster steady-state
# Set TORCHINDUCTOR_CACHE_DIR for persistent caching across runs
torch_compile:
  enabled: true
  mode: default

# FP8 GEMM acceleration (DeepSeek-V3 style)
# Uses TorchAO for FP8 training on H100/H200 GPUs
# Falls back to BF16 on unsupported hardware (A100, etc.)
# Benchmark: 10% speedup with BF16 acc, no quality loss (SmolLM2-135M)
fp8:
  enabled: true  # Auto-enabled; falls back to BF16 on non-Hopper GPUs
  recipe: rowwise  # "rowwise" (more accurate) or "tensorwise" (faster)
  accumulator_dtype: bfloat16  # 10% faster than float32, safe for <10B models
  min_gemm_size: 512
  exclude_patterns:
    - embed_tokens
    - lm_head
    - norm
    - subln

# Sequence packing - concatenate multiple documents with position_id resets
# See: https://huggingface.co/blog/sirluk/llm-sequence-packing
packing:
  enabled: true  # Default on for training efficiency

# Optimization (AdamW for stable training)
optimizer:
  type: adamw
  lr: 1e-4  # Reduced from 2e-4 to stabilize training (prevent spiky loss)
  weight_decay: 0.01
  betas: [0.9, 0.95]

scheduler:
  type: wsd  # Warmup-Stable-Decay (DeepSeek-V3 style)
  warmup_steps: 500  # Short warmup
  decay_ratio: 0.2   # 20% of training for decay
  decay_type: linear  # "linear" or "cosine"
  min_lr_ratio: 0.0   # Decay to zero

# Training parameters
total_tokens: 10_000_000_000  # 10B tokens
max_steps: null  # Optional: override total_tokens with fixed step count
max_seq_length: 512  # Reduced for single GPU
batch_size: 32  # H100 can handle 32 for qwen3_4b
gradient_accumulation_steps: 4  # Effective batch = 128 (same as before, but 4x faster)
gradient_clipping: 1.0

# Loss (no distillation, just LM loss)
loss:
  type: cross_entropy
  ignore_index: -100

# Lambda warmup for gradual quantization
# Prevents catastrophic forgetting by slowly transitioning from FP to ternary
# Reference: https://huggingface.co/blog/1_58_llm_extreme_quantization
lambda_warmup:
  enabled: true  # Enable gradual quantization
  warmup_steps: 1000  # Steps to reach full quantization (lambda=1)
  schedule: linear  # "linear" or "cosine"

# Checkpointing
checkpoint:
  save_interval: 1000
  keep_last_n: 3
  save_optimizer: true

# Logging (no eval - all data is unseen in streaming pre-training)
logging:
  log_interval: 100
  wandb:
    enabled: true
    project: wrinklefree
    entity: null
    name: null  # Override with run name
    tags: [stage2, pretrain]

# CheaperTraining influence-based data selection
influence:
  enabled: true
  update_interval: 1000
  learning_rate: 0.2
  config:
    lambda_val: 0.1
    gamma_val: 0.1
    temperature: 1.0
