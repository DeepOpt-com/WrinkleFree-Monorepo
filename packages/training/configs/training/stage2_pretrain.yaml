# Stage 2: Continue Pre-Training
# Adapt model to 1.58-bit quantization with large-scale pre-training
stage: continue_pretrain

# torch.compile for 2.9x training speedup (A40 benchmark)
# First step takes ~97s to compile, then 3x faster steady-state
# Set TORCHINDUCTOR_CACHE_DIR for persistent caching across runs
torch_compile:
  enabled: true
  mode: default

# FP8 GEMM acceleration (DeepSeek-V3 style)
# Uses TorchAO for FP8 training on H100/H200 GPUs
# Falls back to BF16 on unsupported hardware (A100, etc.)
# Benchmark: 10% speedup with BF16 acc, no quality loss (SmolLM2-135M)
fp8:
  enabled: true  # Auto-enabled; falls back to BF16 on non-Hopper GPUs
  recipe: rowwise  # "rowwise" (more accurate) or "tensorwise" (faster)
  accumulator_dtype: bfloat16  # 10% faster than float32, safe for <10B models
  min_gemm_size: 512
  exclude_patterns:
    - embed_tokens
    - lm_head
    - norm
    - subln

# Sequence packing - concatenate multiple documents with position_id resets
# See: https://huggingface.co/blog/sirluk/llm-sequence-packing
packing:
  enabled: true  # Default on for training efficiency

# Optimization (AdamW for stable training)
optimizer:
  type: adamw
  lr: 2e-4  # Higher LR for faster convergence
  weight_decay: 0.01
  betas: [0.9, 0.95]

scheduler:
  type: wsd  # Warmup-Stable-Decay (DeepSeek-V3 style)
  warmup_steps: 500  # Short warmup
  decay_ratio: 0.2   # 20% of training for decay
  decay_type: linear  # "linear" or "cosine"
  min_lr_ratio: 0.0   # Decay to zero

# Training parameters
total_tokens: 10_000_000_000  # 10B tokens
max_steps: null  # Optional: override total_tokens with fixed step count
max_seq_length: 512  # Reduced for single GPU
batch_size: 32  # H100 can handle 32 for qwen3_4b
gradient_accumulation_steps: 8  # Effective batch = 256 (doubled for faster convergence)
gradient_clipping: 1.0

# Loss (no distillation, just LM loss)
loss:
  type: cross_entropy
  ignore_index: -100

# Lambda warmup for gradual quantization
# Prevents catastrophic forgetting by slowly transitioning from FP to ternary
# Reference: https://huggingface.co/blog/1_58_llm_extreme_quantization
lambda_warmup:
  enabled: true  # Enable gradual quantization
  warmup_steps: 1000  # Steps to reach full quantization (lambda=1)
  schedule: linear  # "linear" or "cosine"

# Pre-Stage 2 mode: Layer-wise distillation before pure LM training
# When enabled, loads teacher model and performs hidden state alignment
# This merges the old "stage1_9" functionality into stage2
pre_stage_2:
  enabled: false  # Default to pure LM training (set true for distillation)

  # Teacher model settings (auto-loaded from model.teacher.pretrained when enabled)
  teacher:
    fp16: true  # Load teacher in FP16/BF16 for memory efficiency
    offload_to_cpu: false  # Offload between forward passes (slower but saves VRAM)
    load_in_4bit: false  # 4-bit NF4 quantization (3x memory reduction)
    use_flash_attention: false  # Flash Attention 2 for teacher

  # Layer-wise distillation settings
  layerwise:
    # Loss type: mse_normalized (recommended), mse, kl, inner_product
    loss_type: mse_normalized
    # Per-layer weights: null (uniform), "progressive", "exponential", or custom list
    layer_weights: progressive
    normalize: true  # L2 normalize hidden states before loss
    temperature: 1.0  # For KL loss
    # LM loss weight: 0.5 = balanced distillation + cross-entropy
    lm_loss_weight: 0.5
    hidden_size: ${model.hidden_size}
    vocab_size: ${model.vocab_size}

  # Distillation schedule: ramps down distillation weight over training
  distill_schedule:
    enabled: true
    type: cosine  # "linear" or "cosine"
    initial_weight: 0.5  # Start with 50% distillation
    final_weight: 0.0  # End with pure LM loss
    warmup_steps: 0  # Keep initial_weight constant for N steps

# Checkpointing
checkpoint:
  save_interval: 1000
  keep_last_n: 3
  save_optimizer: true

# Logging (no eval - all data is unseen in streaming pre-training)
logging:
  log_interval: 10  # Reduced from 100 for faster feedback
  wandb:
    enabled: true
    project: wrinklefree
    entity: null
    name: null  # Override with run name
    tags: [stage2, pretrain]

# Early stopping: stops training if loss plateaus
# Useful for detecting when distillation/conversion can't improve beyond teacher
early_stopping:
  enabled: false  # Enable to stop on plateau
  patience: 5     # Stop after N consecutive checks without improvement
  min_delta: 0.01 # Minimum loss improvement to count as "better"
  min_evals: 10   # Don't stop before this many checks (at log_interval)

# CheaperTraining influence-based data selection
influence:
  enabled: true
  update_interval: 1000
  learning_rate: 0.2
  config:
    lambda_val: 0.1
    gamma_val: 0.1
    temperature: 1.0
