# Stage 2: Continue Pre-Training
# Adapt model to 1.58-bit quantization with large-scale pre-training
stage: continue_pretrain

# torch.compile for 2.9x training speedup (A40 benchmark)
# First step takes ~97s to compile, then 3x faster steady-state
# Set TORCHINDUCTOR_CACHE_DIR for persistent caching across runs
torch_compile:
  enabled: true
  mode: default

# FP8 GEMM acceleration (DeepSeek-V3 style)
# Uses TorchAO for FP8 training on H100/H200 GPUs
# Falls back to BF16 on unsupported hardware (A100, etc.)
# Benchmark: 10% speedup with BF16 acc, no quality loss (SmolLM2-135M)
fp8:
  enabled: true  # Auto-enabled; falls back to BF16 on non-Hopper GPUs
  recipe: rowwise  # "rowwise" (more accurate) or "tensorwise" (faster)
  accumulator_dtype: bfloat16  # 10% faster than float32, safe for <10B models
  min_gemm_size: 512
  exclude_patterns:
    - embed_tokens
    - lm_head
    - norm
    - subln

# Sequence packing - concatenate multiple documents with position_id resets
# See: https://huggingface.co/blog/sirluk/llm-sequence-packing
packing:
  enabled: true  # Default on for training efficiency

# Optimization (MuonClip for BitNet stability - Kimi K2 methodology)
# MuonClip = Muon optimizer + QK-clipping for attention stability
# Reference: https://github.com/GAD-cell/muon-clip
# Muon uses higher LR for hidden weights, AdamW uses standard LR for embeddings/norms
optimizer:
  type: muonclip
  lr_muon: 0.005  # Muon LR for hidden weights (spectral norm units)
  lr_adam: 5e-5  # AdamW LR for embeddings, norms, lm_head
  weight_decay: 0.0  # Muon doesn't need weight decay
  momentum: 0.95  # Muon momentum
  enable_clipping: true  # QK-clipping for attention stability
  clipping_threshold: 50.0
  clipping_alpha: 0.5

scheduler:
  type: wsd  # Warmup-Stable-Decay (DeepSeek-V3 style)
  warmup_steps: 550  # Short warmup
  decay_ratio: 0.2   # 20% of training for decay
  decay_type: linear  # "linear" or "cosine"
  min_lr_ratio: 0.0   # Decay to zero

# Training parameters
total_tokens: 10_000_000_000  # 10B tokens
max_steps: null  # Optional: override total_tokens with fixed step count
max_seq_length: 512  # Reduced for single GPU
batch_size: 8  # Per-device batch size
gradient_accumulation_steps: 64  # Effective batch = 512 (increased for stability with MuonClip)
gradient_clipping: 1.0

# Loss (no distillation, just LM loss)
loss:
  type: cross_entropy
  ignore_index: -100

# Lambda warmup for gradual quantization
# Prevents catastrophic forgetting by slowly transitioning from FP to ternary
# Reference: https://huggingface.co/blog/1_58_llm_extreme_quantization
lambda_warmup:
  enabled: true  # Enable gradual quantization
  warmup_steps: 500  # Steps to reach full quantization (lambda=1)
  schedule: linear  # "linear" or "cosine"

# Pre-Stage 2 mode: Layer-wise distillation before pure LM training
# When enabled, loads teacher model and performs hidden state alignment
# This merges the old "stage1_9" functionality into stage2
pre_stage_2:
  enabled: false  # Default to pure LM training (set true for distillation)

  # Teacher model settings (auto-loaded from model.teacher.pretrained when enabled)
  teacher:
    fp16: true  # Load teacher in FP16/BF16 for memory efficiency
    offload_to_cpu: false  # Offload between forward passes (slower but saves VRAM)
    load_in_4bit: false  # 4-bit NF4 quantization (3x memory reduction)
    use_flash_attention: false  # Flash Attention 2 for teacher

  # Layer-wise distillation settings
  layerwise:
    # Loss type: mse_normalized (recommended), mse, kl, inner_product
    loss_type: mse_normalized
    # Per-layer weights: null (uniform), "progressive", "exponential", or custom list
    layer_weights: progressive
    normalize: true  # L2 normalize hidden states before loss
    temperature: 1.0  # For KL loss
    # LM loss weight: 0.5 = balanced distillation + cross-entropy
    lm_loss_weight: 0.5
    hidden_size: ${model.hidden_size}
    vocab_size: ${model.vocab_size}

  # Distillation schedule: ramps down distillation weight over training
  distill_schedule:
    enabled: true
    type: cosine  # "linear" or "cosine"
    initial_weight: 0.5  # Start with 50% distillation
    final_weight: 0.0  # End with pure LM loss
    warmup_steps: 0  # Keep initial_weight constant for N steps

# Checkpointing
checkpoint:
  save_interval: 200  # Checkpoint every 200 steps
  keep_last_n: 5  # Keep 5 most recent checkpoints
  save_optimizer: true

# Logging (no eval - all data is unseen in streaming pre-training)
logging:
  log_interval: 10  # Reduced from 100 for faster feedback
  wandb:
    enabled: true
    project: wrinklefree
    entity: null
    name: null  # Override with run name
    tags: [stage2, pretrain]

# Early stopping: stops training if loss plateaus
# Useful for detecting when distillation/conversion can't improve beyond teacher
early_stopping:
  enabled: false  # Enable to stop on plateau
  patience: 5     # Stop after N consecutive checks without improvement
  min_delta: 0.01 # Minimum loss improvement to count as "better"
  min_evals: 10   # Don't stop before this many checks (at log_interval)

# Curriculum Learning: Warmup with high-quality data before mixed distribution
# Prevents loss instability when starting with noisy multi-domain data
curriculum:
  enabled: true
  warmup_ratio: 0.2  # First 20% of training uses warmup_data_config
  warmup_data_config: fineweb  # Config name in cheapertraining/configs/data/

# CheaperTraining influence-based data selection
influence:
  enabled: true
  update_interval: 1000
  learning_rate: 0.2
  config:
    lambda_val: 0.1
    gamma_val: 0.1
    temperature: 1.0
