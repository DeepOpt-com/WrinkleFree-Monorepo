# Unified Training Configuration
# Combines continue pre-training with optional layerwise distillation and DLM
# Replaces separate stage1, stage1.9, stage2 with a single composable config

stage: unified

# Auto-convert: Converts model to BitNet on-the-fly if not already converted
auto_convert:
  enabled: true
  exclude_layers:
    - embed_tokens
    - lm_head
    - embed_positions

# torch.compile for 2.9x training speedup
torch_compile:
  enabled: true
  mode: default

# TODO: Disabled for now?
# FP8 GEMM acceleration (H100/H200 only, auto-fallback to BF16)
fp8:
  enabled: true
  recipe: rowwise
  accumulator_dtype: bfloat16
  min_gemm_size: 512
  exclude_patterns:
    - embed_tokens
    - lm_head
    - norm
    - subln

# Sequence packing
packing:
  enabled: true

# Official PyTorch Muon + AdamW (PyTorch 2.9+)
# - Muon: 2D weights (attention, FFN) with Newton-Schulz orthogonalization
# - AdamW: Embeddings, biases, lm_head, LayerNorm
# Built-in muP scaling - LR doesn't need retuning as model scales
optimizer:
  type: muon  # Official PyTorch Muon + AdamW
  lr_muon: 0.02  # Muon LR for 2D weights (recommended default)
  lr_adam: 3e-4  # AdamW LR for embeddings/biases
  weight_decay: 0.01
  momentum: 0.95  # Muon momentum

scheduler:
  type: wsd  # Warmup-Stable-Decay
  warmup_steps: 550
  decay_ratio: 0.2
  decay_type: linear
  min_lr_ratio: 0.0

# Training parameters
total_tokens: 10_000_000_000  # 10B tokens
max_steps: null
max_seq_length: 512
batch_size: 32
gradient_accumulation_steps: 16
gradient_clipping: 1.0
auto_batch_size: false  # Enable for Lightning BatchSizeFinder

# Lambda warmup for gradual quantization
lambda_warmup:
  enabled: true
  warmup_steps: 500
  schedule: linear

# === RESUME CONFIGURATION ===
# Controls what state to load when resuming from a checkpoint
resume:
  checkpoint_path: null           # Path or gs:// URL (overrides auto-discovery)
  load_optimizer_state: true      # false = start with fresh optimizer
  load_scheduler_state: true      # false = start with fresh scheduler
  load_training_state: true       # false = start from step 0
  strict_model_load: true         # Fail on missing/unexpected keys

# === OBJECTIVES SYSTEM ===
# Composable objectives with Hydra-configurable weights
# Each objective can be enabled/disabled and weighted independently
objectives:
  continue_pretrain:
    enabled: true
    weight: 1.0
    ignore_index: -100
    label_smoothing: 0.0

  dlm:
    enabled: true  # Combined STE+DLM training by default
    weight: 0.5
    mask_prob: 0.15
    mask_token_id: 0  # Use unk_token_id (0) as mask token for decoder-only models
    use_complementary_masks: true  # Fast-dLLM v2: duplicate batch with m and (1-m) masks

  layerwise_distill:
    enabled: false  # Enable for distillation
    weight: 0.5
    loss_type: mse_normalized
    layer_weights: progressive
    normalize: true

# === CURRICULUM SCHEDULE ===
# Phase-based training with objective weight transitions
curriculum:
  enabled: true
  interpolation: linear  # or "cosine"
  phases:
    # Phase 1: Warmup - STE only (first 10%)
    - name: warmup
      end_ratio: 0.1
      data_config: fineweb
      objectives:
        continue_pretrain: 1.0
        dlm: 0.0

    # Phase 2: Ramp up DLM (10% - 30%)
    - name: dlm_ramp
      end_ratio: 0.3
      data_config: mixed_pretrain
      objectives:
        continue_pretrain: 1.0
        dlm: 0.3

    # Phase 3: Main combined training (30% - 80%)
    - name: main
      end_ratio: 0.8
      data_config: mixed_pretrain
      objectives:
        continue_pretrain: 1.0
        dlm: 0.5

    # Phase 4: DLM focus (last 20%)
    - name: dlm_focus
      end_ratio: 1.0
      data_config: mixed_pretrain
      objectives:
        continue_pretrain: 0.5
        dlm: 1.0

# Teacher model (only loaded if an objective requires_teacher)
teacher:
  fp16: true
  offload_to_cpu: false
  load_in_4bit: false
  use_flash_attention: false

# Checkpointing
checkpoint:
  save_interval: 200
  keep_last_n: 5
  save_optimizer: true

# Logging with enhanced metrics
logging:
  log_interval: 10
  wandb:
    enabled: true
    project: wrinklefree
    entity: null
    name: null
    tags: [unified, bitnet]

# Early stopping
early_stopping:
  enabled: false
  patience: 5
  min_delta: 0.01
  min_evals: 10

# === VALIDATION (C4 perplexity) ===
# Periodic evaluation on held-out C4 validation set
# License: ODC-BY (commercially friendly)
validation:
  enabled: true
  config_name: c4_validation    # References data_handler config
  val_check_interval: 500       # Evaluate every N steps
  limit_val_batches: 50         # Number of validation batches (streaming)
  batch_size: 8                 # Smaller batch for eval (no gradients)

# CheaperTraining influence-based data selection
influence:
  enabled: true
  method: datainf  # "datainf" or "distillation" (InfluenceDistillation for landmark KRR)
  update_interval: 1000
  warmup_steps: 500
  learning_rate: 0.2
  samples_per_dataset: 1000  # Samples per source for influence computation
  config:
    lambda_val: 0.1
    gamma_val: 0.1
    temperature: 1.0

# === META-OPTIMIZATION (Bi-Level Outer Loop) ===
# Generalizes influence-based data selection to optimize:
# - Dataset mixture weights (via influence functions)
# - Objective weights (CE, DLM, distillation, etc.)
# - Learning rate scales (per parameter group)
# Using multi-objective Pareto optimization for validation signals.
#
# References:
# - LibMOON (NeurIPS 2024): https://arxiv.org/abs/2409.02969
# - ScaleBiO (2024): https://arxiv.org/abs/2406.19976
# - DataInf (ICLR 2024): Tractable influence without Hessian
meta_optimization:
  enabled: false  # Enable for full meta-optimization (replaces influence when true)

  # Which meta-parameters to optimize (all configurable)
  parameters:
    dataset_weights: true   # Optimize dataset mixture (like influence)
    objective_weights: true # Optimize objective weighting (CE, DLM, etc.)
    learning_rates: false   # Experimental: optimize LR scales per param group

  # Update schedule
  update_interval: 1000    # Steps between meta-updates
  warmup_steps: 500        # Skip early training

  # Gradient estimation method
  gradient:
    method: datainf        # "datainf" or "finite_difference"
    lambda_reg: 1e-4       # Regularization for DataInf
    samples_per_source: 256  # Samples per dataset for influence

  # Pareto multi-objective optimization
  # Balances multiple validation objectives (e.g., C4 perplexity + code + math)
  pareto:
    method: mgda           # "mgda" (min-norm), "epo" (preference), or "linear"
    max_iter: 10           # Solver iterations
    normalize_gradients: true
    preferences: null      # For EPO: [0.5, 0.3, 0.2] for 3 objectives

  # Validation objectives for multi-objective Pareto optimization
  # Each gets a probe dataloader from data_handler
  validation_objectives:
    - name: c4_perplexity
      loader: c4_validation
      weight: 1.0
    # Uncomment to enable multi-objective:
    # - name: code_perplexity
    #   loader: code_probe
    #   weight: 0.5
    # - name: math_accuracy
    #   loader: math_probe
    #   weight: 0.3

  # Meta-learning hyperparameters
  meta_lr: 0.1
  meta_momentum: 0.9

  # Constraints on meta-parameters
  constraints:
    dataset_weight_range: [0.05, 0.60]    # No domain dominates
    objective_weight_range: [0.01, 2.0]   # Keep objectives active
    lr_scale_range: [0.5, 2.0]            # Don't change LR too drastically

  # Logging
  log_interval: 100
  log_pareto_front: true
