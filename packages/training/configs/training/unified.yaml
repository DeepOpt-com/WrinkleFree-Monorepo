# Unified Training Configuration
# Combines continue pre-training with optional layerwise distillation and DLM
# Replaces separate stage1, stage1.9, stage2 with a single composable config

stage: unified

# Auto-convert: Converts model to BitNet on-the-fly if not already converted
auto_convert:
  enabled: true
  exclude_layers:
    - embed_tokens
    - lm_head
    - embed_positions

# torch.compile for 2.9x training speedup
torch_compile:
  enabled: true
  mode: default

# TODO: Disabled for now?
# FP8 GEMM acceleration (H100/H200 only, auto-fallback to BF16)
fp8:
  enabled: true
  recipe: rowwise
  accumulator_dtype: bfloat16
  min_gemm_size: 512
  exclude_patterns:
    - embed_tokens
    - lm_head
    - norm
    - subln

# Sequence packing
packing:
  enabled: true

# Official PyTorch Muon + AdamW (PyTorch 2.9+)
# - Muon: 2D weights (attention, FFN) with Newton-Schulz orthogonalization
# - AdamW: Embeddings, biases, lm_head, LayerNorm
# Built-in muP scaling - LR doesn't need retuning as model scales
optimizer:
  type: muon  # Official PyTorch Muon + AdamW
  lr_muon: 0.02  # Muon LR for 2D weights (recommended default)
  lr_adam: 3e-4  # AdamW LR for embeddings/biases
  weight_decay: 0.01
  momentum: 0.95  # Muon momentum

scheduler:
  type: wsd  # Warmup-Stable-Decay
  warmup_steps: 550
  decay_ratio: 0.2
  decay_type: linear
  min_lr_ratio: 0.0

# Training parameters
total_tokens: 10_000_000_000  # 10B tokens
max_steps: null
max_seq_length: 2048
batch_size: 32
gradient_accumulation_steps: 16
gradient_clipping: 1.0
auto_batch_size: false  # Enable for Lightning BatchSizeFinder

# Lambda warmup for gradual quantization (disabled by default)
# When disabled, model trains with full quantization from step 0
lambda_warmup:
  enabled: false
  warmup_steps: 500
  schedule: linear

# === RESUME CONFIGURATION ===
# Controls what state to load when resuming from a checkpoint
resume:
  checkpoint_path: null           # Path or gs:// URL (overrides auto-discovery)
  load_optimizer_state: true      # false = start with fresh optimizer
  load_scheduler_state: true      # false = start with fresh scheduler
  load_training_state: true       # false = start from step 0
  strict_model_load: true         # Fail on missing/unexpected keys

# === OBJECTIVES SYSTEM ===
# Composable objectives with Hydra-configurable weights
# Each objective can be enabled/disabled and weighted independently
objectives:
  continue_pretrain:
    enabled: true
    weight: 1.0
    ignore_index: -100
    label_smoothing: 0.0

  dlm:
    enabled: true  # Combined STE+DLM training by default
    weight: 0.5
    mask_prob: 0.15
    mask_token_id: 0  # Use unk_token_id (0) as mask token for decoder-only models
    use_complementary_masks: true  # Fast-dLLM v2: duplicate batch with m and (1-m) masks

  layerwise_distill:
    enabled: false  # Enable for distillation
    weight: 0.5
    loss_type: mse_normalized
    layer_weights: progressive
    normalize: true

# === CURRICULUM SCHEDULE ===
# Phase-based training with objective weight transitions
# Fast DLM ramp: 5% warmup, then full CE+DLM for the rest
curriculum:
  enabled: true
  interpolation: linear  # or "cosine"
  phases:
    # Phase 1: Brief warmup - CE only (first 5%)
    - name: warmup
      end_ratio: 0.05
      data_config: fineweb
      objectives:
        continue_pretrain: 1.0
        dlm: 0.0

    # Phase 2: Quick DLM ramp (5% - 10%)
    - name: dlm_ramp
      end_ratio: 0.10
      data_config: mixed_pretrain
      objectives:
        continue_pretrain: 1.0
        dlm: 0.5

    # Phase 3: Full combined training (10% - 100%)
    # Equal weight for CE and DLM
    - name: main
      end_ratio: 1.0
      data_config: mixed_pretrain
      objectives:
        continue_pretrain: 1.0
        dlm: 1.0

# Teacher model (only loaded if an objective requires_teacher)
teacher:
  fp16: true
  offload_to_cpu: false
  load_in_4bit: false
  use_flash_attention: false

# Checkpointing
checkpoint:
  save_interval: 200
  keep_last_n: 5
  save_optimizer: true

# Logging with enhanced metrics
logging:
  log_interval: 10
  wandb:
    enabled: true
    project: wrinklefree
    entity: null
    name: null
    tags: [unified, bitnet]

# Early stopping
early_stopping:
  enabled: false
  patience: 5
  min_delta: 0.01
  min_evals: 10

# === VALIDATION (C4 perplexity) ===
# Periodic evaluation on held-out C4 validation set
# License: ODC-BY (commercially friendly)
validation:
  enabled: true
  config_name: c4_validation    # References data_handler config
  val_check_interval: 500       # Evaluate every N steps
  limit_val_batches: 50         # Number of validation batches (streaming)
  batch_size: 8                 # Smaller batch for eval (no gradients)

# CheaperTraining influence-based data selection
influence:
  enabled: true
  method: datainf  # "datainf" or "distillation" (InfluenceDistillation for landmark KRR)
  update_interval: 1000
  warmup_steps: 500
  learning_rate: 0.2
  samples_per_dataset: 1000  # Samples per source for influence computation
  config:
    lambda_val: 0.1
    gamma_val: 0.1
    temperature: 1.0

# === META-OPTIMIZATION (Bi-Level Outer Loop) ===
# Efficient meta-optimization using O(1) methods:
# - LDC-MTL: Objective weight optimization (CE vs DLM vs distillation)
# - ODM/EXP3: Dataset weight optimization (bandit-based data mixing)
#
# Both methods are efficient, principled, and require no external dependencies.
# No Hessian computation, no influence functions, no Pareto solvers.
#
# References:
# - LDC-MTL (2025): https://arxiv.org/abs/2502.08585
# - ODM (2023): https://arxiv.org/abs/2312.02406
meta_optimization:
  enabled: false  # Enable meta-optimization

  # LDC-MTL: Objective weight optimization
  # Uses a small router network to learn task weights with O(1) complexity
  ldc_mtl:
    enabled: true              # Enable objective weight optimization
    lambda_penalty: 0.1        # Discrepancy penalty (higher = more balanced)
    hidden_dim: 32             # Router MLP hidden size
    router_lr: 0.001           # Router learning rate

  # ODM: Dataset weight optimization via EXP3 bandit
  # Uses training loss as reward signal with ~0% overhead
  odm:
    enabled: true              # Enable dataset weight optimization
    reward_smoothing: 0.9      # EMA coefficient (0-1, higher = more smoothing)
    warmup_ratio: 0.01         # Fraction of steps with uniform weights
    min_weight: 0.05           # Minimum sampling probability per dataset
    max_weight: 0.60           # Maximum sampling probability per dataset

  # Logging
  log_interval: 100
