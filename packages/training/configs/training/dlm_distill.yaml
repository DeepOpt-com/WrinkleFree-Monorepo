# DLM + Distillation Configuration
# Combines DLM (for fast inference) with TCS distillation (for quality)
#
# Key insight: AR teachers produce causal (left-to-right) predictions, while DLM
# students need bidirectional masked token predictions. This config uses:
# - TCS (Target Concrete Score) distillation: no token shifting, designed for DLM
# - Teacher receives unmasked input (via _original_input_ids) for meaningful predictions
# - Student receives masked input for DLM training
#
# Reference: "Distilled Diffusion Language Models" (OpenReview)

defaults:
  - base

stage: dlm_distill

# MuonClip optimizer for BitNet stability
optimizer:
  type: muonclip
  lr_muon: 0.005
  lr_adam: 5e-5
  weight_decay: 0.0
  enable_clipping: true
  clipping_threshold: 50.0
  clipping_alpha: 0.5

# Training parameters
max_seq_length: 512
auto_batch_size: false

# Longer lambda warmup
lambda_warmup:
  warmup_steps: 500

# === OBJECTIVES ===
# DLM + TCS distillation for fast inference with quality transfer
objectives:
  # Unified distillation with TCS (sparse logits) for DLM
  distill:
    enabled: true
    weight: 1.0
    hidden:
      enabled: false
      weight: 0.5
      loss_type: mse_normalized
      layer_weights: progressive
      normalize: true
    logits:
      enabled: true
      weight: 5.0
      temperature: 5.0  # Higher temp for softer targets
      mode: sparse  # TCS-style top-K sparse matching
      top_k: 100  # Sparse TCS estimation for efficiency
      shift_labels: false  # NO shifting for DLM
      ignore_index: -100
    attention:
      enabled: false
    lrc:
      enabled: false

# Phase-based training: CE first, then DLM+distill
curriculum:
  interpolation: linear
  phases:
    # Phase 1: Warmup - CE only, no distillation yet (first 10%)
    - name: warmup
      end_ratio: 0.1
      data_config: fineweb
      objectives:
        continue_pretrain: 1.0
        dlm: 0.0
        distill: 0.0

    # Phase 2: Ramp up DLM + distillation (10% - 30%)
    - name: dlm_ramp
      end_ratio: 0.3
      data_config: mixed_pretrain
      objectives:
        continue_pretrain: 1.0
        dlm: 0.3
        distill: 0.6

    # Phase 3: Full DLM + distillation (30% - 80%)
    - name: main
      end_ratio: 0.8
      data_config: mixed_pretrain
      objectives:
        continue_pretrain: 1.0
        dlm: 0.5
        distill: 1.0

    # Phase 4: DLM focus (last 20%)
    - name: dlm_focus
      end_ratio: 1.0
      data_config: mixed_pretrain
      objectives:
        continue_pretrain: 0.5
        dlm: 1.0
        distill: 0.6

# Logging
logging:
  wandb:
    tags: [dlm_distill, bitnet, tcs]

# Influence-based data selection
influence:
  enabled: true
  update_interval: 1000
  learning_rate: 0.2
  config:
    lambda_val: 0.1
    gamma_val: 0.1
    temperature: 1.0
