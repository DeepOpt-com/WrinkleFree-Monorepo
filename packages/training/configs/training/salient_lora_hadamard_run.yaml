# Salient + LoRA + Hadamard: Combined AWQ-style saliency with low-rank correction
# and BitNet v2 online Hadamard quantization
#
# This config enables ALL THREE features:
# 1. Hadamard transform: Apply online Hadamard to o_proj and down_proj (BitNet v2)
# 2. Salient columns: Keep ~1% of columns in FP16 (AWQ-style)
# 3. LoRA adapter: Add low-rank correction matrices (U, V)
#
# The features are applied sequentially:
#   model -> convert_to_bitnet (with Hadamard) -> convert_to_salient -> add_lora
#
# Usage:
#   uv run python scripts/train_lightning.py \
#     model=smollm2_135m training=salient_lora_hadamard_run
#
# Based on:
# - BitNet v2: https://arxiv.org/abs/2504.18415 (online Hadamard)
# - AWQ: https://arxiv.org/abs/2306.00978 (salient columns)
# - LoRA: https://arxiv.org/abs/2106.09685 (low-rank adaptation)
# - LRC: https://arxiv.org/abs/2412.07902 (quantization error correction)

defaults:
  - base

stage: salient_lora_hadamard_run

# === SALIENT COLUMNS ===
salient:
  enabled: true
  ratio: 0.01
  calibration_samples: 128
  calibration_data: fineweb

# === LORA ADAPTER ===
# DISABLED for faster smoke test (SVD init is slow)
lora:
  enabled: false
  rank_percentage: 0.02
  init_method: svd_residual
  alpha: 1.0
  dropout: 0.0
  target_modules: null
  quantized: false
  freeze_base: false

# Enable Hadamard transform for o_proj and down_proj (BitNet v2)
auto_convert:
  use_hadamard: true

# AdamW 8-bit optimizer
optimizer:
  type: adamw
  use_8bit: true
  learning_rate: 5e-5
  weight_decay: 0.01

# Training parameters
total_tokens: 1_000_000_000
max_seq_length: 1024
batch_size: 16
gradient_accumulation_steps: 32
auto_batch_size: false

# === OBJECTIVES ===
objectives:
  continue_pretrain:
    weight: 0.5

  dlm:
    weight: 0.5
    mask_prob: 0.5

# Curriculum: 10% warmup -> 90% main
curriculum:
  phases:
    - name: warmup
      end_ratio: 0.10
      data_config: fineweb
      objectives:
        continue_pretrain: 0.5
        dlm: 0.5
    - name: main
      end_ratio: 1.0
      data_config: mixed_pretrain
      objectives:
        continue_pretrain: 0.5
        dlm: 0.5

# Logging
logging:
  log_interval: 2
  wandb:
    project: wrinklefree_v2
    tags: [salient, lora, hadamard, bitnet_v2, combined]

# Memory optimization
memory:
  gradient_checkpointing: true
  clear_cache_interval: 100
