# Salient + LoRA + Hadamard: Combined AWQ-style saliency with low-rank correction
# and BitNet v2 online Hadamard quantization
#
# This config enables ALL THREE features:
# 1. Hadamard transform: Apply online Hadamard to o_proj and down_proj (BitNet v2)
# 2. Salient columns: Keep ~1% of columns in FP16 (AWQ-style)
# 3. LoRA adapter: Add low-rank correction matrices (U, V)
#
# The features are applied sequentially:
#   model -> convert_to_bitnet (with Hadamard) -> convert_to_salient -> add_lora
#
# Key benefits:
# - Hadamard decorrelates activations, reducing outliers (BitNet v2)
# - Salient columns preserve most important activations in FP16
# - LoRA corrects remaining quantization errors via low-rank adapters
# - Best accuracy potential from combining all three approaches
#
# Usage:
#   uv run python scripts/train_lightning.py \
#     model=smollm2_135m training=salient_lora_hadamard_run
#
# Based on:
# - BitNet v2: https://arxiv.org/abs/2504.18415 (online Hadamard)
# - AWQ: https://arxiv.org/abs/2306.00978 (salient columns)
# - LoRA: https://arxiv.org/abs/2106.09685 (low-rank adaptation)
# - LRC: https://arxiv.org/abs/2412.07902 (quantization error correction)

stage: salient_lora_hadamard_run

# === SALIENT COLUMNS (Step 2 - after Hadamard conversion) ===
# AWQ-style: keep ~1% of columns in FP16 based on activation*weight magnitude
salient:
  enabled: true
  ratio: 0.01  # 1% of columns in FP16
  calibration_samples: 128
  calibration_data: fineweb

# === LORA ADAPTER (Step 3) ===
# Low-rank correction applied after salient conversion
# DISABLED for faster smoke test (SVD init is slow)
lora:
  enabled: false
  # Rank as percentage of min(in_features, out_features)
  # 2% for smaller adapters
  rank_percentage: 0.02
  # Initialization method:
  # - "kaiming": LoRA-style (A=Kaiming, B=zeros). Fast, good gradients.
  # - "svd_residual": SVD of quantization error. Best accuracy.
  # - "zeros": Both A,B=zeros. DON'T USE - causes zero gradients!
  init_method: svd_residual
  # LoRA scaling factor (alpha/rank)
  alpha: 1.0
  # Dropout for LoRA path (0.0 = no dropout)
  dropout: 0.0
  # Target layers (regex patterns). null = all BitLinear/BitLinearSalient layers
  target_modules: null
  # Quantized adapters (QA-LoRA style, optional)
  quantized: false
  quant_bits: 4
  quant_group_size: 32
  # Train WHOLE model (base + LoRA), not just LoRA adapters
  freeze_base: false

# Auto-convert to BitNet first WITH HADAMARD
auto_convert:
  enabled: true
  insert_subln: true  # Add SubLN before projections (BitNet paper)
  use_hadamard: true  # NEW: Enable Hadamard transform for o_proj and down_proj (BitNet v2)
  exclude_layers:
    - embed_tokens
    - lm_head
    - embed_positions

# torch.compile for training speedup
torch_compile:
  enabled: true
  mode: default
  fullgraph: false

# Sequence packing
packing:
  enabled: true

# Optimizer: AdamW 8-bit (via bitsandbytes)
# Simple and stable for LoRA training, memory efficient
optimizer:
  type: adamw
  use_8bit: true  # Uses bitsandbytes AdamW8bit
  learning_rate: 5e-5
  weight_decay: 0.01

# Scheduler: WSD
scheduler:
  type: wsd
  warmup_steps: 200
  decay_ratio: 0.2
  decay_type: linear
  min_lr_ratio: 0.0

# Training parameters
total_tokens: 1_000_000_000
max_steps: null
max_seq_length: 1024
batch_size: 16
gradient_accumulation_steps: 32
gradient_clipping: 1.0
auto_batch_size: false

# Lambda warmup (DISABLED - start with full quantization from step 0)
# Per architecture CLAUDE.md: "Without warmup (default): get_current_lambda() returns 1.0"
lambda_warmup:
  enabled: false
  warmup_steps: 500
  schedule: linear

# Resume configuration
resume:
  checkpoint_path: null
  load_optimizer_state: true
  load_scheduler_state: true
  load_training_state: true
  strict_model_load: true

# === OBJECTIVES ===
# CE + DLM training
objectives:
  continue_pretrain:
    enabled: true
    weight: 0.5
    ignore_index: -100
    label_smoothing: 0.0

  dlm:
    enabled: true
    weight: 0.5
    mask_prob: 0.5
    mask_token_id: 0  # Use 0 (pad token) as mask
    use_complementary_masks: true

  distill:
    enabled: false

# Curriculum: 10% warmup -> 90% main
curriculum:
  enabled: true
  interpolation: step
  phases:
    - name: warmup
      end_ratio: 0.10
      data_config: fineweb
      objectives:
        continue_pretrain: 0.5
        dlm: 0.5
    - name: main
      end_ratio: 1.0
      data_config: mixed_pretrain
      objectives:
        continue_pretrain: 0.5
        dlm: 0.5

# Checkpointing
checkpoint:
  save_interval: 200
  keep_last_n: 5
  save_optimizer: true

# Logging
logging:
  log_interval: 2
  wandb:
    enabled: true
    project: wrinklefree_v2
    entity: null
    name: null
    tags: [salient, lora, hadamard, bitnet_v2, combined]

# Validation
validation:
  enabled: true
  config_name: c4_validation
  val_check_interval: 500
  limit_val_batches: 50
  batch_size: 8

# Memory optimization
memory:
  gradient_checkpointing: true
  clear_cache_interval: 100

# Meta-optimization (disabled for this run)
meta_optimization:
  enabled: false
  ldc_mtl:
    enabled: false
    lambda_penalty: 0.1
    hidden_dim: 32
    router_lr: 0.001
  odm:
    enabled: false
  layer_lr:
    enabled: false
  log_interval: 100
