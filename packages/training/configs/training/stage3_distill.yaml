# Stage 3: Distillation Fine-Tuning
# Fine-tune with teacher guidance using BitDistill loss
stage: distillation

# torch.compile for 38% training speedup (A10G benchmark)
# First step takes ~60s to compile, then faster steady-state
# Set TORCHINDUCTOR_CACHE_DIR for persistent caching across runs
torch_compile:
  enabled: true
  mode: default

# Optimization (MuonClip: Muon + QK-clipping for training stability)
# Hyperparams tuned via Ax Bayesian optimization (Trial 4: loss 4.93)
optimizer:
  type: muonclip
  lr: 2.4e-3  # Optimized from 4e-3 (Ax sweep)
  momentum: 0.95
  weight_decay: 0.024  # Optimized from 0.1 (Ax sweep)
  adamw_betas: [0.9, 0.95]  # For embed/head params
  # QK-clipping settings (Kimi K2 defaults)
  enable_clipping: true
  clipping_threshold: 50.0
  clipping_alpha: 0.5

scheduler:
  type: cosine
  warmup_steps: 2000  # Optimized from 100 (Ax sweep)
  min_lr_ratio: 0.1

# Training parameters
max_steps: 5000
max_seq_length: 512
batch_size: 32
gradient_accumulation_steps: 8  # Optimized from 4 (Ax sweep)
gradient_clipping: 1.0

# Distillation is configured in distillation/*.yaml
# Loss coefficients (lambda, gamma) come from distillation config

# Quantization warmup (optional, usually not needed if stage2 done)
quantization_warmup:
  enabled: false
  warmup_steps: 0
  schedule: linear

# Checkpointing
checkpoint:
  save_interval: 500
  keep_last_n: 3
  save_optimizer: true

# Logging
logging:
  log_interval: 10
  eval_interval: 100
  wandb:
    enabled: true
    project: wrinklefree
    entity: null
    name: null  # Override with run name
    tags: [stage3, distillation]

# Early stopping (optional)
early_stopping:
  enabled: false
  patience: 5
  metric: eval_loss
  mode: min

# CheaperTraining Influence Configuration
influence:
  enabled: true
  update_interval: 1000
  learning_rate: 0.2
  config:
    lambda_val: 0.1
    gamma_val: 0.1
    temperature: 1.0
