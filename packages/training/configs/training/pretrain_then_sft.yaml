# Pretrain-Then-SFT Training Configuration
# Two-phase training: Pretrain with CE+DLM, then SFT on instruction data
#
# Usage:
#   uv run --package wf-train python scripts/train_lightning.py \
#     model=qwen3_4b training=pretrain_then_sft
#
# Phase 1 (0-90%): Pretrain on FineWeb with CE + DLM objectives
# Phase 2 (90-100%): SFT on Nemotron dataset with Qwen chat template

defaults:
  - base

stage: unified_sft

# Lambda warmup disabled (start with full quantization)
lambda_warmup:
  enabled: false

# === OBJECTIVES ===
# All objectives available - curriculum controls which are active
objectives:
  sft:
    enabled: true  # Will be activated in SFT phase
    weight: 1.0
    ignore_index: -100
    label_smoothing: 0.0

# === CURRICULUM SCHEDULE ===
# Two-phase: Pretrain -> SFT
curriculum:
  phases:
    # Phase 1: Pretrain with CE + DLM (90% of training)
    - name: pretrain
      end_ratio: 0.9
      data_config: fineweb
      objectives:
        continue_pretrain: 1.0
        dlm: 1.0
        sft: 0.0  # Disabled during pretrain

    # Phase 2: SFT on instruction data (10% of training)
    - name: sft
      end_ratio: 1.0
      data_config: sft_nemotron
      objectives:
        continue_pretrain: 0.0  # Disabled during SFT
        dlm: 0.0
        sft: 1.0

# Logging
logging:
  wandb:
    tags: [unified, bitnet, sft, nemotron]
