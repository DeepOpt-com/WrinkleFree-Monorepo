# Pretrain-Then-SFT Training Configuration
# Two-phase training: Pretrain with CE, then SFT+DLM on instruction data
#
# Usage:
#   uv run --package wf-train python scripts/train_lightning.py \
#     model=qwen3_4b training=pretrain_then_sft
#
# Phase 1 (0-90%): Pretrain on FineWeb with CE only (no DLM - DLM requires SFT)
# Phase 2 (90-100%): SFT+DLM on Nemotron dataset (DLM masks response tokens only)
#
# NOTE: DLM requires SFT data with instruction masking (labels=-100).
# Fast-dLLM v2 was trained on Qwen2.5-7B-Instruct, not a base model.
# See: https://huggingface.co/Efficient-Large-Model/Fast_dLLM_v2_7B

defaults:
  - base

stage: unified_sft

# Lambda warmup disabled (start with full quantization)
lambda_warmup:
  enabled: false

# === OBJECTIVES ===
# All objectives available - curriculum controls which are active
objectives:
  dlm:
    enabled: true  # Will be activated in SFT phase
  sft:
    enabled: true  # Will be activated in SFT phase
    weight: 1.0
    ignore_index: -100
    label_smoothing: 0.0

# === CURRICULUM SCHEDULE ===
# Two-phase: Pretrain (CE only) -> SFT+DLM
curriculum:
  phases:
    # Phase 1: Pretrain with CE only (90% of training)
    # NOTE: No DLM here - DLM requires SFT data with instruction masking
    - name: pretrain
      end_ratio: 0.9
      data_config: fineweb
      objectives:
        continue_pretrain: 1.0
        dlm: 0.0  # DLM requires SFT
        sft: 0.0  # Not SFT data yet

    # Phase 2: SFT+DLM on instruction data (10% of training)
    # DLM only masks response tokens (labels != -100)
    - name: sft_dlm
      end_ratio: 1.0
      data_config: sft_nemotron
      objectives:
        continue_pretrain: 0.0  # Disabled during SFT
        dlm: 0.5  # DLM with SFT - masks response tokens only
        sft: 1.0

# Logging
logging:
  wandb:
    tags: [unified, bitnet, sft, nemotron]
