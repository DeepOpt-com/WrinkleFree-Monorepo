# Qwen3-0.6B BitNet model configuration
# Based on Qwen/Qwen3-0.6B-Base (smallest Qwen3 model)
name: qwen3_0.6b

# Pretrained source for auto-conversion
pretrained_name: Qwen/Qwen3-0.6B-Base

# Architecture (from HuggingFace config)
vocab_size: 151936
hidden_size: 1024
intermediate_size: 3072
num_hidden_layers: 28
num_attention_heads: 16
num_kv_heads: 8  # GQA
head_dim: 128  # Qwen3 uses explicit head_dim
max_position_embeddings: 32768
rope_theta: 1000000.0

# Quantization settings
quantization:
  weight_bits: 1.58  # Ternary {-1, 0, 1}
  activation_bits: 8
  per_token_activation: true

# SubLN settings
subln:
  enabled: true
  eps: 1.0e-6
  elementwise_affine: true

# Model behavior
attention_dropout: 0.0
hidden_act: silu
rms_norm_eps: 1.0e-6
tie_word_embeddings: true
use_flash_attention: true

# Teacher model for distillation (self-distillation)
teacher:
  pretrained: Qwen/Qwen3-0.6B-Base
  load_in_fp16: true
  freeze: true
