# SmolLM2 135M BitNet model configuration
# Based on HuggingFaceTB/SmolLM2-135M
# Small model ideal for single GPU training and testing
name: smollm2_135m
pretrained_name: HuggingFaceTB/SmolLM2-135M

# Architecture
vocab_size: 49152
hidden_size: 576
intermediate_size: 1536
num_hidden_layers: 30
num_attention_heads: 9
num_kv_heads: 3  # GQA
head_dim: 64  # hidden_size / num_attention_heads
max_position_embeddings: 8192
rope_theta: 100000.0

# Quantization settings
quantization:
  weight_bits: 1.58  # Ternary {-1, 0, 1}
  activation_bits: 8
  per_token_activation: true

# SubLN settings
subln:
  enabled: true
  eps: 1.0e-5
  elementwise_affine: true

# Model behavior
attention_dropout: 0.0
hidden_act: relu2  # ReLU^2 for BitNet (original uses silu)
rms_norm_eps: 1.0e-5
tie_word_embeddings: true
use_flash_attention: false  # Disabled until flash_attn added to Modal image (see issue #8)

# Teacher model for distillation (Stage 3)
teacher:
  pretrained: HuggingFaceTB/SmolLM2-135M
  load_in_fp16: true
  freeze: true
