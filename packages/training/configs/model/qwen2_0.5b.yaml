# Qwen2 0.5B BitNet model configuration
# Based on Qwen/Qwen2-0.5B
# Small model for testing and smoke tests
name: qwen2_0.5b
pretrained_name: Qwen/Qwen2-0.5B

# Architecture
vocab_size: 151936
hidden_size: 896
intermediate_size: 4864
num_hidden_layers: 24
num_attention_heads: 14
num_kv_heads: 2  # GQA
head_dim: 64  # hidden_size / num_attention_heads
max_position_embeddings: 32768
rope_theta: 1000000.0

# Quantization settings
quantization:
  weight_bits: 1.58  # Ternary {-1, 0, 1}
  activation_bits: 8
  per_token_activation: true

# SubLN settings
subln:
  enabled: true
  eps: 1.0e-5
  elementwise_affine: true

# Model behavior
attention_dropout: 0.0
hidden_act: silu
rms_norm_eps: 1.0e-6
tie_word_embeddings: false
use_flash_attention: false

# Teacher model for distillation
teacher:
  pretrained: Qwen/Qwen2-0.5B
  load_in_fp16: true
  freeze: true
