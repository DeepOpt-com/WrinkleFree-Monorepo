<!DOCTYPE html><html><head>
      <title>2025-12-19-training-improvements</title>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      
      <link rel="stylesheet" href="file:////home/lev/.vscode/extensions/shd101wyy.markdown-preview-enhanced-0.8.20/crossnote/dependencies/katex/katex.min.css">
      
      
      
      
      
      <style>
      code[class*=language-],pre[class*=language-]{color:#333;background:0 0;font-family:Consolas,"Liberation Mono",Menlo,Courier,monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.4;-moz-tab-size:8;-o-tab-size:8;tab-size:8;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none}pre[class*=language-]{padding:.8em;overflow:auto;border-radius:3px;background:#f5f5f5}:not(pre)>code[class*=language-]{padding:.1em;border-radius:.3em;white-space:normal;background:#f5f5f5}.token.blockquote,.token.comment{color:#969896}.token.cdata{color:#183691}.token.doctype,.token.macro.property,.token.punctuation,.token.variable{color:#333}.token.builtin,.token.important,.token.keyword,.token.operator,.token.rule{color:#a71d5d}.token.attr-value,.token.regex,.token.string,.token.url{color:#183691}.token.atrule,.token.boolean,.token.code,.token.command,.token.constant,.token.entity,.token.number,.token.property,.token.symbol{color:#0086b3}.token.prolog,.token.selector,.token.tag{color:#63a35c}.token.attr-name,.token.class,.token.class-name,.token.function,.token.id,.token.namespace,.token.pseudo-class,.token.pseudo-element,.token.url-reference .token.variable{color:#795da3}.token.entity{cursor:help}.token.title,.token.title .token.punctuation{font-weight:700;color:#1d3e81}.token.list{color:#ed6a43}.token.inserted{background-color:#eaffea;color:#55a532}.token.deleted{background-color:#ffecec;color:#bd2c00}.token.bold{font-weight:700}.token.italic{font-style:italic}.language-json .token.property{color:#183691}.language-markup .token.tag .token.punctuation{color:#333}.language-css .token.function,code.language-css{color:#0086b3}.language-yaml .token.atrule{color:#63a35c}code.language-yaml{color:#183691}.language-ruby .token.function{color:#333}.language-markdown .token.url{color:#795da3}.language-makefile .token.symbol{color:#795da3}.language-makefile .token.variable{color:#183691}.language-makefile .token.builtin{color:#0086b3}.language-bash .token.keyword{color:#0086b3}pre[data-line]{position:relative;padding:1em 0 1em 3em}pre[data-line] .line-highlight-wrapper{position:absolute;top:0;left:0;background-color:transparent;display:block;width:100%}pre[data-line] .line-highlight{position:absolute;left:0;right:0;padding:inherit 0;margin-top:1em;background:hsla(24,20%,50%,.08);background:linear-gradient(to right,hsla(24,20%,50%,.1) 70%,hsla(24,20%,50%,0));pointer-events:none;line-height:inherit;white-space:pre}pre[data-line] .line-highlight:before,pre[data-line] .line-highlight[data-end]:after{content:attr(data-start);position:absolute;top:.4em;left:.6em;min-width:1em;padding:0 .5em;background-color:hsla(24,20%,50%,.4);color:#f4f1ef;font:bold 65%/1.5 sans-serif;text-align:center;vertical-align:.3em;border-radius:999px;text-shadow:none;box-shadow:0 1px #fff}pre[data-line] .line-highlight[data-end]:after{content:attr(data-end);top:auto;bottom:.4em}html body{font-family:'Helvetica Neue',Helvetica,'Segoe UI',Arial,freesans,sans-serif;font-size:16px;line-height:1.6;color:#333;background-color:#fff;overflow:initial;box-sizing:border-box;word-wrap:break-word}html body>:first-child{margin-top:0}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{line-height:1.2;margin-top:1em;margin-bottom:16px;color:#000}html body h1{font-size:2.25em;font-weight:300;padding-bottom:.3em}html body h2{font-size:1.75em;font-weight:400;padding-bottom:.3em}html body h3{font-size:1.5em;font-weight:500}html body h4{font-size:1.25em;font-weight:600}html body h5{font-size:1.1em;font-weight:600}html body h6{font-size:1em;font-weight:600}html body h1,html body h2,html body h3,html body h4,html body h5{font-weight:600}html body h5{font-size:1em}html body h6{color:#5c5c5c}html body strong{color:#000}html body del{color:#5c5c5c}html body a:not([href]){color:inherit;text-decoration:none}html body a{color:#08c;text-decoration:none}html body a:hover{color:#00a3f5;text-decoration:none}html body img{max-width:100%}html body>p{margin-top:0;margin-bottom:16px;word-wrap:break-word}html body>ol,html body>ul{margin-bottom:16px}html body ol,html body ul{padding-left:2em}html body ol.no-list,html body ul.no-list{padding:0;list-style-type:none}html body ol ol,html body ol ul,html body ul ol,html body ul ul{margin-top:0;margin-bottom:0}html body li{margin-bottom:0}html body li.task-list-item{list-style:none}html body li>p{margin-top:0;margin-bottom:0}html body .task-list-item-checkbox{margin:0 .2em .25em -1.8em;vertical-align:middle}html body .task-list-item-checkbox:hover{cursor:pointer}html body blockquote{margin:16px 0;font-size:inherit;padding:0 15px;color:#5c5c5c;background-color:#f0f0f0;border-left:4px solid #d6d6d6}html body blockquote>:first-child{margin-top:0}html body blockquote>:last-child{margin-bottom:0}html body hr{height:4px;margin:32px 0;background-color:#d6d6d6;border:0 none}html body table{margin:10px 0 15px 0;border-collapse:collapse;border-spacing:0;display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}html body table th{font-weight:700;color:#000}html body table td,html body table th{border:1px solid #d6d6d6;padding:6px 13px}html body dl{padding:0}html body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:700}html body dl dd{padding:0 16px;margin-bottom:16px}html body code{font-family:Menlo,Monaco,Consolas,'Courier New',monospace;font-size:.85em;color:#000;background-color:#f0f0f0;border-radius:3px;padding:.2em 0}html body code::after,html body code::before{letter-spacing:-.2em;content:'\00a0'}html body pre>code{padding:0;margin:0;word-break:normal;white-space:pre;background:0 0;border:0}html body .highlight{margin-bottom:16px}html body .highlight pre,html body pre{padding:1em;overflow:auto;line-height:1.45;border:#d6d6d6;border-radius:3px}html body .highlight pre{margin-bottom:0;word-break:normal}html body pre code,html body pre tt{display:inline;max-width:initial;padding:0;margin:0;overflow:initial;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}html body pre code:after,html body pre code:before,html body pre tt:after,html body pre tt:before{content:normal}html body blockquote,html body dl,html body ol,html body p,html body pre,html body ul{margin-top:0;margin-bottom:16px}html body kbd{color:#000;border:1px solid #d6d6d6;border-bottom:2px solid #c7c7c7;padding:2px 4px;background-color:#f0f0f0;border-radius:3px}@media print{html body{background-color:#fff}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{color:#000;page-break-after:avoid}html body blockquote{color:#5c5c5c}html body pre{page-break-inside:avoid}html body table{display:table}html body img{display:block;max-width:100%;max-height:100%}html body code,html body pre{word-wrap:break-word;white-space:pre}}.markdown-preview{width:100%;height:100%;box-sizing:border-box}.markdown-preview ul{list-style:disc}.markdown-preview ul ul{list-style:circle}.markdown-preview ul ul ul{list-style:square}.markdown-preview ol{list-style:decimal}.markdown-preview ol ol,.markdown-preview ul ol{list-style-type:lower-roman}.markdown-preview ol ol ol,.markdown-preview ol ul ol,.markdown-preview ul ol ol,.markdown-preview ul ul ol{list-style-type:lower-alpha}.markdown-preview .newpage,.markdown-preview .pagebreak{page-break-before:always}.markdown-preview pre.line-numbers{position:relative;padding-left:3.8em;counter-reset:linenumber}.markdown-preview pre.line-numbers>code{position:relative}.markdown-preview pre.line-numbers .line-numbers-rows{position:absolute;pointer-events:none;top:1em;font-size:100%;left:0;width:3em;letter-spacing:-1px;border-right:1px solid #999;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.markdown-preview pre.line-numbers .line-numbers-rows>span{pointer-events:none;display:block;counter-increment:linenumber}.markdown-preview pre.line-numbers .line-numbers-rows>span:before{content:counter(linenumber);color:#999;display:block;padding-right:.8em;text-align:right}.markdown-preview .mathjax-exps .MathJax_Display{text-align:center!important}.markdown-preview:not([data-for=preview]) .code-chunk .code-chunk-btn-group{display:none}.markdown-preview:not([data-for=preview]) .code-chunk .status{display:none}.markdown-preview:not([data-for=preview]) .code-chunk .output-div{margin-bottom:16px}.markdown-preview .md-toc{padding:0}.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link{display:inline;padding:.25rem 0}.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link div,.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link p{display:inline}.markdown-preview .md-toc .md-toc-link-wrapper.highlighted .md-toc-link{font-weight:800}.scrollbar-style::-webkit-scrollbar{width:8px}.scrollbar-style::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}.scrollbar-style::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,.66);border:4px solid rgba(150,150,150,.66);background-clip:content-box}html body[for=html-export]:not([data-presentation-mode]){position:relative;width:100%;height:100%;top:0;left:0;margin:0;padding:0;overflow:auto}html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{position:relative;top:0;min-height:100vh}@media screen and (min-width:914px){html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{padding:2em calc(50% - 457px + 2em)}}@media screen and (max-width:914px){html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{font-size:14px!important;padding:1em}}@media print{html body[for=html-export]:not([data-presentation-mode]) #sidebar-toc-btn{display:none}}html body[for=html-export]:not([data-presentation-mode]) #sidebar-toc-btn{position:fixed;bottom:8px;left:8px;font-size:28px;cursor:pointer;color:inherit;z-index:99;width:32px;text-align:center;opacity:.4}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] #sidebar-toc-btn{opacity:1}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc{position:fixed;top:0;left:0;width:300px;height:100%;padding:32px 0 48px 0;font-size:14px;box-shadow:0 0 4px rgba(150,150,150,.33);box-sizing:border-box;overflow:auto;background-color:inherit}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar{width:8px}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,.66);border:4px solid rgba(150,150,150,.66);background-clip:content-box}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc a{text-decoration:none}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc{padding:0 16px}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link{display:inline;padding:.25rem 0}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link div,html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link p{display:inline}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper.highlighted .md-toc-link{font-weight:800}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{left:300px;width:calc(100% - 300px);padding:2em calc(50% - 457px - 300px / 2);margin:0;box-sizing:border-box}@media screen and (max-width:1274px){html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{width:100%}}html body[for=html-export]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .markdown-preview{left:50%;transform:translateX(-50%)}html body[for=html-export]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .md-sidebar-toc{display:none}
/* Please visit the URL below for more information: */
/*   https://shd101wyy.github.io/markdown-preview-enhanced/#/customize-css */

      </style>
      <!-- The content below will be included at the end of the <head> element. --><script type="text/javascript">
  document.addEventListener("DOMContentLoaded", function () {
    // your code here
  });
</script></head><body for="html-export">
    
    
      <div class="crossnote markdown-preview  ">
      
<h1 id="training-improvements-research">Training Improvements Research </h1>
<p><strong>Date:</strong> 2025-12-19<br>
<strong>Topic:</strong> Opportunities to make WrinkleFree training faster and better</p>
<hr>
<h2 id="executive-summary">Executive Summary </h2>
<p>Research into recent advances in LLM training, 1.58-bit quantization, and datasets reveals several high-impact opportunities for improving WrinkleFree's training pipeline. Key findings include better datasets (MegaMath, DCLM), training optimizations (sequence packing, optimal quantization transitions), and distillation improvements.</p>
<hr>
<h2 id="1-better-datasets">1. Better Datasets </h2>
<h3 id="megamath-recommended">MegaMath (Recommended) </h3>
<ul>
<li><strong>Size:</strong> 370B tokens</li>
<li><strong>Source:</strong> <a href="https://huggingface.co/datasets/LLM360/MegaMath">HuggingFace</a> | <a href="https://arxiv.org/abs/2504.02807">Paper</a> | <a href="https://github.com/LLM360/MegaMath">GitHub</a></li>
<li><strong>Why:</strong> Largest open math pre-training dataset, accepted to COLM 2025. Provides 15-20% performance boost on downstream math benchmarks vs smaller alternatives like OpenWebMath.</li>
<li><strong>Contents:</strong>
<ul>
<li>Re-extracted mathematical documents from Common Crawl with math-oriented HTML optimizations</li>
<li>Math-related code from Stack-V2</li>
<li>Synthesized QA-style text and interleaved text-code blocks</li>
</ul>
</li>
<li><strong>Recommendation:</strong> Replace OpenWebMath with MegaMath for math reasoning capability</li>
</ul>
<h3 id="dclm-baseline">DCLM-Baseline </h3>
<ul>
<li><strong>Size:</strong> 4T tokens / 3B documents</li>
<li><strong>Source:</strong> <a href="https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0">HuggingFace</a> | <a href="https://github.com/mlfoundations/dclm">GitHub</a></li>
<li><strong>Why:</strong> Apple's DataComp-LM achieves strong performance through rigorous cleaning, filtering, and deduplication of Common Crawl. Outperforms FineWeb on many benchmarks.</li>
<li><strong>Recommendation:</strong> Consider as primary web corpus replacement for FineWeb</li>
</ul>
<h3 id="dclm-edu">DCLM-Edu </h3>
<ul>
<li><strong>Size:</strong> Educational subset of DCLM</li>
<li><strong>Source:</strong> <a href="https://huggingface.co/datasets/HuggingFaceTB/dclm-edu">HuggingFace</a></li>
<li><strong>Why:</strong> Filtered using FineWeb-Edu classifier (score &gt; 2). Used to train SmolLM2-135M and SmolLM2-360M. May be better than FineWeb-Edu for small models.</li>
</ul>
<h3 id="ultra-fineweb">Ultra-FineWeb </h3>
<ul>
<li><strong>Size:</strong> ~120B Chinese tokens (Chinese focus, but methodology applicable)</li>
<li><strong>Source:</strong> <a href="https://arxiv.org/html/2505.05427">Paper</a></li>
<li><strong>Why:</strong> FastText-based lightweight classifier filtering of FineWeb. Significant quality improvements on benchmarks.</li>
</ul>
<h3 id="other-notable-datasets">Other Notable Datasets </h3>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Size</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr>
<td>Falcon RefinedWeb</td>
<td>500-650B tokens</td>
<td>High-quality filtered web data</td>
</tr>
<tr>
<td>Nemotron-CC-HQ</td>
<td>Large</td>
<td>NVIDIA's CC corpus for high-quality pre-training</td>
</tr>
<tr>
<td>FineWeb2</td>
<td>Multilingual</td>
<td>1000+ languages support</td>
</tr>
<tr>
<td>StarCoder Data</td>
<td>783GB</td>
<td>Code in 86 programming languages</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="2-training-technique-improvements">2. Training Technique Improvements </h2>
<h3 id="continual-quantization-aware-pre-training">Continual Quantization-Aware Pre-Training </h3>
<p><strong>Source:</strong> <a href="https://arxiv.org/abs/2502.11895">arXiv:2502.11895</a></p>
<p>Research shows there's an <strong>optimal transition point</strong> from 16-bit to 1.58-bit training:</p>
<blockquote>
<p>"Results on 11 downstream tasks show that this 16-to-1.58-bit training strategy is preferable over full 1.58-bit training and leaves models closer to those which have undergone 16-bit training."</p>
</blockquote>
<p><strong>Key Finding:</strong> The data-optimal transition point exists - switching too early or too late hurts final model quality.</p>
<p><strong>Current State:</strong> WrinkleFree uses a 3-stage approach (SubLN insertion → continue pre-training → distillation). May need to investigate optimal transition timing.</p>
<h3 id="sequence-packing">Sequence Packing </h3>
<p><strong>Source:</strong> <a href="https://huggingface.co/blog/sirluk/llm-sequence-packing">HuggingFace Blog</a> | <a href="https://arxiv.org/abs/2107.02027">Paper</a></p>
<p><strong>Problem:</strong> Up to 50% of tokens can be padding in standard training, wasting compute.</p>
<p><strong>Solution:</strong> Concatenate multiple shorter sequences into single inputs with proper attention masking.</p>
<p><strong>Benefits:</strong></p>
<ul>
<li>2x speedup for pre-training</li>
<li>40% speed increase demonstrated with Vicuna-1.5 13B</li>
<li>Effectively increases batch size with minimal overhead</li>
</ul>
<p><strong>Implementation:</strong></p>
<ul>
<li>Available in Transformers 4.43+ via <code>DataCollatorWithFlattening</code></li>
<li>Flash Attention 2 supports proper masking for packed sequences</li>
<li>Must adjust position IDs to demarcate sequence boundaries</li>
<li>Requires hyperparameter adjustment (LR, optimizer params) due to effective batch size increase</li>
</ul>
<p><strong>Caution:</strong> Cross-contamination can occur if attention isn't properly masked between sequences.</p>
<h3 id="curriculum-learning--lr-decay-interaction">Curriculum Learning + LR Decay Interaction </h3>
<p><strong>Source:</strong> <a href="https://arxiv.org/abs/2511.18903">arXiv:2511.18903</a></p>
<p><strong>Critical Finding:</strong> Standard learning rate decay schedules are incompatible with curriculum learning (easy→hard data ordering):</p>
<blockquote>
<p>"While curriculum-based training substantially outperforms random shuffling when using a constant LR, its advantage diminishes under standard LR decay schedules."</p>
</blockquote>
<p><strong>Problem:</strong> Best/hardest data arrives when LR is lowest, wasting its potential.</p>
<p><strong>Solutions:</strong></p>
<ol>
<li>Use moderate LR decay (final LR only slightly smaller than peak)</li>
<li>Replace LR decay with model averaging</li>
<li>Combining both strategies: 1.64% improvement over random shuffling</li>
</ol>
<h3 id="multi-stage-pretraining-mid-training">Multi-Stage Pretraining (Mid-Training) </h3>
<p><strong>Source:</strong> <a href="https://arxiv.org/html/2510.23081">OLMo 2 Survey</a></p>
<p>Successful curriculum strategy:</p>
<ol>
<li>First stage: Data mixture dominated by massive web data</li>
<li>Second stage (mid-training): Shift to high-quality data with LR annealing</li>
</ol>
<p><strong>Techniques:</strong></p>
<ul>
<li><strong>Microanneals:</strong> Extrapolate efficacy of large-scale mixtures from small-scale trials</li>
<li><strong>RegMix:</strong> Formalizes ratio optimization as regression task</li>
</ul>
<hr>
<h2 id="3-optimizer-improvements">3. Optimizer Improvements </h2>
<h3 id="current-state">Current State </h3>
<p>WrinkleFree uses Muon (default) and Apollo optimizers. Apollo emerged as winner in Bayesian hyperparameter search.</p>
<h3 id="8-bit-muon">8-bit Muon </h3>
<p><strong>Source:</strong> <a href="https://github.com/KellerJordan/Muon">GitHub/Muon</a></p>
<p>Blockwise quantization of Muon optimizer:</p>
<ul>
<li>~74% reduction in memory footprint vs full-precision Muon</li>
<li>Supports both linear and dynamic quantization schemes</li>
<li>Minimal quality loss</li>
</ul>
<h3 id="muonclip">MuonClip </h3>
<p><strong>Source:</strong> <a href="https://arxiv.org/abs/2502.16982">Moonlight Paper</a></p>
<p>Used in Kimi-2 (1 trillion parameter LLM):</p>
<ul>
<li>Dramatically smoother loss curves</li>
<li>Better training dynamics than standard Muon</li>
</ul>
<h3 id="normuon-neuron-wise-normalized-muon">NorMuon (Neuron-wise Normalized Muon) </h3>
<p>Addresses non-uniform neuron norms in Muon updates:</p>
<ul>
<li>Combines orthogonalization with neuron-level adaptive learning rates</li>
<li>Fixes the finding that Muon's resulting updates exhibit highly non-uniform neuron norms</li>
</ul>
<h3 id="muon-scalability-improvements-2025">Muon Scalability Improvements (2025) </h3>
<p><strong>Source:</strong> <a href="https://arxiv.org/abs/2502.16982">arXiv:2502.16982</a></p>
<p>Two crucial techniques for large-scale Muon:</p>
<ol>
<li>Adding weight decay</li>
<li>Carefully adjusting per-parameter update scale</li>
</ol>
<p><strong>Result:</strong> Muon achieves comparable performance to AdamW while requiring only ~52% of training FLOPs.</p>
<hr>
<h2 id="4-distillation-improvements">4. Distillation Improvements </h2>
<h3 id="current-state-1">Current State </h3>
<p>WrinkleFree BitDistill uses:</p>
<pre data-role="codeBlock" data-info="" class="language-text"><code>L = L_CE + λ×L_LD + γ×L_AD
</code></pre><ul>
<li>L_CE: Cross-entropy loss</li>
<li>L_LD: Logits KL divergence (λ=10.0 classification, 1.0 summarization)</li>
<li>L_AD: Attention distribution distillation (γ=1e-5 classification, 1e-3 summarization)</li>
</ul>
<h3 id="intermediate-layer-distillation">Intermediate Layer Distillation </h3>
<p><strong>Source:</strong> <a href="https://www.sciencedirect.com/science/article/pii/S2666827024000811">Survey</a></p>
<p>Add hidden state distillation from intermediate layers:</p>
<pre data-role="codeBlock" data-info="" class="language-text"><code>L = L_CE + λ×L_LD + γ×L_AD + δ×L_hidden
</code></pre><p>Benefits:</p>
<ul>
<li>Transfers more detailed and structured information</li>
<li>Student captures teacher's internal representations, not just outputs</li>
</ul>
<h3 id="multi-teacher-distillation">Multi-Teacher Distillation </h3>
<p>Use multiple teacher models (e.g., Qwen + LLaMA):</p>
<ul>
<li>More comprehensive knowledge transfer</li>
<li>Improved robustness</li>
<li>Student benefits from diverse teacher perspectives</li>
</ul>
<h3 id="synthetic-data-generation">Synthetic Data Generation </h3>
<p><strong>Source:</strong> <a href="https://www.exxactcorp.com/blog/deep-learning/what-is-llm-distillation-vs-quantization">LLM QAT Research</a></p>
<blockquote>
<p>"LLM QAT shows the utility of synthetic data for QAT, generating synthetic training data from full-precision model and using knowledge distillation to train quantized models."</p>
</blockquote>
<p>The teacher model can generate synthetic data specifically tuned for the student's weaknesses.</p>
<h3 id="combined-compression-pipeline">Combined Compression Pipeline </h3>
<p><strong>Source:</strong> <a href="https://developer.nvidia.com/blog/llm-model-pruning-and-knowledge-distillation-with-nvidia-nemo-framework/">NVIDIA Blog</a></p>
<p>Optimal workflow:</p>
<ol>
<li>First prune the model</li>
<li>Then quantize</li>
<li>Use knowledge distillation to fine-tune and recover lost performance</li>
</ol>
<p>Result: ~4x size reduction (e.g., 2.7B → 700M parameters), then int8 quantization for ~14x total reduction.</p>
<hr>
<h2 id="5-quick-wins-summary">5. Quick Wins Summary </h2>
<table>
<thead>
<tr>
<th>Improvement</th>
<th>Effort</th>
<th>Expected Gain</th>
<th>Priority</th>
</tr>
</thead>
<tbody>
<tr>
<td>Switch to MegaMath (replace OpenWebMath)</td>
<td>Low</td>
<td>15-20% on math benchmarks</td>
<td>High</td>
</tr>
<tr>
<td>Add sequence packing</td>
<td>Medium</td>
<td>Up to 2x training throughput</td>
<td>High</td>
</tr>
<tr>
<td>Use DCLM instead of FineWeb</td>
<td>Low</td>
<td>Better downstream performance</td>
<td>Medium</td>
</tr>
<tr>
<td>Implement 8-bit Muon</td>
<td>Low</td>
<td>74% less optimizer memory</td>
<td>Medium</td>
</tr>
<tr>
<td>Investigate optimal 16→1.58bit transition</td>
<td>Medium</td>
<td>Better final quality</td>
<td>Medium</td>
</tr>
<tr>
<td>Add intermediate layer distillation</td>
<td>Medium</td>
<td>More structured knowledge transfer</td>
<td>Low</td>
</tr>
<tr>
<td>Fix LR decay + curriculum interaction</td>
<td>Low</td>
<td>~1.6% improvement</td>
<td>Low</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="6-recommended-data-mix">6. Recommended Data Mix </h2>
<p>Based on research, suggested replacement for current mix:</p>
<p><strong>Current Mix:</strong></p>
<ul>
<li>FineWeb (40%)</li>
<li>SlimPajama (30%)</li>
<li>OpenWebMath (15%)</li>
<li>CodeParrot (15%)</li>
</ul>
<p><strong>Proposed Mix:</strong></p>
<ul>
<li>DCLM-Baseline (40%) - Higher quality than FineWeb</li>
<li>MegaMath-Web (25%) - Much larger than OpenWebMath</li>
<li>MegaMath-Code (15%) - Math-focused code</li>
<li>StarCoder/CodeParrot (20%) - General code</li>
</ul>
<hr>
<h2 id="references">References </h2>
<ol>
<li><a href="https://huggingface.co/datasets/LLM360/MegaMath">MegaMath Dataset</a> - LLM360</li>
<li><a href="https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0">DCLM-Baseline Dataset</a> - ML Foundations</li>
<li><a href="https://arxiv.org/html/2505.05427">Ultra-FineWeb Paper</a> - arXiv</li>
<li><a href="https://arxiv.org/abs/2502.11895">Continual QAT for BitNet</a> - ACL 2025</li>
<li><a href="https://github.com/KellerJordan/Muon">Muon Optimizer</a> - GitHub</li>
<li><a href="https://arxiv.org/abs/2502.16982">Muon Scalability Paper</a> - Moonshot AI</li>
<li><a href="https://huggingface.co/blog/sirluk/llm-sequence-packing">Sequence Packing Guide</a> - HuggingFace</li>
<li><a href="https://arxiv.org/abs/2511.18903">Curriculum + LR Decay</a> - arXiv</li>
<li><a href="https://arxiv.org/pdf/2510.13998">BitDistill Paper</a> - Microsoft</li>
<li><a href="https://developer.nvidia.com/blog/llm-model-pruning-and-knowledge-distillation-with-nvidia-nemo-framework/">NVIDIA Pruning + Distillation</a> - NVIDIA</li>
<li><a href="https://github.com/mlfoundations/dclm">DataComp-LM</a> - GitHub</li>
<li><a href="https://arxiv.org/html/2510.23081">OLMo 2 Mid-Training Survey</a> - arXiv</li>
</ol>

      </div>
      
      
    
    
    
    
    
    
  
    </body></html>